{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN3_3 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "        self.power_mean = 0 \n",
    "        self.power_std = 0 \n",
    "        self.sum = 0 \n",
    "        self.sum_deviation = 0 \n",
    "        self.N = 0 \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self.N += 1 \n",
    "        power = sample[0][0]\n",
    "        self.sum += power \n",
    "        self.power_mean = self.sum / self.N \n",
    "        self.sum_deviation += (power - self.power_mean) ** 2\n",
    "        self.power_std = np.sqrt(self.sum_deviation / self.N)\n",
    "        \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    next_power_states = np.array([0 if val[3] is None else (val[3][0] - memory.power_mean) / memory.power_std  \n",
    "                        for val in batch])\n",
    "    \n",
    "    states[:, 0] = (states[:, 0] - memory.power_mean) / memory.power_std \n",
    "    next_states[:, 0] = next_power_states\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 2\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -5140.226195291837 Explore P: 0.9217 SOC: 1.0000 Cumulative_SOC_deviation: 498.7396 Fuel Consumption: 152.8302\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -5033.019760848865 Explore P: 0.8970 SOC: 1.0000 Cumulative_SOC_deviation: 489.2599 Fuel Consumption: 140.4207\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -5085.605415949135 Explore P: 0.8730 SOC: 1.0000 Cumulative_SOC_deviation: 494.0752 Fuel Consumption: 144.8537\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -5056.2389730870755 Explore P: 0.8496 SOC: 1.0000 Cumulative_SOC_deviation: 491.4313 Fuel Consumption: 141.9261\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -5051.39342816724 Explore P: 0.8269 SOC: 1.0000 Cumulative_SOC_deviation: 490.9321 Fuel Consumption: 142.0727\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -5033.788288908213 Explore P: 0.8048 SOC: 1.0000 Cumulative_SOC_deviation: 489.0147 Fuel Consumption: 143.6416\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -4964.66407107627 Explore P: 0.7832 SOC: 1.0000 Cumulative_SOC_deviation: 482.4194 Fuel Consumption: 140.4696\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -5014.136905817339 Explore P: 0.7623 SOC: 1.0000 Cumulative_SOC_deviation: 487.6224 Fuel Consumption: 137.9134\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -4939.349202101055 Explore P: 0.7419 SOC: 1.0000 Cumulative_SOC_deviation: 479.9579 Fuel Consumption: 139.7707\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -4897.390765565927 Explore P: 0.7221 SOC: 1.0000 Cumulative_SOC_deviation: 476.0079 Fuel Consumption: 137.3122\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -5006.258523048914 Explore P: 0.7028 SOC: 1.0000 Cumulative_SOC_deviation: 486.1581 Fuel Consumption: 144.6778\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -4921.630709877807 Explore P: 0.6840 SOC: 1.0000 Cumulative_SOC_deviation: 478.6508 Fuel Consumption: 135.1226\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -5021.490171912917 Explore P: 0.6658 SOC: 1.0000 Cumulative_SOC_deviation: 487.7140 Fuel Consumption: 144.3503\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -4951.798656487032 Explore P: 0.6480 SOC: 1.0000 Cumulative_SOC_deviation: 480.9692 Fuel Consumption: 142.1069\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -5050.427490442187 Explore P: 0.6307 SOC: 1.0000 Cumulative_SOC_deviation: 491.9019 Fuel Consumption: 131.4081\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -5072.301907278187 Explore P: 0.6139 SOC: 1.0000 Cumulative_SOC_deviation: 492.9643 Fuel Consumption: 142.6592\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -5095.770004911721 Explore P: 0.5976 SOC: 1.0000 Cumulative_SOC_deviation: 495.5965 Fuel Consumption: 139.8049\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -4984.034049038864 Explore P: 0.5816 SOC: 1.0000 Cumulative_SOC_deviation: 484.8892 Fuel Consumption: 135.1422\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -4884.7597187975 Explore P: 0.5662 SOC: 1.0000 Cumulative_SOC_deviation: 474.6631 Fuel Consumption: 138.1284\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -4936.139299142282 Explore P: 0.5511 SOC: 1.0000 Cumulative_SOC_deviation: 481.0596 Fuel Consumption: 125.5430\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -4628.162995789614 Explore P: 0.5364 SOC: 1.0000 Cumulative_SOC_deviation: 450.1335 Fuel Consumption: 126.8285\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -4634.161681625409 Explore P: 0.5222 SOC: 1.0000 Cumulative_SOC_deviation: 450.6595 Fuel Consumption: 127.5665\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -4463.4544250066965 Explore P: 0.5083 SOC: 1.0000 Cumulative_SOC_deviation: 434.4637 Fuel Consumption: 118.8178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -4767.506986996572 Explore P: 0.4948 SOC: 1.0000 Cumulative_SOC_deviation: 464.7071 Fuel Consumption: 120.4356\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -4873.533726340343 Explore P: 0.4817 SOC: 1.0000 Cumulative_SOC_deviation: 475.1446 Fuel Consumption: 122.0875\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -4488.368118548133 Explore P: 0.4689 SOC: 1.0000 Cumulative_SOC_deviation: 437.2439 Fuel Consumption: 115.9292\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -4825.230954344753 Explore P: 0.4565 SOC: 1.0000 Cumulative_SOC_deviation: 469.8065 Fuel Consumption: 127.1657\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -4498.108023023983 Explore P: 0.4444 SOC: 1.0000 Cumulative_SOC_deviation: 436.6162 Fuel Consumption: 131.9457\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -4173.571278446328 Explore P: 0.4326 SOC: 1.0000 Cumulative_SOC_deviation: 404.9279 Fuel Consumption: 124.2918\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -4477.826507332265 Explore P: 0.4212 SOC: 1.0000 Cumulative_SOC_deviation: 434.7049 Fuel Consumption: 130.7776\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -4529.8425901641085 Explore P: 0.4100 SOC: 1.0000 Cumulative_SOC_deviation: 439.6304 Fuel Consumption: 133.5390\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -3892.7698533245916 Explore P: 0.3992 SOC: 1.0000 Cumulative_SOC_deviation: 377.0829 Fuel Consumption: 121.9409\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -3483.322144149803 Explore P: 0.3887 SOC: 1.0000 Cumulative_SOC_deviation: 336.3322 Fuel Consumption: 120.0006\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -4128.9704238784025 Explore P: 0.3784 SOC: 1.0000 Cumulative_SOC_deviation: 400.9967 Fuel Consumption: 119.0035\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -3648.497516791598 Explore P: 0.3684 SOC: 1.0000 Cumulative_SOC_deviation: 353.7979 Fuel Consumption: 110.5187\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -3646.8319982972803 Explore P: 0.3587 SOC: 1.0000 Cumulative_SOC_deviation: 353.9969 Fuel Consumption: 106.8629\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -3695.947461306922 Explore P: 0.3493 SOC: 1.0000 Cumulative_SOC_deviation: 357.5707 Fuel Consumption: 120.2401\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -2742.879364409473 Explore P: 0.3401 SOC: 1.0000 Cumulative_SOC_deviation: 264.3944 Fuel Consumption: 98.9353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -3468.3766224299056 Explore P: 0.3311 SOC: 1.0000 Cumulative_SOC_deviation: 336.7061 Fuel Consumption: 101.3155\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -3241.9211866212936 Explore P: 0.3224 SOC: 1.0000 Cumulative_SOC_deviation: 314.3797 Fuel Consumption: 98.1239\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -2712.351033160729 Explore P: 0.3140 SOC: 1.0000 Cumulative_SOC_deviation: 262.7262 Fuel Consumption: 85.0889\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -4762.570792856518 Explore P: 0.3057 SOC: 1.0000 Cumulative_SOC_deviation: 464.6397 Fuel Consumption: 116.1736\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -2200.5552886261776 Explore P: 0.2977 SOC: 1.0000 Cumulative_SOC_deviation: 211.1444 Fuel Consumption: 89.1113\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -3216.8245519311367 Explore P: 0.2899 SOC: 1.0000 Cumulative_SOC_deviation: 310.2953 Fuel Consumption: 113.8716\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -2171.6875982793026 Explore P: 0.2824 SOC: 1.0000 Cumulative_SOC_deviation: 208.8158 Fuel Consumption: 83.5297\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -2639.3407927643916 Explore P: 0.2750 SOC: 1.0000 Cumulative_SOC_deviation: 253.6935 Fuel Consumption: 102.4054\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -1309.7465705369368 Explore P: 0.2678 SOC: 0.8637 Cumulative_SOC_deviation: 123.7404 Fuel Consumption: 72.3422\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -2504.977596895831 Explore P: 0.2608 SOC: 1.0000 Cumulative_SOC_deviation: 239.6927 Fuel Consumption: 108.0505\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -2217.285164198618 Explore P: 0.2540 SOC: 1.0000 Cumulative_SOC_deviation: 213.0872 Fuel Consumption: 86.4134\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -3174.642756487721 Explore P: 0.2474 SOC: 1.0000 Cumulative_SOC_deviation: 305.8904 Fuel Consumption: 115.7386\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -1030.7063026758137 Explore P: 0.2410 SOC: 0.7980 Cumulative_SOC_deviation: 96.2905 Fuel Consumption: 67.8016\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -722.9061569897088 Explore P: 0.2347 SOC: 0.7907 Cumulative_SOC_deviation: 65.5148 Fuel Consumption: 67.7577\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -860.5982156158276 Explore P: 0.2286 SOC: 0.8270 Cumulative_SOC_deviation: 79.0773 Fuel Consumption: 69.8251\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -1309.7796223343016 Explore P: 0.2227 SOC: 1.0000 Cumulative_SOC_deviation: 122.6968 Fuel Consumption: 82.8113\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -759.371111692285 Explore P: 0.2170 SOC: 0.7642 Cumulative_SOC_deviation: 69.3192 Fuel Consumption: 66.1790\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -817.8246332131595 Explore P: 0.2114 SOC: 0.7659 Cumulative_SOC_deviation: 75.2012 Fuel Consumption: 65.8124\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -686.4570636172986 Explore P: 0.2059 SOC: 0.7169 Cumulative_SOC_deviation: 62.4452 Fuel Consumption: 62.0050\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -394.4902337972126 Explore P: 0.2006 SOC: 0.6460 Cumulative_SOC_deviation: 33.7422 Fuel Consumption: 57.0686\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -617.605853266563 Explore P: 0.1954 SOC: 0.7330 Cumulative_SOC_deviation: 55.4873 Fuel Consumption: 62.7333\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -384.86937138142383 Explore P: 0.1904 SOC: 0.6769 Cumulative_SOC_deviation: 32.6110 Fuel Consumption: 58.7597\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -732.328131716304 Explore P: 0.1855 SOC: 0.9143 Cumulative_SOC_deviation: 65.8427 Fuel Consumption: 73.9013\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -2638.5363841233925 Explore P: 0.1808 SOC: 1.0000 Cumulative_SOC_deviation: 251.8570 Fuel Consumption: 119.9664\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -351.9168199304087 Explore P: 0.1761 SOC: 0.6376 Cumulative_SOC_deviation: 29.6554 Fuel Consumption: 55.3629\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -332.53738203106377 Explore P: 0.1716 SOC: 0.6445 Cumulative_SOC_deviation: 27.7394 Fuel Consumption: 55.1429\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -314.67776420488667 Explore P: 0.1673 SOC: 0.6624 Cumulative_SOC_deviation: 25.6959 Fuel Consumption: 57.7187\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -364.05670547781057 Explore P: 0.1630 SOC: 0.6734 Cumulative_SOC_deviation: 30.5673 Fuel Consumption: 58.3834\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -306.95773464934615 Explore P: 0.1589 SOC: 0.6531 Cumulative_SOC_deviation: 24.9479 Fuel Consumption: 57.4792\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -471.77370685646474 Explore P: 0.1548 SOC: 0.6661 Cumulative_SOC_deviation: 41.4060 Fuel Consumption: 57.7138\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -255.7331968108823 Explore P: 0.1509 SOC: 0.6315 Cumulative_SOC_deviation: 19.9833 Fuel Consumption: 55.9005\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -266.4054821306945 Explore P: 0.1471 SOC: 0.6126 Cumulative_SOC_deviation: 21.2401 Fuel Consumption: 54.0041\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -312.73059385690317 Explore P: 0.1434 SOC: 0.6339 Cumulative_SOC_deviation: 25.6987 Fuel Consumption: 55.7441\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -236.7586123076362 Explore P: 0.1398 SOC: 0.6214 Cumulative_SOC_deviation: 18.2412 Fuel Consumption: 54.3462\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -253.7214104012027 Explore P: 0.1362 SOC: 0.6264 Cumulative_SOC_deviation: 19.9253 Fuel Consumption: 54.4684\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -325.70785525282145 Explore P: 0.1328 SOC: 0.6287 Cumulative_SOC_deviation: 26.9641 Fuel Consumption: 56.0667\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -304.52579179598143 Explore P: 0.1295 SOC: 0.5976 Cumulative_SOC_deviation: 25.1773 Fuel Consumption: 52.7529\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -359.33557973366146 Explore P: 0.1263 SOC: 0.6046 Cumulative_SOC_deviation: 30.6402 Fuel Consumption: 52.9338\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -339.9080323550801 Explore P: 0.1231 SOC: 0.6144 Cumulative_SOC_deviation: 28.5009 Fuel Consumption: 54.8985\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -358.89984028732937 Explore P: 0.1200 SOC: 0.5887 Cumulative_SOC_deviation: 30.6279 Fuel Consumption: 52.6209\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -337.9283160919676 Explore P: 0.1171 SOC: 0.5983 Cumulative_SOC_deviation: 28.4418 Fuel Consumption: 53.5105\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -386.43913345134587 Explore P: 0.1142 SOC: 0.6106 Cumulative_SOC_deviation: 33.2137 Fuel Consumption: 54.3023\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -410.67431982874155 Explore P: 0.1113 SOC: 0.5996 Cumulative_SOC_deviation: 35.7780 Fuel Consumption: 52.8947\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -361.4814222539754 Explore P: 0.1086 SOC: 0.5905 Cumulative_SOC_deviation: 30.9501 Fuel Consumption: 51.9807\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -342.48478696419147 Explore P: 0.1059 SOC: 0.5935 Cumulative_SOC_deviation: 29.0191 Fuel Consumption: 52.2935\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -403.79528323789145 Explore P: 0.1033 SOC: 0.5836 Cumulative_SOC_deviation: 35.2338 Fuel Consumption: 51.4577\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -390.61028792164814 Explore P: 0.1008 SOC: 0.5914 Cumulative_SOC_deviation: 33.8537 Fuel Consumption: 52.0735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -383.2335430755207 Explore P: 0.0983 SOC: 0.5935 Cumulative_SOC_deviation: 33.0710 Fuel Consumption: 52.5232\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -368.5179667339064 Explore P: 0.0960 SOC: 0.5985 Cumulative_SOC_deviation: 31.5364 Fuel Consumption: 53.1537\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -331.37050177126343 Explore P: 0.0936 SOC: 0.5834 Cumulative_SOC_deviation: 27.9571 Fuel Consumption: 51.7998\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -333.3314354756426 Explore P: 0.0914 SOC: 0.5941 Cumulative_SOC_deviation: 28.1189 Fuel Consumption: 52.1420\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -290.7748007125042 Explore P: 0.0892 SOC: 0.6026 Cumulative_SOC_deviation: 23.6805 Fuel Consumption: 53.9699\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -283.32360078659457 Explore P: 0.0870 SOC: 0.6001 Cumulative_SOC_deviation: 23.0097 Fuel Consumption: 53.2270\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -232.5099449870789 Explore P: 0.0849 SOC: 0.6072 Cumulative_SOC_deviation: 17.7841 Fuel Consumption: 54.6688\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -273.0959074068114 Explore P: 0.0829 SOC: 0.6165 Cumulative_SOC_deviation: 22.0177 Fuel Consumption: 52.9191\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -258.90557021878567 Explore P: 0.0809 SOC: 0.6265 Cumulative_SOC_deviation: 20.4139 Fuel Consumption: 54.7666\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -272.36198011940166 Explore P: 0.0790 SOC: 0.6195 Cumulative_SOC_deviation: 21.8925 Fuel Consumption: 53.4372\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -1392.2355012223768 Explore P: 0.0771 SOC: 1.0000 Cumulative_SOC_deviation: 129.0852 Fuel Consumption: 101.3839\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -419.1876772285163 Explore P: 0.0753 SOC: 0.8178 Cumulative_SOC_deviation: 35.1352 Fuel Consumption: 67.8359\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -825.4597792217737 Explore P: 0.0735 SOC: 1.0000 Cumulative_SOC_deviation: 73.8709 Fuel Consumption: 86.7506\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -258.760241651681 Explore P: 0.0718 SOC: 0.7386 Cumulative_SOC_deviation: 19.6814 Fuel Consumption: 61.9464\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -228.86491942737908 Explore P: 0.0701 SOC: 0.6244 Cumulative_SOC_deviation: 17.5374 Fuel Consumption: 53.4909\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -2498.9297311574214 Explore P: 0.0685 SOC: 1.0000 Cumulative_SOC_deviation: 238.9017 Fuel Consumption: 109.9127\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -293.97488916151207 Explore P: 0.0669 SOC: 0.6537 Cumulative_SOC_deviation: 23.8026 Fuel Consumption: 55.9494\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -324.82520064807994 Explore P: 0.0654 SOC: 0.6388 Cumulative_SOC_deviation: 27.0337 Fuel Consumption: 54.4880\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -226.18938315358892 Explore P: 0.0639 SOC: 0.6173 Cumulative_SOC_deviation: 17.3202 Fuel Consumption: 52.9875\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -264.69516834036443 Explore P: 0.0624 SOC: 0.6063 Cumulative_SOC_deviation: 21.3247 Fuel Consumption: 51.4479\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -310.96349520481965 Explore P: 0.0610 SOC: 0.6245 Cumulative_SOC_deviation: 25.7658 Fuel Consumption: 53.3052\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -334.3770365637434 Explore P: 0.0596 SOC: 0.6826 Cumulative_SOC_deviation: 27.7416 Fuel Consumption: 56.9611\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -978.143070282718 Explore P: 0.0583 SOC: 1.0000 Cumulative_SOC_deviation: 88.9667 Fuel Consumption: 88.4759\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -299.2308785740647 Explore P: 0.0570 SOC: 0.6769 Cumulative_SOC_deviation: 24.4015 Fuel Consumption: 55.2162\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -433.3142285633567 Explore P: 0.0557 SOC: 0.7909 Cumulative_SOC_deviation: 36.7961 Fuel Consumption: 65.3530\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -198.24019506483583 Explore P: 0.0545 SOC: 0.6004 Cumulative_SOC_deviation: 14.7638 Fuel Consumption: 50.6024\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -448.04416463111403 Explore P: 0.0533 SOC: 0.8261 Cumulative_SOC_deviation: 38.1494 Fuel Consumption: 66.5504\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -168.15802617869966 Explore P: 0.0521 SOC: 0.5999 Cumulative_SOC_deviation: 11.6715 Fuel Consumption: 51.4431\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -1075.6902176287451 Explore P: 0.0510 SOC: 1.0000 Cumulative_SOC_deviation: 99.2376 Fuel Consumption: 83.3147\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -164.7580299472507 Explore P: 0.0498 SOC: 0.6003 Cumulative_SOC_deviation: 11.3765 Fuel Consumption: 50.9934\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -275.4859545974576 Explore P: 0.0488 SOC: 0.6689 Cumulative_SOC_deviation: 21.9952 Fuel Consumption: 55.5339\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -143.9089551945818 Explore P: 0.0477 SOC: 0.6126 Cumulative_SOC_deviation: 9.4049 Fuel Consumption: 49.8595\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -1028.5118779697593 Explore P: 0.0467 SOC: 0.9898 Cumulative_SOC_deviation: 95.0759 Fuel Consumption: 77.7527\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -168.29691078677115 Explore P: 0.0457 SOC: 0.6199 Cumulative_SOC_deviation: 11.7963 Fuel Consumption: 50.3336\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -177.427687473091 Explore P: 0.0447 SOC: 0.6084 Cumulative_SOC_deviation: 12.6723 Fuel Consumption: 50.7050\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -148.14393206306468 Explore P: 0.0438 SOC: 0.6045 Cumulative_SOC_deviation: 9.8881 Fuel Consumption: 49.2632\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -159.683540189532 Explore P: 0.0429 SOC: 0.6046 Cumulative_SOC_deviation: 10.9472 Fuel Consumption: 50.2114\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -140.24654168195926 Explore P: 0.0420 SOC: 0.6059 Cumulative_SOC_deviation: 8.9307 Fuel Consumption: 50.9396\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -150.9249918185538 Explore P: 0.0411 SOC: 0.6076 Cumulative_SOC_deviation: 10.0684 Fuel Consumption: 50.2407\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -150.9859851187483 Explore P: 0.0403 SOC: 0.6054 Cumulative_SOC_deviation: 10.0608 Fuel Consumption: 50.3776\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -148.66758824031294 Explore P: 0.0395 SOC: 0.6020 Cumulative_SOC_deviation: 9.8207 Fuel Consumption: 50.4607\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -146.1049046565133 Explore P: 0.0387 SOC: 0.6069 Cumulative_SOC_deviation: 9.5757 Fuel Consumption: 50.3482\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -149.0118042663414 Explore P: 0.0379 SOC: 0.6063 Cumulative_SOC_deviation: 9.8849 Fuel Consumption: 50.1625\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -148.71791524289483 Explore P: 0.0371 SOC: 0.6064 Cumulative_SOC_deviation: 9.8580 Fuel Consumption: 50.1381\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -156.92867229601717 Explore P: 0.0364 SOC: 0.6083 Cumulative_SOC_deviation: 10.6820 Fuel Consumption: 50.1088\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -177.32179171878985 Explore P: 0.0357 SOC: 0.6069 Cumulative_SOC_deviation: 12.5659 Fuel Consumption: 51.6630\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -146.76909437773529 Explore P: 0.0350 SOC: 0.6068 Cumulative_SOC_deviation: 9.5619 Fuel Consumption: 51.1498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -182.31498248726652 Explore P: 0.0343 SOC: 0.6125 Cumulative_SOC_deviation: 13.0672 Fuel Consumption: 51.6434\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -145.35956421714766 Explore P: 0.0336 SOC: 0.6124 Cumulative_SOC_deviation: 9.4923 Fuel Consumption: 50.4362\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -229.15136351586693 Explore P: 0.0330 SOC: 0.6492 Cumulative_SOC_deviation: 17.6281 Fuel Consumption: 52.8702\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -1300.4515524192593 Explore P: 0.0324 SOC: 0.9999 Cumulative_SOC_deviation: 121.3232 Fuel Consumption: 87.2198\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -139.33668403939996 Explore P: 0.0318 SOC: 0.6061 Cumulative_SOC_deviation: 8.9463 Fuel Consumption: 49.8742\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -127.69046535861955 Explore P: 0.0312 SOC: 0.6093 Cumulative_SOC_deviation: 7.8344 Fuel Consumption: 49.3463\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -128.4417628742355 Explore P: 0.0306 SOC: 0.6124 Cumulative_SOC_deviation: 7.8724 Fuel Consumption: 49.7178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -159.5654017964351 Explore P: 0.0301 SOC: 0.6108 Cumulative_SOC_deviation: 10.9202 Fuel Consumption: 50.3629\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -145.828262049448 Explore P: 0.0295 SOC: 0.6084 Cumulative_SOC_deviation: 9.6330 Fuel Consumption: 49.4978\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -136.31960956386592 Explore P: 0.0290 SOC: 0.6052 Cumulative_SOC_deviation: 8.6807 Fuel Consumption: 49.5125\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -134.92804093748524 Explore P: 0.0285 SOC: 0.6096 Cumulative_SOC_deviation: 8.4179 Fuel Consumption: 50.7490\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -151.33774741251455 Explore P: 0.0280 SOC: 0.6109 Cumulative_SOC_deviation: 10.1209 Fuel Consumption: 50.1283\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -322.8235795131371 Explore P: 0.0275 SOC: 0.6077 Cumulative_SOC_deviation: 27.2583 Fuel Consumption: 50.2407\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -138.9142611958661 Explore P: 0.0270 SOC: 0.6055 Cumulative_SOC_deviation: 8.9294 Fuel Consumption: 49.6200\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -159.0303514496312 Explore P: 0.0265 SOC: 0.6141 Cumulative_SOC_deviation: 10.8228 Fuel Consumption: 50.8028\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -159.17654182896112 Explore P: 0.0261 SOC: 0.6118 Cumulative_SOC_deviation: 10.8379 Fuel Consumption: 50.7979\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -1386.4871465293472 Explore P: 0.0257 SOC: 1.0000 Cumulative_SOC_deviation: 129.6095 Fuel Consumption: 90.3919\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -1748.8100596286167 Explore P: 0.0252 SOC: 1.0000 Cumulative_SOC_deviation: 166.6233 Fuel Consumption: 82.5767\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -170.35218605676604 Explore P: 0.0248 SOC: 0.6065 Cumulative_SOC_deviation: 12.0659 Fuel Consumption: 49.6933\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -2661.293758354022 Explore P: 0.0244 SOC: 1.0000 Cumulative_SOC_deviation: 256.2715 Fuel Consumption: 98.5785\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -164.60751110714816 Explore P: 0.0240 SOC: 0.6083 Cumulative_SOC_deviation: 11.5134 Fuel Consumption: 49.4734\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -154.13192954443969 Explore P: 0.0237 SOC: 0.6107 Cumulative_SOC_deviation: 10.4747 Fuel Consumption: 49.3854\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -151.46478191588366 Explore P: 0.0233 SOC: 0.6105 Cumulative_SOC_deviation: 10.1014 Fuel Consumption: 50.4509\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -139.5563319088654 Explore P: 0.0229 SOC: 0.6084 Cumulative_SOC_deviation: 8.9536 Fuel Consumption: 50.0208\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -172.54282751033585 Explore P: 0.0226 SOC: 0.6157 Cumulative_SOC_deviation: 12.2048 Fuel Consumption: 50.4949\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -837.9650761866732 Explore P: 0.0222 SOC: 0.8537 Cumulative_SOC_deviation: 77.0916 Fuel Consumption: 67.0490\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -152.559199384964 Explore P: 0.0219 SOC: 0.6013 Cumulative_SOC_deviation: 10.2783 Fuel Consumption: 49.7764\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -162.42064148928216 Explore P: 0.0216 SOC: 0.6072 Cumulative_SOC_deviation: 11.3162 Fuel Consumption: 49.2583\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -147.52151555967757 Explore P: 0.0213 SOC: 0.6097 Cumulative_SOC_deviation: 9.8649 Fuel Consumption: 48.8722\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -143.21294979735907 Explore P: 0.0210 SOC: 0.6048 Cumulative_SOC_deviation: 9.3178 Fuel Consumption: 50.0354\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -174.05630832197738 Explore P: 0.0207 SOC: 0.6024 Cumulative_SOC_deviation: 12.4280 Fuel Consumption: 49.7764\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -166.34090240384006 Explore P: 0.0204 SOC: 0.6052 Cumulative_SOC_deviation: 11.7420 Fuel Consumption: 48.9211\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -174.54062053218786 Explore P: 0.0201 SOC: 0.6076 Cumulative_SOC_deviation: 12.5023 Fuel Consumption: 49.5174\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -908.486267893131 Explore P: 0.0198 SOC: 0.7920 Cumulative_SOC_deviation: 84.7200 Fuel Consumption: 61.2866\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -171.66941599827373 Explore P: 0.0196 SOC: 0.6101 Cumulative_SOC_deviation: 12.2445 Fuel Consumption: 49.2241\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -198.75915311207584 Explore P: 0.0193 SOC: 0.6133 Cumulative_SOC_deviation: 14.9188 Fuel Consumption: 49.5711\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -202.2265872850905 Explore P: 0.0190 SOC: 0.6072 Cumulative_SOC_deviation: 15.2836 Fuel Consumption: 49.3903\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -181.70307427183587 Explore P: 0.0188 SOC: 0.6083 Cumulative_SOC_deviation: 13.1946 Fuel Consumption: 49.7569\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -192.43668385228366 Explore P: 0.0186 SOC: 0.6134 Cumulative_SOC_deviation: 14.2944 Fuel Consumption: 49.4929\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -195.81321095437352 Explore P: 0.0183 SOC: 0.6022 Cumulative_SOC_deviation: 14.6560 Fuel Consumption: 49.2534\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -177.9535178422168 Explore P: 0.0181 SOC: 0.6021 Cumulative_SOC_deviation: 12.9643 Fuel Consumption: 48.3101\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -181.3500228189361 Explore P: 0.0179 SOC: 0.6062 Cumulative_SOC_deviation: 13.1916 Fuel Consumption: 49.4343\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -243.15511831204424 Explore P: 0.0177 SOC: 0.6165 Cumulative_SOC_deviation: 19.3496 Fuel Consumption: 49.6591\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -864.0325728452224 Explore P: 0.0175 SOC: 1.0000 Cumulative_SOC_deviation: 77.8582 Fuel Consumption: 85.4506\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -207.06446309961134 Explore P: 0.0173 SOC: 0.6146 Cumulative_SOC_deviation: 15.6257 Fuel Consumption: 50.8077\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -4981.896575363013 Explore P: 0.0171 SOC: 1.0000 Cumulative_SOC_deviation: 480.9282 Fuel Consumption: 172.6149\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -296.04000213855295 Explore P: 0.0169 SOC: 0.6064 Cumulative_SOC_deviation: 24.8082 Fuel Consumption: 47.9582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -420.5147457430037 Explore P: 0.0167 SOC: 0.6532 Cumulative_SOC_deviation: 36.7820 Fuel Consumption: 52.6943\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -215.55183021175972 Explore P: 0.0165 SOC: 0.6067 Cumulative_SOC_deviation: 16.7080 Fuel Consumption: 48.4714\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -224.01085924278075 Explore P: 0.0163 SOC: 0.6097 Cumulative_SOC_deviation: 17.5662 Fuel Consumption: 48.3492\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -500.45576618819297 Explore P: 0.0162 SOC: 0.6961 Cumulative_SOC_deviation: 44.5919 Fuel Consumption: 54.5369\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -395.507518856945 Explore P: 0.0160 SOC: 0.6253 Cumulative_SOC_deviation: 34.5013 Fuel Consumption: 50.4949\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -179.82515247139224 Explore P: 0.0158 SOC: 0.6070 Cumulative_SOC_deviation: 13.0464 Fuel Consumption: 49.3610\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -128.70834760953906 Explore P: 0.0157 SOC: 0.6104 Cumulative_SOC_deviation: 7.9929 Fuel Consumption: 48.7793\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -151.7415845372321 Explore P: 0.0155 SOC: 0.6154 Cumulative_SOC_deviation: 10.2654 Fuel Consumption: 49.0873\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -152.03617576424384 Explore P: 0.0154 SOC: 0.6115 Cumulative_SOC_deviation: 10.4141 Fuel Consumption: 47.8947\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -120.86246563942076 Explore P: 0.0152 SOC: 0.6100 Cumulative_SOC_deviation: 7.4023 Fuel Consumption: 46.8390\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -150.6640440541251 Explore P: 0.0151 SOC: 0.6113 Cumulative_SOC_deviation: 10.2794 Fuel Consumption: 47.8703\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -162.4259951156376 Explore P: 0.0149 SOC: 0.6112 Cumulative_SOC_deviation: 11.4121 Fuel Consumption: 48.3053\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -2754.942893924881 Explore P: 0.0148 SOC: 0.9987 Cumulative_SOC_deviation: 267.3515 Fuel Consumption: 81.4281\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -155.2851926187326 Explore P: 0.0147 SOC: 0.6047 Cumulative_SOC_deviation: 10.8456 Fuel Consumption: 46.8292\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -148.987621888288 Explore P: 0.0146 SOC: 0.6085 Cumulative_SOC_deviation: 10.1161 Fuel Consumption: 47.8263\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -138.2793534902468 Explore P: 0.0144 SOC: 0.6008 Cumulative_SOC_deviation: 9.0649 Fuel Consumption: 47.6308\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -131.71592494901236 Explore P: 0.0143 SOC: 0.6053 Cumulative_SOC_deviation: 8.4838 Fuel Consumption: 46.8781\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -138.80519887146153 Explore P: 0.0142 SOC: 0.6135 Cumulative_SOC_deviation: 8.9762 Fuel Consumption: 49.0433\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -123.64894563898599 Explore P: 0.0141 SOC: 0.6028 Cumulative_SOC_deviation: 7.6439 Fuel Consumption: 47.2104\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN3_3_mass1200.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"DDQN3_3.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

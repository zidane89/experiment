{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN3_3 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "        self.power_mean = 0 \n",
    "        self.power_std = 0 \n",
    "        self.sum = 0 \n",
    "        self.sum_deviation = 0 \n",
    "        self.N = 0 \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self.N += 1 \n",
    "        power = sample[0][0]\n",
    "        self.sum += power \n",
    "        self.power_mean = self.sum / self.N \n",
    "        self.sum_deviation += (power - self.power_mean) ** 2\n",
    "        self.power_std = np.sqrt(self.sum_deviation / self.N)\n",
    "        \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    next_power_states = np.array([0 if val[3] is None else (val[3][0] - memory.power_mean) / memory.power_std  \n",
    "                        for val in batch])\n",
    "    \n",
    "    states[:, 0] = (states[:, 0] - memory.power_mean) / memory.power_std \n",
    "    next_states[:, 0] = next_power_states\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 2\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -4910.2781245915985 Explore P: 0.9217 SOC: 1.0000 Cumulative_SOC_deviation: 478.8516 Fuel Consumption: 121.7618\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -4741.754738211532 Explore P: 0.8970 SOC: 1.0000 Cumulative_SOC_deviation: 461.8693 Fuel Consumption: 123.0619\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -4863.694785130036 Explore P: 0.8730 SOC: 1.0000 Cumulative_SOC_deviation: 474.1454 Fuel Consumption: 122.2408\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -4837.136759178958 Explore P: 0.8496 SOC: 1.0000 Cumulative_SOC_deviation: 472.0155 Fuel Consumption: 116.9818\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -4844.392461665168 Explore P: 0.8269 SOC: 1.0000 Cumulative_SOC_deviation: 472.5671 Fuel Consumption: 118.7217\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -4830.634860765366 Explore P: 0.8048 SOC: 1.0000 Cumulative_SOC_deviation: 470.9802 Fuel Consumption: 120.8331\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -4774.994818474948 Explore P: 0.7832 SOC: 1.0000 Cumulative_SOC_deviation: 465.5784 Fuel Consumption: 119.2105\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -4835.880166970008 Explore P: 0.7623 SOC: 1.0000 Cumulative_SOC_deviation: 471.4988 Fuel Consumption: 120.8918\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -4807.258050023422 Explore P: 0.7419 SOC: 1.0000 Cumulative_SOC_deviation: 468.2339 Fuel Consumption: 124.9191\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -4628.684163635048 Explore P: 0.7221 SOC: 1.0000 Cumulative_SOC_deviation: 450.3765 Fuel Consumption: 124.9191\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -4581.451801573748 Explore P: 0.7028 SOC: 1.0000 Cumulative_SOC_deviation: 445.5506 Fuel Consumption: 125.9455\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -4600.4785740201805 Explore P: 0.6840 SOC: 1.0000 Cumulative_SOC_deviation: 448.5393 Fuel Consumption: 115.0854\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -4587.753634461183 Explore P: 0.6658 SOC: 1.0000 Cumulative_SOC_deviation: 446.2356 Fuel Consumption: 125.3981\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -4371.58511246074 Explore P: 0.6480 SOC: 1.0000 Cumulative_SOC_deviation: 426.0244 Fuel Consumption: 111.3415\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -4537.230512167625 Explore P: 0.6307 SOC: 1.0000 Cumulative_SOC_deviation: 441.9525 Fuel Consumption: 117.7051\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -4094.944708687657 Explore P: 0.6139 SOC: 1.0000 Cumulative_SOC_deviation: 398.7171 Fuel Consumption: 107.7736\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -4011.2501677550867 Explore P: 0.5976 SOC: 1.0000 Cumulative_SOC_deviation: 390.4464 Fuel Consumption: 106.7864\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -4350.510200793465 Explore P: 0.5816 SOC: 1.0000 Cumulative_SOC_deviation: 423.8201 Fuel Consumption: 112.3093\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -3874.7225815714064 Explore P: 0.5662 SOC: 1.0000 Cumulative_SOC_deviation: 377.5962 Fuel Consumption: 98.7610\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -4099.865473334983 Explore P: 0.5511 SOC: 1.0000 Cumulative_SOC_deviation: 400.0723 Fuel Consumption: 99.1422\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -3957.425268057789 Explore P: 0.5364 SOC: 1.0000 Cumulative_SOC_deviation: 385.6152 Fuel Consumption: 101.2732\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -3657.687248243749 Explore P: 0.5222 SOC: 1.0000 Cumulative_SOC_deviation: 355.6570 Fuel Consumption: 101.1168\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -3918.876238254373 Explore P: 0.5083 SOC: 1.0000 Cumulative_SOC_deviation: 380.0497 Fuel Consumption: 118.3796\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -3635.3153679141305 Explore P: 0.4948 SOC: 1.0000 Cumulative_SOC_deviation: 353.9438 Fuel Consumption: 95.8774\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -3098.15407145007 Explore P: 0.4817 SOC: 1.0000 Cumulative_SOC_deviation: 301.2179 Fuel Consumption: 85.9752\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -3377.0145052214907 Explore P: 0.4689 SOC: 1.0000 Cumulative_SOC_deviation: 329.1460 Fuel Consumption: 85.5549\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -3912.8875914977907 Explore P: 0.4565 SOC: 1.0000 Cumulative_SOC_deviation: 382.0099 Fuel Consumption: 92.7884\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -2838.670435914418 Explore P: 0.4444 SOC: 1.0000 Cumulative_SOC_deviation: 275.5647 Fuel Consumption: 83.0231\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -3489.7641819456867 Explore P: 0.4326 SOC: 1.0000 Cumulative_SOC_deviation: 339.0172 Fuel Consumption: 99.5919\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -3328.2603602558775 Explore P: 0.4212 SOC: 1.0000 Cumulative_SOC_deviation: 324.3966 Fuel Consumption: 84.2939\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -2618.426664357504 Explore P: 0.4100 SOC: 1.0000 Cumulative_SOC_deviation: 253.3204 Fuel Consumption: 85.2225\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -3208.6781350778933 Explore P: 0.3992 SOC: 1.0000 Cumulative_SOC_deviation: 311.4531 Fuel Consumption: 94.1472\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -2512.3025309729096 Explore P: 0.3887 SOC: 1.0000 Cumulative_SOC_deviation: 242.6630 Fuel Consumption: 85.6722\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -2056.61806392792 Explore P: 0.3784 SOC: 1.0000 Cumulative_SOC_deviation: 198.1513 Fuel Consumption: 75.1053\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -2940.8120761125506 Explore P: 0.3684 SOC: 1.0000 Cumulative_SOC_deviation: 285.6342 Fuel Consumption: 84.4699\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -2072.4229845764194 Explore P: 0.3587 SOC: 1.0000 Cumulative_SOC_deviation: 199.7679 Fuel Consumption: 74.7437\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -2959.224233465578 Explore P: 0.3493 SOC: 1.0000 Cumulative_SOC_deviation: 287.6690 Fuel Consumption: 82.5344\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -2458.35254323116 Explore P: 0.3401 SOC: 1.0000 Cumulative_SOC_deviation: 237.7617 Fuel Consumption: 80.7358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -2092.989857615888 Explore P: 0.3311 SOC: 1.0000 Cumulative_SOC_deviation: 200.8588 Fuel Consumption: 84.4014\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -2250.8055840698344 Explore P: 0.3224 SOC: 1.0000 Cumulative_SOC_deviation: 217.1810 Fuel Consumption: 78.9958\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -2545.2187539121655 Explore P: 0.3140 SOC: 1.0000 Cumulative_SOC_deviation: 245.9556 Fuel Consumption: 85.6624\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -2029.5551069869211 Explore P: 0.3057 SOC: 1.0000 Cumulative_SOC_deviation: 194.2593 Fuel Consumption: 86.9625\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -2307.031086044395 Explore P: 0.2977 SOC: 1.0000 Cumulative_SOC_deviation: 221.0352 Fuel Consumption: 96.6789\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -987.2299349387964 Explore P: 0.2899 SOC: 0.8203 Cumulative_SOC_deviation: 92.8019 Fuel Consumption: 59.2111\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -1178.3998129476352 Explore P: 0.2824 SOC: 0.8764 Cumulative_SOC_deviation: 111.5191 Fuel Consumption: 63.2091\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -1550.331713243237 Explore P: 0.2750 SOC: 1.0000 Cumulative_SOC_deviation: 146.6204 Fuel Consumption: 84.1277\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -816.9250422753719 Explore P: 0.2678 SOC: 0.7974 Cumulative_SOC_deviation: 76.0187 Fuel Consumption: 56.7380\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -905.1181127531784 Explore P: 0.2608 SOC: 0.7636 Cumulative_SOC_deviation: 84.9328 Fuel Consumption: 55.7898\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -1100.1976192073087 Explore P: 0.2540 SOC: 0.9517 Cumulative_SOC_deviation: 103.1290 Fuel Consumption: 68.9079\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -922.6261556645875 Explore P: 0.2474 SOC: 0.7553 Cumulative_SOC_deviation: 86.8293 Fuel Consumption: 54.3333\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -1185.3067117530811 Explore P: 0.2410 SOC: 0.9476 Cumulative_SOC_deviation: 111.7591 Fuel Consumption: 67.7154\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -903.1310954733179 Explore P: 0.2347 SOC: 0.8801 Cumulative_SOC_deviation: 84.0362 Fuel Consumption: 62.7692\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -827.4487678234782 Explore P: 0.2286 SOC: 0.7919 Cumulative_SOC_deviation: 77.1219 Fuel Consumption: 56.2297\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -693.3411298240313 Explore P: 0.2227 SOC: 0.8183 Cumulative_SOC_deviation: 63.5469 Fuel Consumption: 57.8719\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -1192.9735775880247 Explore P: 0.2170 SOC: 0.9545 Cumulative_SOC_deviation: 112.5033 Fuel Consumption: 67.9402\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -538.2805772266858 Explore P: 0.2114 SOC: 0.7269 Cumulative_SOC_deviation: 48.6792 Fuel Consumption: 51.4887\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -2373.055813837886 Explore P: 0.2059 SOC: 1.0000 Cumulative_SOC_deviation: 228.8752 Fuel Consumption: 84.3037\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -1514.9855215139569 Explore P: 0.2006 SOC: 1.0000 Cumulative_SOC_deviation: 143.6557 Fuel Consumption: 78.4289\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -2815.9680172145113 Explore P: 0.1954 SOC: 1.0000 Cumulative_SOC_deviation: 271.8595 Fuel Consumption: 97.3730\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -891.0243093562683 Explore P: 0.1904 SOC: 0.7898 Cumulative_SOC_deviation: 83.6153 Fuel Consumption: 54.8709\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -564.1821663631118 Explore P: 0.1855 SOC: 0.7003 Cumulative_SOC_deviation: 51.5372 Fuel Consumption: 48.8104\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -604.2288796233272 Explore P: 0.1808 SOC: 0.7390 Cumulative_SOC_deviation: 55.2642 Fuel Consumption: 51.5865\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -905.087025606079 Explore P: 0.1761 SOC: 0.8678 Cumulative_SOC_deviation: 84.3119 Fuel Consumption: 61.9676\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -511.3335187154097 Explore P: 0.1716 SOC: 0.6888 Cumulative_SOC_deviation: 46.2240 Fuel Consumption: 49.0938\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -546.4365729368011 Explore P: 0.1673 SOC: 0.7039 Cumulative_SOC_deviation: 49.6561 Fuel Consumption: 49.8759\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -453.29618918303197 Explore P: 0.1630 SOC: 0.6734 Cumulative_SOC_deviation: 40.5923 Fuel Consumption: 47.3734\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -486.282336877609 Explore P: 0.1589 SOC: 0.6863 Cumulative_SOC_deviation: 43.7912 Fuel Consumption: 48.3705\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -498.3716150477745 Explore P: 0.1548 SOC: 0.7229 Cumulative_SOC_deviation: 44.7196 Fuel Consumption: 51.1759\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -853.5077638047176 Explore P: 0.1509 SOC: 0.8652 Cumulative_SOC_deviation: 79.3212 Fuel Consumption: 60.2961\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -537.0871026565281 Explore P: 0.1471 SOC: 0.7749 Cumulative_SOC_deviation: 48.2500 Fuel Consumption: 54.5874\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -3169.3438182649365 Explore P: 0.1434 SOC: 1.0000 Cumulative_SOC_deviation: 306.7005 Fuel Consumption: 102.3387\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -407.8136349710028 Explore P: 0.1398 SOC: 0.6544 Cumulative_SOC_deviation: 36.2513 Fuel Consumption: 45.3011\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -372.18972343163284 Explore P: 0.1362 SOC: 0.6523 Cumulative_SOC_deviation: 32.6204 Fuel Consumption: 45.9854\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -309.94372799498865 Explore P: 0.1328 SOC: 0.6494 Cumulative_SOC_deviation: 26.4770 Fuel Consumption: 45.1740\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -356.3898703928673 Explore P: 0.1295 SOC: 0.6511 Cumulative_SOC_deviation: 31.0874 Fuel Consumption: 45.5162\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -454.0337875988334 Explore P: 0.1263 SOC: 0.6838 Cumulative_SOC_deviation: 40.5996 Fuel Consumption: 48.0381\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -397.8507872787484 Explore P: 0.1231 SOC: 0.6735 Cumulative_SOC_deviation: 35.0037 Fuel Consumption: 47.8133\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -332.17451198865535 Explore P: 0.1200 SOC: 0.6627 Cumulative_SOC_deviation: 28.6189 Fuel Consumption: 45.9854\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -295.3692866727382 Explore P: 0.1171 SOC: 0.6578 Cumulative_SOC_deviation: 24.9765 Fuel Consumption: 45.6041\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -340.02720903949904 Explore P: 0.1142 SOC: 0.6482 Cumulative_SOC_deviation: 29.4570 Fuel Consumption: 45.4575\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -559.5959548492826 Explore P: 0.1113 SOC: 0.7811 Cumulative_SOC_deviation: 50.4451 Fuel Consumption: 55.1446\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -2929.2328848831016 Explore P: 0.1086 SOC: 1.0000 Cumulative_SOC_deviation: 283.2632 Fuel Consumption: 96.6007\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -282.779340864463 Explore P: 0.1059 SOC: 0.6389 Cumulative_SOC_deviation: 23.8466 Fuel Consumption: 44.3138\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -304.1137303647661 Explore P: 0.1033 SOC: 0.6377 Cumulative_SOC_deviation: 25.9458 Fuel Consumption: 44.6560\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -637.5068828502053 Explore P: 0.1008 SOC: 0.8543 Cumulative_SOC_deviation: 57.7660 Fuel Consumption: 59.8464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -332.06962862912906 Explore P: 0.0983 SOC: 0.6297 Cumulative_SOC_deviation: 28.7042 Fuel Consumption: 45.0274\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -388.27543870403696 Explore P: 0.0960 SOC: 0.6313 Cumulative_SOC_deviation: 34.4675 Fuel Consumption: 43.6003\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -357.402350319889 Explore P: 0.0936 SOC: 0.6339 Cumulative_SOC_deviation: 31.3607 Fuel Consumption: 43.7958\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -368.454966677906 Explore P: 0.0914 SOC: 0.6361 Cumulative_SOC_deviation: 32.3496 Fuel Consumption: 44.9590\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -293.5456623199981 Explore P: 0.0892 SOC: 0.6208 Cumulative_SOC_deviation: 25.0004 Fuel Consumption: 43.5416\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -373.62908054390493 Explore P: 0.0870 SOC: 0.6355 Cumulative_SOC_deviation: 32.9657 Fuel Consumption: 43.9717\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -274.61613327345816 Explore P: 0.0849 SOC: 0.6058 Cumulative_SOC_deviation: 23.3352 Fuel Consumption: 41.2640\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -387.83699438181117 Explore P: 0.0829 SOC: 0.6183 Cumulative_SOC_deviation: 34.5273 Fuel Consumption: 42.5641\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -316.7749183454986 Explore P: 0.0809 SOC: 0.6477 Cumulative_SOC_deviation: 27.2187 Fuel Consumption: 44.5875\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -713.363368228547 Explore P: 0.0790 SOC: 0.7684 Cumulative_SOC_deviation: 65.8835 Fuel Consumption: 54.5288\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -571.4803519076144 Explore P: 0.0771 SOC: 0.7191 Cumulative_SOC_deviation: 52.0969 Fuel Consumption: 50.5112\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -459.87369194642855 Explore P: 0.0753 SOC: 0.6219 Cumulative_SOC_deviation: 41.6879 Fuel Consumption: 42.9942\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -278.6651752606504 Explore P: 0.0735 SOC: 0.6192 Cumulative_SOC_deviation: 23.5388 Fuel Consumption: 43.2777\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -371.1227249443002 Explore P: 0.0718 SOC: 0.6041 Cumulative_SOC_deviation: 32.9565 Fuel Consumption: 41.5573\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -421.34043902134385 Explore P: 0.0701 SOC: 0.6023 Cumulative_SOC_deviation: 37.9509 Fuel Consumption: 41.8310\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -404.25327271811545 Explore P: 0.0685 SOC: 0.6118 Cumulative_SOC_deviation: 36.1816 Fuel Consumption: 42.4370\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -483.11870557046495 Explore P: 0.0669 SOC: 0.6224 Cumulative_SOC_deviation: 44.0330 Fuel Consumption: 42.7889\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -464.310112841585 Explore P: 0.0654 SOC: 0.7481 Cumulative_SOC_deviation: 41.1883 Fuel Consumption: 52.4271\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -456.21709128444724 Explore P: 0.0639 SOC: 0.7676 Cumulative_SOC_deviation: 40.3536 Fuel Consumption: 52.6813\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -1712.9632868731999 Explore P: 0.0624 SOC: 0.9940 Cumulative_SOC_deviation: 164.4075 Fuel Consumption: 68.8884\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -307.21688235471595 Explore P: 0.0610 SOC: 0.7098 Cumulative_SOC_deviation: 25.8993 Fuel Consumption: 48.2239\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -223.6866541226848 Explore P: 0.0596 SOC: 0.6625 Cumulative_SOC_deviation: 17.8571 Fuel Consumption: 45.1154\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -253.6359767475709 Explore P: 0.0583 SOC: 0.6347 Cumulative_SOC_deviation: 21.1072 Fuel Consumption: 42.5641\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -270.84089659552393 Explore P: 0.0570 SOC: 0.6741 Cumulative_SOC_deviation: 22.5618 Fuel Consumption: 45.2229\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -450.02007093789587 Explore P: 0.0557 SOC: 0.7630 Cumulative_SOC_deviation: 39.7593 Fuel Consumption: 52.4271\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -340.6411903817472 Explore P: 0.0545 SOC: 0.7515 Cumulative_SOC_deviation: 28.8664 Fuel Consumption: 51.9775\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -305.97027365017715 Explore P: 0.0533 SOC: 0.6972 Cumulative_SOC_deviation: 25.7707 Fuel Consumption: 48.2630\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -641.24317091013 Explore P: 0.0521 SOC: 0.8522 Cumulative_SOC_deviation: 58.2716 Fuel Consumption: 58.5268\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -2386.5791329135373 Explore P: 0.0510 SOC: 1.0000 Cumulative_SOC_deviation: 230.9920 Fuel Consumption: 76.6596\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -4921.146886293111 Explore P: 0.0498 SOC: 1.0000 Cumulative_SOC_deviation: 480.4028 Fuel Consumption: 117.1186\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -2388.9611082954034 Explore P: 0.0488 SOC: 1.0000 Cumulative_SOC_deviation: 230.1256 Fuel Consumption: 87.7054\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -211.08034248252557 Explore P: 0.0477 SOC: 0.6190 Cumulative_SOC_deviation: 16.8800 Fuel Consumption: 42.2806\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -209.42625600895008 Explore P: 0.0467 SOC: 0.6249 Cumulative_SOC_deviation: 16.6882 Fuel Consumption: 42.5445\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -230.5889209006787 Explore P: 0.0457 SOC: 0.6232 Cumulative_SOC_deviation: 18.8328 Fuel Consumption: 42.2611\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -208.07613288623642 Explore P: 0.0447 SOC: 0.6259 Cumulative_SOC_deviation: 16.5434 Fuel Consumption: 42.6423\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -184.28884957083437 Explore P: 0.0438 SOC: 0.6197 Cumulative_SOC_deviation: 14.2038 Fuel Consumption: 42.2513\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -277.68015633092483 Explore P: 0.0429 SOC: 0.6239 Cumulative_SOC_deviation: 23.3777 Fuel Consumption: 43.9033\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -237.86901110088286 Explore P: 0.0420 SOC: 0.6264 Cumulative_SOC_deviation: 19.4670 Fuel Consumption: 43.1995\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -365.0328881695295 Explore P: 0.0411 SOC: 0.6366 Cumulative_SOC_deviation: 32.1198 Fuel Consumption: 43.8349\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -198.58927281605577 Explore P: 0.0403 SOC: 0.6163 Cumulative_SOC_deviation: 15.6553 Fuel Consumption: 42.0362\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -246.5890203169233 Explore P: 0.0395 SOC: 0.6183 Cumulative_SOC_deviation: 20.4191 Fuel Consumption: 42.3979\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -282.49514968183297 Explore P: 0.0387 SOC: 0.6302 Cumulative_SOC_deviation: 23.9433 Fuel Consumption: 43.0626\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -379.46828350089584 Explore P: 0.0379 SOC: 0.7917 Cumulative_SOC_deviation: 32.4705 Fuel Consumption: 54.7634\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -324.99831068618136 Explore P: 0.0371 SOC: 0.6140 Cumulative_SOC_deviation: 28.2737 Fuel Consumption: 42.2611\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -1152.9463449629857 Explore P: 0.0364 SOC: 1.0000 Cumulative_SOC_deviation: 107.9689 Fuel Consumption: 73.2578\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -538.8570478993809 Explore P: 0.0357 SOC: 0.8010 Cumulative_SOC_deviation: 48.3302 Fuel Consumption: 55.5552\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -225.36152789377718 Explore P: 0.0350 SOC: 0.6257 Cumulative_SOC_deviation: 18.2025 Fuel Consumption: 43.3363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -344.9953820142692 Explore P: 0.0343 SOC: 0.6113 Cumulative_SOC_deviation: 30.2627 Fuel Consumption: 42.3686\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -193.51024979715692 Explore P: 0.0336 SOC: 0.6173 Cumulative_SOC_deviation: 15.1474 Fuel Consumption: 42.0362\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -169.1891344837507 Explore P: 0.0330 SOC: 0.6127 Cumulative_SOC_deviation: 12.7710 Fuel Consumption: 41.4791\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -208.97236415482507 Explore P: 0.0324 SOC: 0.6192 Cumulative_SOC_deviation: 16.7474 Fuel Consumption: 41.4986\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -639.9846064597657 Explore P: 0.0318 SOC: 0.8333 Cumulative_SOC_deviation: 58.1849 Fuel Consumption: 58.1358\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -234.153274456194 Explore P: 0.0312 SOC: 0.6307 Cumulative_SOC_deviation: 19.1667 Fuel Consumption: 42.4859\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -248.0186347360128 Explore P: 0.0306 SOC: 0.6562 Cumulative_SOC_deviation: 20.3324 Fuel Consumption: 44.6951\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -307.1670845530258 Explore P: 0.0301 SOC: 0.6824 Cumulative_SOC_deviation: 26.0888 Fuel Consumption: 46.2786\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -3405.7005546758764 Explore P: 0.0295 SOC: 1.0000 Cumulative_SOC_deviation: 331.3059 Fuel Consumption: 92.6418\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -390.88452174879654 Explore P: 0.0290 SOC: 0.6873 Cumulative_SOC_deviation: 34.4518 Fuel Consumption: 46.3666\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -339.95652702251505 Explore P: 0.0285 SOC: 0.6774 Cumulative_SOC_deviation: 29.4206 Fuel Consumption: 45.7508\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -350.3350328411961 Explore P: 0.0280 SOC: 0.6858 Cumulative_SOC_deviation: 30.3157 Fuel Consumption: 47.1779\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -367.22359101744144 Explore P: 0.0275 SOC: 0.6931 Cumulative_SOC_deviation: 31.9909 Fuel Consumption: 47.3148\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -256.918962043545 Explore P: 0.0270 SOC: 0.6598 Cumulative_SOC_deviation: 21.1549 Fuel Consumption: 45.3695\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -379.2505364265577 Explore P: 0.0265 SOC: 0.6439 Cumulative_SOC_deviation: 33.5386 Fuel Consumption: 43.8642\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -266.9372394837521 Explore P: 0.0261 SOC: 0.6666 Cumulative_SOC_deviation: 22.2272 Fuel Consumption: 44.6657\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -241.74416168481375 Explore P: 0.0257 SOC: 0.6731 Cumulative_SOC_deviation: 19.6189 Fuel Consumption: 45.5553\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -297.780229001469 Explore P: 0.0252 SOC: 0.7164 Cumulative_SOC_deviation: 24.9273 Fuel Consumption: 48.5073\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -159.79534263358175 Explore P: 0.0248 SOC: 0.6230 Cumulative_SOC_deviation: 11.7427 Fuel Consumption: 42.3686\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -483.94024566852494 Explore P: 0.0244 SOC: 0.7433 Cumulative_SOC_deviation: 43.3419 Fuel Consumption: 50.5210\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -1004.2816247149976 Explore P: 0.0240 SOC: 0.8294 Cumulative_SOC_deviation: 94.7788 Fuel Consumption: 56.4936\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -341.1971822235815 Explore P: 0.0237 SOC: 0.7031 Cumulative_SOC_deviation: 29.3413 Fuel Consumption: 47.7840\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -182.90110230487116 Explore P: 0.0233 SOC: 0.6293 Cumulative_SOC_deviation: 14.1041 Fuel Consumption: 41.8603\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -197.32109130418763 Explore P: 0.0229 SOC: 0.6330 Cumulative_SOC_deviation: 15.4356 Fuel Consumption: 42.9649\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -194.71641607249896 Explore P: 0.0226 SOC: 0.6246 Cumulative_SOC_deviation: 15.2788 Fuel Consumption: 41.9287\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -513.6061683146969 Explore P: 0.0222 SOC: 0.7176 Cumulative_SOC_deviation: 46.4610 Fuel Consumption: 48.9961\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -143.4163945078668 Explore P: 0.0219 SOC: 0.6383 Cumulative_SOC_deviation: 9.9670 Fuel Consumption: 43.7469\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -194.88118785892019 Explore P: 0.0216 SOC: 0.6242 Cumulative_SOC_deviation: 15.2669 Fuel Consumption: 42.2122\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -152.51682434126838 Explore P: 0.0213 SOC: 0.6148 Cumulative_SOC_deviation: 11.1937 Fuel Consumption: 40.5798\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -155.1911002120003 Explore P: 0.0210 SOC: 0.6157 Cumulative_SOC_deviation: 11.3106 Fuel Consumption: 42.0851\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -880.3594466568925 Explore P: 0.0207 SOC: 0.8989 Cumulative_SOC_deviation: 81.7688 Fuel Consumption: 62.6714\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -233.99797160137038 Explore P: 0.0204 SOC: 0.6555 Cumulative_SOC_deviation: 18.9459 Fuel Consumption: 44.5387\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -158.52495402665133 Explore P: 0.0201 SOC: 0.6305 Cumulative_SOC_deviation: 11.6254 Fuel Consumption: 42.2708\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -157.78758000307894 Explore P: 0.0198 SOC: 0.6508 Cumulative_SOC_deviation: 11.4451 Fuel Consumption: 43.3363\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -143.11614412856514 Explore P: 0.0196 SOC: 0.6144 Cumulative_SOC_deviation: 10.2419 Fuel Consumption: 40.6971\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -172.5495680059518 Explore P: 0.0193 SOC: 0.6085 Cumulative_SOC_deviation: 13.1618 Fuel Consumption: 40.9317\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -127.26352558340633 Explore P: 0.0190 SOC: 0.6176 Cumulative_SOC_deviation: 8.6283 Fuel Consumption: 40.9805\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -163.69884690578562 Explore P: 0.0188 SOC: 0.6236 Cumulative_SOC_deviation: 12.1575 Fuel Consumption: 42.1242\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -177.45484556023862 Explore P: 0.0186 SOC: 0.6085 Cumulative_SOC_deviation: 13.6494 Fuel Consumption: 40.9610\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -152.44680449557322 Explore P: 0.0183 SOC: 0.6243 Cumulative_SOC_deviation: 10.9844 Fuel Consumption: 42.6032\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -179.39543830088053 Explore P: 0.0181 SOC: 0.6475 Cumulative_SOC_deviation: 13.5297 Fuel Consumption: 44.0988\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -674.6821220919261 Explore P: 0.0179 SOC: 0.6347 Cumulative_SOC_deviation: 63.0456 Fuel Consumption: 44.2259\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -118.01136466229667 Explore P: 0.0177 SOC: 0.6148 Cumulative_SOC_deviation: 7.7021 Fuel Consumption: 40.9903\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -107.53382103443558 Explore P: 0.0175 SOC: 0.6122 Cumulative_SOC_deviation: 6.7003 Fuel Consumption: 40.5309\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -114.18199129109531 Explore P: 0.0173 SOC: 0.6054 Cumulative_SOC_deviation: 7.4238 Fuel Consumption: 39.9444\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -114.8368190424052 Explore P: 0.0171 SOC: 0.6146 Cumulative_SOC_deviation: 7.3827 Fuel Consumption: 41.0099\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -127.95398832268802 Explore P: 0.0169 SOC: 0.6085 Cumulative_SOC_deviation: 8.7208 Fuel Consumption: 40.7459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -108.6456279671423 Explore P: 0.0167 SOC: 0.6050 Cumulative_SOC_deviation: 6.8144 Fuel Consumption: 40.5016\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -129.22497684437587 Explore P: 0.0165 SOC: 0.6080 Cumulative_SOC_deviation: 8.7844 Fuel Consumption: 41.3813\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -115.10560641609572 Explore P: 0.0163 SOC: 0.6097 Cumulative_SOC_deviation: 7.4487 Fuel Consumption: 40.6189\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -110.68952811137657 Explore P: 0.0162 SOC: 0.6075 Cumulative_SOC_deviation: 6.9924 Fuel Consumption: 40.7655\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -116.20440130979884 Explore P: 0.0160 SOC: 0.6120 Cumulative_SOC_deviation: 7.4628 Fuel Consumption: 41.5768\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -142.75321871665406 Explore P: 0.0158 SOC: 0.6096 Cumulative_SOC_deviation: 10.1743 Fuel Consumption: 41.0099\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -138.02773850786238 Explore P: 0.0157 SOC: 0.6127 Cumulative_SOC_deviation: 9.6637 Fuel Consumption: 41.3911\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -111.15134396020012 Explore P: 0.0155 SOC: 0.6076 Cumulative_SOC_deviation: 7.0572 Fuel Consumption: 40.5798\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -148.1238022823897 Explore P: 0.0154 SOC: 0.6040 Cumulative_SOC_deviation: 10.7622 Fuel Consumption: 40.5016\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -121.93226086347839 Explore P: 0.0152 SOC: 0.6114 Cumulative_SOC_deviation: 8.1001 Fuel Consumption: 40.9317\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -127.41300693687451 Explore P: 0.0151 SOC: 0.6112 Cumulative_SOC_deviation: 8.6765 Fuel Consumption: 40.6482\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -128.77168742680237 Explore P: 0.0149 SOC: 0.6072 Cumulative_SOC_deviation: 8.8065 Fuel Consumption: 40.7068\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -140.4061814353083 Explore P: 0.0148 SOC: 0.6061 Cumulative_SOC_deviation: 9.9475 Fuel Consumption: 40.9317\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -146.3572967675606 Explore P: 0.0147 SOC: 0.6146 Cumulative_SOC_deviation: 10.5367 Fuel Consumption: 40.9903\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -125.76596624172386 Explore P: 0.0146 SOC: 0.6227 Cumulative_SOC_deviation: 8.4346 Fuel Consumption: 41.4204\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -138.63510964886657 Explore P: 0.0144 SOC: 0.6112 Cumulative_SOC_deviation: 9.7899 Fuel Consumption: 40.7362\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -155.53300447622925 Explore P: 0.0143 SOC: 0.6109 Cumulative_SOC_deviation: 11.5100 Fuel Consumption: 40.4331\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -134.65592253348902 Explore P: 0.0142 SOC: 0.6033 Cumulative_SOC_deviation: 9.4281 Fuel Consumption: 40.3745\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -126.41409910487992 Explore P: 0.0141 SOC: 0.6100 Cumulative_SOC_deviation: 8.6255 Fuel Consumption: 40.1594\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN3_3_ver2.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"DDQN3_3.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

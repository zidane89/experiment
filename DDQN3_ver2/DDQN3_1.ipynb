{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN3_1 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 1\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -4880.758666072782 Explore P: 0.9217 SOC: 1.0000 Cumulative_SOC_deviation: 475.0483 Fuel Consumption: 130.2759\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -4936.938511287538 Explore P: 0.8970 SOC: 1.0000 Cumulative_SOC_deviation: 480.5930 Fuel Consumption: 131.0090\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -4928.808020474126 Explore P: 0.8730 SOC: 1.0000 Cumulative_SOC_deviation: 479.9471 Fuel Consumption: 129.3375\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -4939.28775389381 Explore P: 0.8496 SOC: 1.0000 Cumulative_SOC_deviation: 481.2609 Fuel Consumption: 126.6786\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -4870.821567806998 Explore P: 0.8269 SOC: 1.0000 Cumulative_SOC_deviation: 474.1396 Fuel Consumption: 129.4254\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -4951.829149917341 Explore P: 0.8048 SOC: 1.0000 Cumulative_SOC_deviation: 482.1495 Fuel Consumption: 130.3345\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -4880.210654693505 Explore P: 0.7832 SOC: 1.0000 Cumulative_SOC_deviation: 475.1098 Fuel Consumption: 129.1126\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -4982.87683194943 Explore P: 0.7623 SOC: 1.0000 Cumulative_SOC_deviation: 485.5074 Fuel Consumption: 127.8028\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -4926.467268341912 Explore P: 0.7419 SOC: 1.0000 Cumulative_SOC_deviation: 479.6739 Fuel Consumption: 129.7285\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -4867.8232659431615 Explore P: 0.7221 SOC: 1.0000 Cumulative_SOC_deviation: 473.9972 Fuel Consumption: 127.8517\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -4966.771118052489 Explore P: 0.7028 SOC: 1.0000 Cumulative_SOC_deviation: 483.4941 Fuel Consumption: 131.8301\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -4987.398889073849 Explore P: 0.6840 SOC: 1.0000 Cumulative_SOC_deviation: 484.9821 Fuel Consumption: 137.5779\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -4946.574287256498 Explore P: 0.6658 SOC: 1.0000 Cumulative_SOC_deviation: 481.0727 Fuel Consumption: 135.8477\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -4924.059601800379 Explore P: 0.6480 SOC: 1.0000 Cumulative_SOC_deviation: 479.5690 Fuel Consumption: 128.3697\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -4818.671561064834 Explore P: 0.6307 SOC: 1.0000 Cumulative_SOC_deviation: 469.5659 Fuel Consumption: 123.0130\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -4868.244147973477 Explore P: 0.6139 SOC: 1.0000 Cumulative_SOC_deviation: 474.3413 Fuel Consumption: 124.8312\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -4839.4569422023 Explore P: 0.5976 SOC: 1.0000 Cumulative_SOC_deviation: 471.0315 Fuel Consumption: 129.1420\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -4933.495431870717 Explore P: 0.5816 SOC: 1.0000 Cumulative_SOC_deviation: 480.2203 Fuel Consumption: 131.2925\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -4934.083809826447 Explore P: 0.5662 SOC: 1.0000 Cumulative_SOC_deviation: 480.9546 Fuel Consumption: 124.5379\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -4929.120614630384 Explore P: 0.5511 SOC: 1.0000 Cumulative_SOC_deviation: 479.9744 Fuel Consumption: 129.3766\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -4932.279629643241 Explore P: 0.5364 SOC: 1.0000 Cumulative_SOC_deviation: 480.3450 Fuel Consumption: 128.8292\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -4900.47142372356 Explore P: 0.5222 SOC: 1.0000 Cumulative_SOC_deviation: 477.5591 Fuel Consumption: 124.8800\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -4940.9360704173405 Explore P: 0.5083 SOC: 1.0000 Cumulative_SOC_deviation: 481.2566 Fuel Consumption: 128.3697\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -4931.207614353222 Explore P: 0.4948 SOC: 1.0000 Cumulative_SOC_deviation: 479.6171 Fuel Consumption: 135.0363\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -5003.232750004126 Explore P: 0.4817 SOC: 1.0000 Cumulative_SOC_deviation: 486.5362 Fuel Consumption: 137.8711\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -4930.466628861823 Explore P: 0.4689 SOC: 1.0000 Cumulative_SOC_deviation: 479.7200 Fuel Consumption: 133.2670\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -4863.063179801631 Explore P: 0.4565 SOC: 1.0000 Cumulative_SOC_deviation: 473.2875 Fuel Consumption: 130.1879\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -4918.865025823955 Explore P: 0.4444 SOC: 1.0000 Cumulative_SOC_deviation: 478.5764 Fuel Consumption: 133.1009\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -4832.0803976727775 Explore P: 0.4326 SOC: 1.0000 Cumulative_SOC_deviation: 470.5089 Fuel Consumption: 126.9914\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -4666.127614121857 Explore P: 0.4212 SOC: 1.0000 Cumulative_SOC_deviation: 454.7572 Fuel Consumption: 118.5556\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -4821.913608381333 Explore P: 0.4100 SOC: 1.0000 Cumulative_SOC_deviation: 469.3446 Fuel Consumption: 128.4675\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -4902.235348885703 Explore P: 0.3992 SOC: 1.0000 Cumulative_SOC_deviation: 477.8216 Fuel Consumption: 124.0198\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -4907.815289619466 Explore P: 0.3887 SOC: 1.0000 Cumulative_SOC_deviation: 478.2798 Fuel Consumption: 125.0169\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -4975.214703237713 Explore P: 0.3784 SOC: 1.0000 Cumulative_SOC_deviation: 484.6190 Fuel Consumption: 129.0247\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -4698.826578278535 Explore P: 0.3684 SOC: 1.0000 Cumulative_SOC_deviation: 457.2519 Fuel Consumption: 126.3072\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -5046.716033177347 Explore P: 0.3587 SOC: 1.0000 Cumulative_SOC_deviation: 491.0986 Fuel Consumption: 135.7304\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -4727.107308069274 Explore P: 0.3493 SOC: 1.0000 Cumulative_SOC_deviation: 461.1631 Fuel Consumption: 115.4764\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -4846.930097899868 Explore P: 0.3401 SOC: 1.0000 Cumulative_SOC_deviation: 472.5051 Fuel Consumption: 121.8791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -4826.884198446341 Explore P: 0.3311 SOC: 1.0000 Cumulative_SOC_deviation: 470.5904 Fuel Consumption: 120.9798\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -4762.878586843365 Explore P: 0.3224 SOC: 1.0000 Cumulative_SOC_deviation: 463.6923 Fuel Consumption: 125.9553\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -4943.47034397267 Explore P: 0.3140 SOC: 1.0000 Cumulative_SOC_deviation: 481.3771 Fuel Consumption: 129.6991\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -4808.97463063938 Explore P: 0.3057 SOC: 1.0000 Cumulative_SOC_deviation: 468.5434 Fuel Consumption: 123.5408\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -4917.616170701908 Explore P: 0.2977 SOC: 1.0000 Cumulative_SOC_deviation: 479.0322 Fuel Consumption: 127.2945\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -4558.394655812883 Explore P: 0.2899 SOC: 1.0000 Cumulative_SOC_deviation: 444.4404 Fuel Consumption: 113.9906\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -4869.597712235118 Explore P: 0.2824 SOC: 1.0000 Cumulative_SOC_deviation: 474.3496 Fuel Consumption: 126.1019\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -4843.195604456027 Explore P: 0.2750 SOC: 1.0000 Cumulative_SOC_deviation: 472.3506 Fuel Consumption: 119.6895\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -4866.974990167 Explore P: 0.2678 SOC: 1.0000 Cumulative_SOC_deviation: 473.9886 Fuel Consumption: 127.0892\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -4785.677024964966 Explore P: 0.2608 SOC: 1.0000 Cumulative_SOC_deviation: 466.3896 Fuel Consumption: 121.7813\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -4589.5552728952325 Explore P: 0.2540 SOC: 1.0000 Cumulative_SOC_deviation: 447.1958 Fuel Consumption: 117.5976\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -4947.0349135639835 Explore P: 0.2474 SOC: 1.0000 Cumulative_SOC_deviation: 482.5009 Fuel Consumption: 122.0257\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -4927.107562003668 Explore P: 0.2410 SOC: 1.0000 Cumulative_SOC_deviation: 480.4368 Fuel Consumption: 122.7393\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -4864.698276381135 Explore P: 0.2347 SOC: 1.0000 Cumulative_SOC_deviation: 472.7531 Fuel Consumption: 137.1673\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -4803.334195198854 Explore P: 0.2286 SOC: 1.0000 Cumulative_SOC_deviation: 467.9129 Fuel Consumption: 124.2055\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -4902.03578644779 Explore P: 0.2227 SOC: 1.0000 Cumulative_SOC_deviation: 478.0039 Fuel Consumption: 121.9964\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -4873.5702177022695 Explore P: 0.2170 SOC: 1.0000 Cumulative_SOC_deviation: 475.5562 Fuel Consumption: 118.0081\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -4792.415585624384 Explore P: 0.2114 SOC: 1.0000 Cumulative_SOC_deviation: 467.6157 Fuel Consumption: 116.2584\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -4919.366858774629 Explore P: 0.2059 SOC: 1.0000 Cumulative_SOC_deviation: 480.2590 Fuel Consumption: 116.7765\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -4799.019497338016 Explore P: 0.2006 SOC: 1.0000 Cumulative_SOC_deviation: 467.8704 Fuel Consumption: 120.3151\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -4925.514260216879 Explore P: 0.1954 SOC: 1.0000 Cumulative_SOC_deviation: 479.9754 Fuel Consumption: 125.7598\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -4737.505607604556 Explore P: 0.1904 SOC: 1.0000 Cumulative_SOC_deviation: 462.1707 Fuel Consumption: 115.7990\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -4844.928255008705 Explore P: 0.1855 SOC: 1.0000 Cumulative_SOC_deviation: 472.1368 Fuel Consumption: 123.5604\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -4826.279821238733 Explore P: 0.1808 SOC: 1.0000 Cumulative_SOC_deviation: 469.7597 Fuel Consumption: 128.6825\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -4769.1018382938355 Explore P: 0.1761 SOC: 1.0000 Cumulative_SOC_deviation: 464.8562 Fuel Consumption: 120.5399\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -4693.540181514688 Explore P: 0.1716 SOC: 1.0000 Cumulative_SOC_deviation: 456.3968 Fuel Consumption: 129.5721\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -4822.402548645141 Explore P: 0.1673 SOC: 1.0000 Cumulative_SOC_deviation: 469.6897 Fuel Consumption: 125.5056\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -4894.107560707869 Explore P: 0.1630 SOC: 1.0000 Cumulative_SOC_deviation: 476.1310 Fuel Consumption: 132.7978\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -4863.667007181399 Explore P: 0.1589 SOC: 1.0000 Cumulative_SOC_deviation: 474.0996 Fuel Consumption: 122.6709\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -4807.889012446798 Explore P: 0.1548 SOC: 1.0000 Cumulative_SOC_deviation: 469.2559 Fuel Consumption: 115.3298\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -4756.790106958257 Explore P: 0.1509 SOC: 1.0000 Cumulative_SOC_deviation: 463.5488 Fuel Consumption: 121.3024\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -4708.557113196002 Explore P: 0.1471 SOC: 1.0000 Cumulative_SOC_deviation: 458.2348 Fuel Consumption: 126.2094\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -4781.385291563983 Explore P: 0.1434 SOC: 1.0000 Cumulative_SOC_deviation: 467.1862 Fuel Consumption: 109.5234\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -4758.733121987194 Explore P: 0.1398 SOC: 1.0000 Cumulative_SOC_deviation: 464.4156 Fuel Consumption: 114.5771\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -4785.891156512927 Explore P: 0.1362 SOC: 1.0000 Cumulative_SOC_deviation: 466.8176 Fuel Consumption: 117.7149\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -4824.161591870408 Explore P: 0.1328 SOC: 1.0000 Cumulative_SOC_deviation: 469.7933 Fuel Consumption: 126.2290\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -4781.860653199285 Explore P: 0.1295 SOC: 1.0000 Cumulative_SOC_deviation: 465.8378 Fuel Consumption: 123.4822\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -4765.317752724836 Explore P: 0.1263 SOC: 1.0000 Cumulative_SOC_deviation: 465.2138 Fuel Consumption: 113.1793\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -4827.676830158225 Explore P: 0.1231 SOC: 1.0000 Cumulative_SOC_deviation: 470.9493 Fuel Consumption: 118.1841\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -4802.047502027018 Explore P: 0.1200 SOC: 1.0000 Cumulative_SOC_deviation: 467.7852 Fuel Consumption: 124.1958\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -4920.47686467982 Explore P: 0.1171 SOC: 1.0000 Cumulative_SOC_deviation: 478.5538 Fuel Consumption: 134.9386\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -4934.183233204281 Explore P: 0.1142 SOC: 1.0000 Cumulative_SOC_deviation: 480.6674 Fuel Consumption: 127.5095\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -4778.018577080362 Explore P: 0.1113 SOC: 1.0000 Cumulative_SOC_deviation: 465.3002 Fuel Consumption: 125.0169\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -4819.548935336691 Explore P: 0.1086 SOC: 1.0000 Cumulative_SOC_deviation: 469.6810 Fuel Consumption: 122.7393\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -4794.145513595373 Explore P: 0.1059 SOC: 1.0000 Cumulative_SOC_deviation: 467.6704 Fuel Consumption: 117.4412\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -4798.077627903961 Explore P: 0.1033 SOC: 1.0000 Cumulative_SOC_deviation: 469.0812 Fuel Consumption: 107.2653\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -4790.544752930134 Explore P: 0.1008 SOC: 1.0000 Cumulative_SOC_deviation: 467.3201 Fuel Consumption: 117.3434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -4764.237463024827 Explore P: 0.0983 SOC: 1.0000 Cumulative_SOC_deviation: 464.4177 Fuel Consumption: 120.0609\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -4659.335752990775 Explore P: 0.0960 SOC: 1.0000 Cumulative_SOC_deviation: 453.5541 Fuel Consumption: 123.7950\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -4785.9859308989335 Explore P: 0.0936 SOC: 1.0000 Cumulative_SOC_deviation: 467.4781 Fuel Consumption: 111.2047\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -4685.055530127713 Explore P: 0.0914 SOC: 1.0000 Cumulative_SOC_deviation: 456.6158 Fuel Consumption: 118.8977\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -4881.272504198909 Explore P: 0.0892 SOC: 1.0000 Cumulative_SOC_deviation: 476.9452 Fuel Consumption: 111.8205\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -4792.807968421967 Explore P: 0.0870 SOC: 1.0000 Cumulative_SOC_deviation: 466.4194 Fuel Consumption: 128.6141\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -4925.0260662756 Explore P: 0.0849 SOC: 1.0000 Cumulative_SOC_deviation: 481.2433 Fuel Consumption: 112.5928\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -4843.155501281893 Explore P: 0.0829 SOC: 1.0000 Cumulative_SOC_deviation: 472.3564 Fuel Consumption: 119.5917\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -4785.1549693108545 Explore P: 0.0809 SOC: 1.0000 Cumulative_SOC_deviation: 467.3061 Fuel Consumption: 112.0942\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -4828.909576524037 Explore P: 0.0790 SOC: 1.0000 Cumulative_SOC_deviation: 470.0980 Fuel Consumption: 127.9299\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -4793.452723139485 Explore P: 0.0771 SOC: 1.0000 Cumulative_SOC_deviation: 467.4223 Fuel Consumption: 119.2300\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -4851.765841613185 Explore P: 0.0753 SOC: 1.0000 Cumulative_SOC_deviation: 471.2008 Fuel Consumption: 139.7577\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -5009.330182027321 Explore P: 0.0735 SOC: 1.0000 Cumulative_SOC_deviation: 487.3551 Fuel Consumption: 135.7792\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -4959.526789205874 Explore P: 0.0718 SOC: 1.0000 Cumulative_SOC_deviation: 483.6133 Fuel Consumption: 123.3942\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -4740.983052412899 Explore P: 0.0701 SOC: 1.0000 Cumulative_SOC_deviation: 462.7071 Fuel Consumption: 113.9124\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -4833.139955700441 Explore P: 0.0685 SOC: 1.0000 Cumulative_SOC_deviation: 470.7136 Fuel Consumption: 126.0042\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -4748.721652242563 Explore P: 0.0669 SOC: 1.0000 Cumulative_SOC_deviation: 462.6334 Fuel Consumption: 122.3874\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -4940.7611494144585 Explore P: 0.0654 SOC: 1.0000 Cumulative_SOC_deviation: 481.3222 Fuel Consumption: 127.5389\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -4578.046826320457 Explore P: 0.0639 SOC: 1.0000 Cumulative_SOC_deviation: 445.4281 Fuel Consumption: 123.7657\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -5042.558352130832 Explore P: 0.0624 SOC: 1.0000 Cumulative_SOC_deviation: 491.4550 Fuel Consumption: 128.0081\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -4676.637971153358 Explore P: 0.0610 SOC: 1.0000 Cumulative_SOC_deviation: 455.8923 Fuel Consumption: 117.7149\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -4334.431865980278 Explore P: 0.0596 SOC: 1.0000 Cumulative_SOC_deviation: 422.6052 Fuel Consumption: 108.3797\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -4620.255190306918 Explore P: 0.0583 SOC: 1.0000 Cumulative_SOC_deviation: 449.5053 Fuel Consumption: 125.2026\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -4680.855158790179 Explore P: 0.0570 SOC: 1.0000 Cumulative_SOC_deviation: 456.9318 Fuel Consumption: 111.5370\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -4335.139326959706 Explore P: 0.0557 SOC: 1.0000 Cumulative_SOC_deviation: 423.2087 Fuel Consumption: 103.0523\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -4456.112794634603 Explore P: 0.0545 SOC: 1.0000 Cumulative_SOC_deviation: 434.8877 Fuel Consumption: 107.2360\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -4572.807583474922 Explore P: 0.0533 SOC: 1.0000 Cumulative_SOC_deviation: 445.8993 Fuel Consumption: 113.8146\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -4815.959622927513 Explore P: 0.0521 SOC: 1.0000 Cumulative_SOC_deviation: 471.8303 Fuel Consumption: 97.6564\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -4862.717784487984 Explore P: 0.0510 SOC: 1.0000 Cumulative_SOC_deviation: 475.2569 Fuel Consumption: 110.1490\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -4702.911321307019 Explore P: 0.0498 SOC: 1.0000 Cumulative_SOC_deviation: 457.9009 Fuel Consumption: 123.9025\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -4870.764735053368 Explore P: 0.0488 SOC: 1.0000 Cumulative_SOC_deviation: 475.2483 Fuel Consumption: 118.2819\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -4638.639988924402 Explore P: 0.0477 SOC: 1.0000 Cumulative_SOC_deviation: 453.0094 Fuel Consumption: 108.5459\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -4665.236284446627 Explore P: 0.0467 SOC: 1.0000 Cumulative_SOC_deviation: 455.2194 Fuel Consumption: 113.0424\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -4802.299117461071 Explore P: 0.0457 SOC: 1.0000 Cumulative_SOC_deviation: 470.1368 Fuel Consumption: 100.9311\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -4801.343723889629 Explore P: 0.0447 SOC: 1.0000 Cumulative_SOC_deviation: 469.1136 Fuel Consumption: 110.2076\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -4928.7235918330825 Explore P: 0.0438 SOC: 1.0000 Cumulative_SOC_deviation: 481.2710 Fuel Consumption: 116.0140\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -4657.752639538393 Explore P: 0.0429 SOC: 1.0000 Cumulative_SOC_deviation: 455.1924 Fuel Consumption: 105.8284\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -4055.8289861288417 Explore P: 0.0420 SOC: 1.0000 Cumulative_SOC_deviation: 396.3119 Fuel Consumption: 92.7102\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -4481.392700316917 Explore P: 0.0411 SOC: 1.0000 Cumulative_SOC_deviation: 438.4548 Fuel Consumption: 96.8451\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -4503.690514918357 Explore P: 0.0403 SOC: 1.0000 Cumulative_SOC_deviation: 439.4607 Fuel Consumption: 109.0835\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -4564.696326514382 Explore P: 0.0395 SOC: 1.0000 Cumulative_SOC_deviation: 445.9973 Fuel Consumption: 104.7238\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -4719.660373666835 Explore P: 0.0387 SOC: 1.0000 Cumulative_SOC_deviation: 461.2160 Fuel Consumption: 107.4999\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -4756.837219858234 Explore P: 0.0379 SOC: 1.0000 Cumulative_SOC_deviation: 464.3365 Fuel Consumption: 113.4725\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -4738.392572783149 Explore P: 0.0371 SOC: 1.0000 Cumulative_SOC_deviation: 462.6064 Fuel Consumption: 112.3288\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -4131.667877108275 Explore P: 0.0364 SOC: 1.0000 Cumulative_SOC_deviation: 402.6856 Fuel Consumption: 104.8118\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -4640.757056524026 Explore P: 0.0357 SOC: 1.0000 Cumulative_SOC_deviation: 452.0853 Fuel Consumption: 119.9045\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -4985.079640430505 Explore P: 0.0350 SOC: 1.0000 Cumulative_SOC_deviation: 487.3396 Fuel Consumption: 111.6837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -4823.519236403862 Explore P: 0.0343 SOC: 1.0000 Cumulative_SOC_deviation: 471.9089 Fuel Consumption: 104.4306\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -4517.28850461816 Explore P: 0.0336 SOC: 1.0000 Cumulative_SOC_deviation: 441.1851 Fuel Consumption: 105.4374\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -4408.102011916895 Explore P: 0.0330 SOC: 1.0000 Cumulative_SOC_deviation: 430.7562 Fuel Consumption: 100.5401\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -4672.101852364662 Explore P: 0.0324 SOC: 1.0000 Cumulative_SOC_deviation: 456.4602 Fuel Consumption: 107.4999\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -4499.78460740143 Explore P: 0.0318 SOC: 1.0000 Cumulative_SOC_deviation: 439.8795 Fuel Consumption: 100.9897\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -4570.656902994395 Explore P: 0.0312 SOC: 1.0000 Cumulative_SOC_deviation: 445.6979 Fuel Consumption: 113.6778\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -4714.878922948163 Explore P: 0.0306 SOC: 1.0000 Cumulative_SOC_deviation: 459.2931 Fuel Consumption: 121.9475\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -4912.182264928426 Explore P: 0.0301 SOC: 1.0000 Cumulative_SOC_deviation: 479.5464 Fuel Consumption: 116.7178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -4659.307399994199 Explore P: 0.0295 SOC: 1.0000 Cumulative_SOC_deviation: 454.3802 Fuel Consumption: 115.5057\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -5013.259468341658 Explore P: 0.0290 SOC: 1.0000 Cumulative_SOC_deviation: 488.9777 Fuel Consumption: 123.4822\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -4813.505625428813 Explore P: 0.0285 SOC: 1.0000 Cumulative_SOC_deviation: 468.8059 Fuel Consumption: 125.4470\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -4713.347734130969 Explore P: 0.0280 SOC: 1.0000 Cumulative_SOC_deviation: 459.3590 Fuel Consumption: 119.7579\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -4759.615989376605 Explore P: 0.0275 SOC: 1.0000 Cumulative_SOC_deviation: 464.7170 Fuel Consumption: 112.4461\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -4740.644691892285 Explore P: 0.0270 SOC: 1.0000 Cumulative_SOC_deviation: 462.7964 Fuel Consumption: 112.6807\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -4551.091617776752 Explore P: 0.0265 SOC: 1.0000 Cumulative_SOC_deviation: 444.0180 Fuel Consumption: 110.9114\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -4820.833337631336 Explore P: 0.0261 SOC: 1.0000 Cumulative_SOC_deviation: 470.7009 Fuel Consumption: 113.8244\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -4604.403930948613 Explore P: 0.0257 SOC: 1.0000 Cumulative_SOC_deviation: 450.2007 Fuel Consumption: 102.3973\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -4524.4114400569115 Explore P: 0.0252 SOC: 1.0000 Cumulative_SOC_deviation: 441.6872 Fuel Consumption: 107.5390\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -4411.956867175444 Explore P: 0.0248 SOC: 1.0000 Cumulative_SOC_deviation: 432.3020 Fuel Consumption: 88.9371\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -2777.574835079514 Explore P: 0.0244 SOC: 0.9409 Cumulative_SOC_deviation: 271.1775 Fuel Consumption: 65.7995\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -1930.9279063686222 Explore P: 0.0240 SOC: 0.9564 Cumulative_SOC_deviation: 186.5529 Fuel Consumption: 65.3987\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -2072.022419211664 Explore P: 0.0237 SOC: 1.0000 Cumulative_SOC_deviation: 200.1081 Fuel Consumption: 70.9411\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -1962.2061279795564 Explore P: 0.0233 SOC: 0.9197 Cumulative_SOC_deviation: 189.9916 Fuel Consumption: 62.2902\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -2656.498639401903 Explore P: 0.0229 SOC: 0.9891 Cumulative_SOC_deviation: 258.9292 Fuel Consumption: 67.2071\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -2768.911697320828 Explore P: 0.0226 SOC: 1.0000 Cumulative_SOC_deviation: 269.5537 Fuel Consumption: 73.3751\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -3093.0024484961996 Explore P: 0.0222 SOC: 1.0000 Cumulative_SOC_deviation: 301.1895 Fuel Consumption: 81.1072\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -3596.3028052183454 Explore P: 0.0219 SOC: 1.0000 Cumulative_SOC_deviation: 352.5274 Fuel Consumption: 71.0291\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -2025.1291648021372 Explore P: 0.0216 SOC: 0.9289 Cumulative_SOC_deviation: 196.1744 Fuel Consumption: 63.3850\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -3762.8399422990406 Explore P: 0.0213 SOC: 1.0000 Cumulative_SOC_deviation: 368.2104 Fuel Consumption: 80.7358\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -3016.3452439543935 Explore P: 0.0210 SOC: 1.0000 Cumulative_SOC_deviation: 293.8923 Fuel Consumption: 77.4220\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -2949.0351721351144 Explore P: 0.0207 SOC: 1.0000 Cumulative_SOC_deviation: 287.4536 Fuel Consumption: 74.4993\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -3318.3428697515988 Explore P: 0.0204 SOC: 1.0000 Cumulative_SOC_deviation: 324.6141 Fuel Consumption: 72.2021\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -2751.3549400406023 Explore P: 0.0201 SOC: 0.9989 Cumulative_SOC_deviation: 268.2867 Fuel Consumption: 68.4876\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -4281.683272624147 Explore P: 0.0198 SOC: 1.0000 Cumulative_SOC_deviation: 420.9706 Fuel Consumption: 71.9773\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -2721.6761164918603 Explore P: 0.0196 SOC: 1.0000 Cumulative_SOC_deviation: 264.5877 Fuel Consumption: 75.7994\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -2405.3093557671473 Explore P: 0.0193 SOC: 0.9525 Cumulative_SOC_deviation: 233.8894 Fuel Consumption: 66.4153\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -4113.407091968192 Explore P: 0.0190 SOC: 1.0000 Cumulative_SOC_deviation: 403.1733 Fuel Consumption: 81.6742\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -636.1712082096369 Explore P: 0.0188 SOC: 0.6854 Cumulative_SOC_deviation: 58.8016 Fuel Consumption: 48.1554\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -1428.8460602023956 Explore P: 0.0186 SOC: 0.5621 Cumulative_SOC_deviation: 138.8921 Fuel Consumption: 39.9248\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -1889.9491886071114 Explore P: 0.0183 SOC: 0.5241 Cumulative_SOC_deviation: 185.2322 Fuel Consumption: 37.6277\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -1095.2844361746095 Explore P: 0.0181 SOC: 0.6818 Cumulative_SOC_deviation: 104.6464 Fuel Consumption: 48.8201\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -823.4175649166896 Explore P: 0.0179 SOC: 0.7419 Cumulative_SOC_deviation: 77.1205 Fuel Consumption: 52.2121\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -1323.592572838404 Explore P: 0.0177 SOC: 0.8219 Cumulative_SOC_deviation: 126.6952 Fuel Consumption: 56.6402\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -2334.925365283859 Explore P: 0.0175 SOC: 0.8957 Cumulative_SOC_deviation: 227.3212 Fuel Consumption: 61.7135\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -2287.4080639184676 Explore P: 0.0173 SOC: 0.9139 Cumulative_SOC_deviation: 222.3573 Fuel Consumption: 63.8347\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -1778.047920465028 Explore P: 0.0171 SOC: 0.8512 Cumulative_SOC_deviation: 171.8856 Fuel Consumption: 59.1915\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -2088.140733758807 Explore P: 0.0169 SOC: 0.9461 Cumulative_SOC_deviation: 202.0719 Fuel Consumption: 67.4221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -2657.4886026670806 Explore P: 0.0167 SOC: 0.9898 Cumulative_SOC_deviation: 258.5912 Fuel Consumption: 71.5765\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -2568.307870145044 Explore P: 0.0165 SOC: 0.9924 Cumulative_SOC_deviation: 249.7680 Fuel Consumption: 70.6283\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -2975.474779410259 Explore P: 0.0163 SOC: 1.0000 Cumulative_SOC_deviation: 289.4895 Fuel Consumption: 80.5794\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -1578.860443870445 Explore P: 0.0162 SOC: 0.8727 Cumulative_SOC_deviation: 151.6472 Fuel Consumption: 62.3880\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -1720.6958043220422 Explore P: 0.0160 SOC: 0.9376 Cumulative_SOC_deviation: 165.4007 Fuel Consumption: 66.6890\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -2030.6598908702513 Explore P: 0.0158 SOC: 1.0000 Cumulative_SOC_deviation: 195.8204 Fuel Consumption: 72.4563\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -2258.649765644397 Explore P: 0.0157 SOC: 1.0000 Cumulative_SOC_deviation: 218.1433 Fuel Consumption: 77.2167\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -2553.193188875218 Explore P: 0.0155 SOC: 1.0000 Cumulative_SOC_deviation: 247.6974 Fuel Consumption: 76.2197\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -2888.0098424728426 Explore P: 0.0154 SOC: 1.0000 Cumulative_SOC_deviation: 280.1722 Fuel Consumption: 86.2880\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -2777.3991140957664 Explore P: 0.0152 SOC: 1.0000 Cumulative_SOC_deviation: 269.7875 Fuel Consumption: 79.5237\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -2479.796011530129 Explore P: 0.0151 SOC: 1.0000 Cumulative_SOC_deviation: 239.9627 Fuel Consumption: 80.1688\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -3328.9950116930277 Explore P: 0.0149 SOC: 1.0000 Cumulative_SOC_deviation: 323.3626 Fuel Consumption: 95.3691\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -2899.7016276553045 Explore P: 0.0148 SOC: 1.0000 Cumulative_SOC_deviation: 281.2788 Fuel Consumption: 86.9136\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -2452.844774282531 Explore P: 0.0147 SOC: 1.0000 Cumulative_SOC_deviation: 236.8785 Fuel Consumption: 84.0593\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -1653.6597750908372 Explore P: 0.0146 SOC: 0.9798 Cumulative_SOC_deviation: 158.4801 Fuel Consumption: 68.8591\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -1081.7840157787566 Explore P: 0.0144 SOC: 0.8648 Cumulative_SOC_deviation: 102.0579 Fuel Consumption: 61.2052\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -861.26610219355 Explore P: 0.0143 SOC: 0.8330 Cumulative_SOC_deviation: 80.2221 Fuel Consumption: 59.0449\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -1003.9045118584071 Explore P: 0.0142 SOC: 0.8700 Cumulative_SOC_deviation: 94.2406 Fuel Consumption: 61.4984\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -3458.4235277761595 Explore P: 0.0141 SOC: 1.0000 Cumulative_SOC_deviation: 337.6202 Fuel Consumption: 82.2216\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN3_1.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

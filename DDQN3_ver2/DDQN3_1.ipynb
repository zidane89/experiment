{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN3_1 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 1\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -5083.6673452691875 Explore P: 0.9217 SOC: 1.0000 Cumulative_SOC_deviation: 493.8442 Fuel Consumption: 145.2252\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -5104.218378030346 Explore P: 0.8970 SOC: 1.0000 Cumulative_SOC_deviation: 494.8871 Fuel Consumption: 155.3472\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -5088.100325828151 Explore P: 0.8730 SOC: 1.0000 Cumulative_SOC_deviation: 493.7577 Fuel Consumption: 150.5232\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -5092.288534046134 Explore P: 0.8496 SOC: 1.0000 Cumulative_SOC_deviation: 494.3036 Fuel Consumption: 149.2525\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -5118.034159513818 Explore P: 0.8269 SOC: 1.0000 Cumulative_SOC_deviation: 496.7355 Fuel Consumption: 150.6796\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -5102.632032614728 Explore P: 0.8048 SOC: 1.0000 Cumulative_SOC_deviation: 495.1664 Fuel Consumption: 150.9680\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -5096.803523492517 Explore P: 0.7832 SOC: 1.0000 Cumulative_SOC_deviation: 494.7145 Fuel Consumption: 149.6582\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -5119.401197645809 Explore P: 0.7623 SOC: 1.0000 Cumulative_SOC_deviation: 496.6073 Fuel Consumption: 153.3287\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -5093.992447864286 Explore P: 0.7419 SOC: 1.0000 Cumulative_SOC_deviation: 494.2712 Fuel Consumption: 151.2808\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -5071.543107980846 Explore P: 0.7221 SOC: 1.0000 Cumulative_SOC_deviation: 492.2203 Fuel Consumption: 149.3405\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -5044.357695723272 Explore P: 0.7028 SOC: 1.0000 Cumulative_SOC_deviation: 489.8844 Fuel Consumption: 145.5135\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -4977.878942010085 Explore P: 0.6840 SOC: 1.0000 Cumulative_SOC_deviation: 482.5733 Fuel Consumption: 152.1459\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -5119.421679505971 Explore P: 0.6658 SOC: 1.0000 Cumulative_SOC_deviation: 496.1362 Fuel Consumption: 158.0598\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -5148.623369632351 Explore P: 0.6480 SOC: 1.0000 Cumulative_SOC_deviation: 499.5686 Fuel Consumption: 152.9377\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -5074.897226089979 Explore P: 0.6307 SOC: 1.0000 Cumulative_SOC_deviation: 492.7517 Fuel Consumption: 147.3806\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -5042.574184197662 Explore P: 0.6139 SOC: 1.0000 Cumulative_SOC_deviation: 489.4861 Fuel Consumption: 147.7129\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -5018.63066349292 Explore P: 0.5976 SOC: 1.0000 Cumulative_SOC_deviation: 487.0419 Fuel Consumption: 148.2114\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -5092.964252533629 Explore P: 0.5816 SOC: 1.0000 Cumulative_SOC_deviation: 494.0496 Fuel Consumption: 152.4685\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -5158.521953426197 Explore P: 0.5662 SOC: 1.0000 Cumulative_SOC_deviation: 500.4445 Fuel Consumption: 154.0765\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -5129.719393385264 Explore P: 0.5511 SOC: 1.0000 Cumulative_SOC_deviation: 497.8287 Fuel Consumption: 151.4323\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -5021.9140971320285 Explore P: 0.5364 SOC: 1.0000 Cumulative_SOC_deviation: 487.2872 Fuel Consumption: 149.0423\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -5157.232220859086 Explore P: 0.5222 SOC: 1.0000 Cumulative_SOC_deviation: 499.7799 Fuel Consumption: 159.4332\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -5099.073996914083 Explore P: 0.5083 SOC: 1.0000 Cumulative_SOC_deviation: 495.1860 Fuel Consumption: 147.2144\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -5088.652861103192 Explore P: 0.4948 SOC: 1.0000 Cumulative_SOC_deviation: 493.6092 Fuel Consumption: 152.5614\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -5099.695089821624 Explore P: 0.4817 SOC: 1.0000 Cumulative_SOC_deviation: 494.9812 Fuel Consumption: 149.8830\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -5207.969593172004 Explore P: 0.4689 SOC: 1.0000 Cumulative_SOC_deviation: 505.7241 Fuel Consumption: 150.7285\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -5101.573431374269 Explore P: 0.4565 SOC: 1.0000 Cumulative_SOC_deviation: 494.8885 Fuel Consumption: 152.6884\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -5153.997092271221 Explore P: 0.4444 SOC: 1.0000 Cumulative_SOC_deviation: 499.8337 Fuel Consumption: 155.6601\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -5153.9502043558 Explore P: 0.4326 SOC: 1.0000 Cumulative_SOC_deviation: 499.8720 Fuel Consumption: 155.2299\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -5220.057757696981 Explore P: 0.4212 SOC: 1.0000 Cumulative_SOC_deviation: 505.6646 Fuel Consumption: 163.4117\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -5199.152957550548 Explore P: 0.4100 SOC: 1.0000 Cumulative_SOC_deviation: 504.7462 Fuel Consumption: 151.6914\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -5169.958288033211 Explore P: 0.3992 SOC: 1.0000 Cumulative_SOC_deviation: 500.6982 Fuel Consumption: 162.9767\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -5129.806895603658 Explore P: 0.3887 SOC: 1.0000 Cumulative_SOC_deviation: 497.7832 Fuel Consumption: 151.9748\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -5152.948207222671 Explore P: 0.3784 SOC: 1.0000 Cumulative_SOC_deviation: 499.6100 Fuel Consumption: 156.8477\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -5192.612546826832 Explore P: 0.3684 SOC: 1.0000 Cumulative_SOC_deviation: 503.7617 Fuel Consumption: 154.9953\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -5193.432497729805 Explore P: 0.3587 SOC: 1.0000 Cumulative_SOC_deviation: 504.1976 Fuel Consumption: 151.4568\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -5196.753510190093 Explore P: 0.3493 SOC: 1.0000 Cumulative_SOC_deviation: 504.3997 Fuel Consumption: 152.7569\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -5137.951747392671 Explore P: 0.3401 SOC: 1.0000 Cumulative_SOC_deviation: 497.9379 Fuel Consumption: 158.5730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -5164.021837417714 Explore P: 0.3311 SOC: 1.0000 Cumulative_SOC_deviation: 501.4676 Fuel Consumption: 149.3454\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -4966.293478026906 Explore P: 0.3224 SOC: 1.0000 Cumulative_SOC_deviation: 482.1117 Fuel Consumption: 145.1763\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -5178.921516706905 Explore P: 0.3140 SOC: 1.0000 Cumulative_SOC_deviation: 502.8599 Fuel Consumption: 150.3229\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -5202.590461988539 Explore P: 0.3057 SOC: 1.0000 Cumulative_SOC_deviation: 504.6310 Fuel Consumption: 156.2808\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -4924.520719639782 Explore P: 0.2977 SOC: 1.0000 Cumulative_SOC_deviation: 478.2463 Fuel Consumption: 142.0580\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -5238.285922793203 Explore P: 0.2899 SOC: 1.0000 Cumulative_SOC_deviation: 506.9933 Fuel Consumption: 168.3530\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -5025.026879978494 Explore P: 0.2824 SOC: 1.0000 Cumulative_SOC_deviation: 487.5691 Fuel Consumption: 149.3356\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -5135.301946486308 Explore P: 0.2750 SOC: 1.0000 Cumulative_SOC_deviation: 498.3801 Fuel Consumption: 151.5008\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -5131.701085231909 Explore P: 0.2678 SOC: 1.0000 Cumulative_SOC_deviation: 497.7297 Fuel Consumption: 154.4040\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -4946.8968728382415 Explore P: 0.2608 SOC: 1.0000 Cumulative_SOC_deviation: 480.1955 Fuel Consumption: 144.9417\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -4951.25088117208 Explore P: 0.2540 SOC: 1.0000 Cumulative_SOC_deviation: 480.8567 Fuel Consumption: 142.6836\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -4869.478458657112 Explore P: 0.2474 SOC: 1.0000 Cumulative_SOC_deviation: 472.6902 Fuel Consumption: 142.5761\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -5070.1379523648375 Explore P: 0.2410 SOC: 1.0000 Cumulative_SOC_deviation: 491.1658 Fuel Consumption: 158.4802\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -4984.972843060958 Explore P: 0.2347 SOC: 1.0000 Cumulative_SOC_deviation: 482.4357 Fuel Consumption: 160.6160\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -5013.286125937738 Explore P: 0.2286 SOC: 1.0000 Cumulative_SOC_deviation: 486.1859 Fuel Consumption: 151.4274\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -5033.28696914652 Explore P: 0.2227 SOC: 1.0000 Cumulative_SOC_deviation: 488.1952 Fuel Consumption: 151.3346\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -5024.729913472055 Explore P: 0.2170 SOC: 1.0000 Cumulative_SOC_deviation: 488.1342 Fuel Consumption: 143.3874\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -4994.524511124634 Explore P: 0.2114 SOC: 1.0000 Cumulative_SOC_deviation: 485.4421 Fuel Consumption: 140.1030\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -4764.913212202993 Explore P: 0.2059 SOC: 1.0000 Cumulative_SOC_deviation: 463.0397 Fuel Consumption: 134.5166\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -5181.882151666079 Explore P: 0.2006 SOC: 1.0000 Cumulative_SOC_deviation: 502.9590 Fuel Consumption: 152.2925\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -5093.127041539788 Explore P: 0.1954 SOC: 1.0000 Cumulative_SOC_deviation: 494.6685 Fuel Consumption: 146.4422\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -5154.182109002858 Explore P: 0.1904 SOC: 1.0000 Cumulative_SOC_deviation: 499.7031 Fuel Consumption: 157.1508\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -5099.9733348216305 Explore P: 0.1855 SOC: 1.0000 Cumulative_SOC_deviation: 495.6166 Fuel Consumption: 143.8078\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -5039.246570362098 Explore P: 0.1808 SOC: 1.0000 Cumulative_SOC_deviation: 490.0361 Fuel Consumption: 138.8860\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -5082.099133654928 Explore P: 0.1761 SOC: 1.0000 Cumulative_SOC_deviation: 493.1434 Fuel Consumption: 150.6650\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -5107.665523545071 Explore P: 0.1716 SOC: 1.0000 Cumulative_SOC_deviation: 495.6883 Fuel Consumption: 150.7823\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -5116.206289365072 Explore P: 0.1673 SOC: 1.0000 Cumulative_SOC_deviation: 497.6167 Fuel Consumption: 140.0395\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -5053.719041218129 Explore P: 0.1630 SOC: 1.0000 Cumulative_SOC_deviation: 490.5224 Fuel Consumption: 148.4949\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -5120.590909180085 Explore P: 0.1589 SOC: 1.0000 Cumulative_SOC_deviation: 496.9325 Fuel Consumption: 151.2662\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -5073.970062623172 Explore P: 0.1548 SOC: 1.0000 Cumulative_SOC_deviation: 491.9336 Fuel Consumption: 154.6337\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -5157.208608816474 Explore P: 0.1509 SOC: 1.0000 Cumulative_SOC_deviation: 501.3528 Fuel Consumption: 143.6807\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -4925.232389677948 Explore P: 0.1471 SOC: 1.0000 Cumulative_SOC_deviation: 478.0965 Fuel Consumption: 144.2672\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -4858.3505188538065 Explore P: 0.1434 SOC: 1.0000 Cumulative_SOC_deviation: 472.8355 Fuel Consumption: 129.9956\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -5089.252911410853 Explore P: 0.1398 SOC: 1.0000 Cumulative_SOC_deviation: 493.8119 Fuel Consumption: 151.1342\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -5132.381948863641 Explore P: 0.1362 SOC: 1.0000 Cumulative_SOC_deviation: 499.0124 Fuel Consumption: 142.2584\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -4967.992016118315 Explore P: 0.1328 SOC: 1.0000 Cumulative_SOC_deviation: 482.3974 Fuel Consumption: 144.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -5033.85125998436 Explore P: 0.1295 SOC: 1.0000 Cumulative_SOC_deviation: 488.5273 Fuel Consumption: 148.5780\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -5144.744338381193 Explore P: 0.1263 SOC: 1.0000 Cumulative_SOC_deviation: 498.0360 Fuel Consumption: 164.3843\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -5036.1000570386295 Explore P: 0.1231 SOC: 1.0000 Cumulative_SOC_deviation: 490.1227 Fuel Consumption: 134.8733\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -5153.456693020962 Explore P: 0.1200 SOC: 1.0000 Cumulative_SOC_deviation: 500.0118 Fuel Consumption: 153.3385\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -5235.400294123087 Explore P: 0.1171 SOC: 1.0000 Cumulative_SOC_deviation: 507.8963 Fuel Consumption: 156.4372\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -5007.058369841045 Explore P: 0.1142 SOC: 1.0000 Cumulative_SOC_deviation: 486.1134 Fuel Consumption: 145.9241\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -5021.532889585882 Explore P: 0.1113 SOC: 1.0000 Cumulative_SOC_deviation: 488.2060 Fuel Consumption: 139.4725\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -4718.126747701255 Explore P: 0.1086 SOC: 1.0000 Cumulative_SOC_deviation: 458.0511 Fuel Consumption: 137.6153\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -4783.283211212847 Explore P: 0.1059 SOC: 1.0000 Cumulative_SOC_deviation: 464.6323 Fuel Consumption: 136.9603\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -4895.637428146062 Explore P: 0.1033 SOC: 1.0000 Cumulative_SOC_deviation: 476.6043 Fuel Consumption: 129.5948\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -4957.315452795751 Explore P: 0.1008 SOC: 1.0000 Cumulative_SOC_deviation: 481.4803 Fuel Consumption: 142.5126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -5026.941233094804 Explore P: 0.0983 SOC: 1.0000 Cumulative_SOC_deviation: 488.4766 Fuel Consumption: 142.1753\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -5045.635298205193 Explore P: 0.0960 SOC: 1.0000 Cumulative_SOC_deviation: 490.2492 Fuel Consumption: 143.1431\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -5072.211843383239 Explore P: 0.0936 SOC: 1.0000 Cumulative_SOC_deviation: 492.7749 Fuel Consumption: 144.4627\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -5055.456030892804 Explore P: 0.0914 SOC: 1.0000 Cumulative_SOC_deviation: 491.4292 Fuel Consumption: 141.1636\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -5158.438387833676 Explore P: 0.0892 SOC: 1.0000 Cumulative_SOC_deviation: 500.6806 Fuel Consumption: 151.6327\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -5054.466505470424 Explore P: 0.0870 SOC: 1.0000 Cumulative_SOC_deviation: 490.0752 Fuel Consumption: 153.7148\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -5230.132701091077 Explore P: 0.0849 SOC: 1.0000 Cumulative_SOC_deviation: 508.2141 Fuel Consumption: 147.9915\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -5179.974346621028 Explore P: 0.0829 SOC: 1.0000 Cumulative_SOC_deviation: 502.2980 Fuel Consumption: 156.9944\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -5075.040012087315 Explore P: 0.0809 SOC: 1.0000 Cumulative_SOC_deviation: 493.3598 Fuel Consumption: 141.4422\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -5127.66175511518 Explore P: 0.0790 SOC: 1.0000 Cumulative_SOC_deviation: 497.6967 Fuel Consumption: 150.6943\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -4968.067669434482 Explore P: 0.0771 SOC: 1.0000 Cumulative_SOC_deviation: 481.7862 Fuel Consumption: 150.2056\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -5109.549766317328 Explore P: 0.0753 SOC: 1.0000 Cumulative_SOC_deviation: 498.0541 Fuel Consumption: 129.0083\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -5085.055736532239 Explore P: 0.0735 SOC: 1.0000 Cumulative_SOC_deviation: 495.0647 Fuel Consumption: 134.4090\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -4971.652129819935 Explore P: 0.0718 SOC: 1.0000 Cumulative_SOC_deviation: 483.6515 Fuel Consumption: 135.1373\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -5054.735996881541 Explore P: 0.0701 SOC: 1.0000 Cumulative_SOC_deviation: 492.4261 Fuel Consumption: 130.4746\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -4909.207937812002 Explore P: 0.0685 SOC: 1.0000 Cumulative_SOC_deviation: 476.9882 Fuel Consumption: 139.3259\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -5011.985936375166 Explore P: 0.0669 SOC: 1.0000 Cumulative_SOC_deviation: 488.2328 Fuel Consumption: 129.6583\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -4983.384733953477 Explore P: 0.0654 SOC: 1.0000 Cumulative_SOC_deviation: 483.8116 Fuel Consumption: 145.2691\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -5020.327475445912 Explore P: 0.0639 SOC: 1.0000 Cumulative_SOC_deviation: 487.6026 Fuel Consumption: 144.3014\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -5042.836253841434 Explore P: 0.0624 SOC: 1.0000 Cumulative_SOC_deviation: 489.9581 Fuel Consumption: 143.2555\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -4892.083296047806 Explore P: 0.0610 SOC: 1.0000 Cumulative_SOC_deviation: 475.0319 Fuel Consumption: 141.7648\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -5224.061828368872 Explore P: 0.0596 SOC: 1.0000 Cumulative_SOC_deviation: 507.4443 Fuel Consumption: 149.6191\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -4767.961715621899 Explore P: 0.0583 SOC: 1.0000 Cumulative_SOC_deviation: 462.9325 Fuel Consumption: 138.6368\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -5131.987371281626 Explore P: 0.0570 SOC: 1.0000 Cumulative_SOC_deviation: 499.0032 Fuel Consumption: 141.9554\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -5145.329933269937 Explore P: 0.0557 SOC: 1.0000 Cumulative_SOC_deviation: 499.0041 Fuel Consumption: 155.2886\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -5130.599066919595 Explore P: 0.0545 SOC: 1.0000 Cumulative_SOC_deviation: 497.5560 Fuel Consumption: 155.0393\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -5128.980378587521 Explore P: 0.0533 SOC: 1.0000 Cumulative_SOC_deviation: 498.4918 Fuel Consumption: 144.0619\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -5081.239917937947 Explore P: 0.0521 SOC: 1.0000 Cumulative_SOC_deviation: 493.8297 Fuel Consumption: 142.9427\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -5259.926827094053 Explore P: 0.0510 SOC: 1.0000 Cumulative_SOC_deviation: 511.0713 Fuel Consumption: 149.2134\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -5091.912322892863 Explore P: 0.0498 SOC: 1.0000 Cumulative_SOC_deviation: 495.1902 Fuel Consumption: 140.0102\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -5010.27550546671 Explore P: 0.0488 SOC: 1.0000 Cumulative_SOC_deviation: 486.5994 Fuel Consumption: 144.2819\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -5058.104630705851 Explore P: 0.0477 SOC: 1.0000 Cumulative_SOC_deviation: 492.8520 Fuel Consumption: 129.5850\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -5102.602721160434 Explore P: 0.0467 SOC: 1.0000 Cumulative_SOC_deviation: 496.5046 Fuel Consumption: 137.5566\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -5185.810947270836 Explore P: 0.0457 SOC: 1.0000 Cumulative_SOC_deviation: 503.2961 Fuel Consumption: 152.8497\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -5202.070185736025 Explore P: 0.0447 SOC: 1.0000 Cumulative_SOC_deviation: 506.2368 Fuel Consumption: 139.7022\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -5192.950926255142 Explore P: 0.0438 SOC: 1.0000 Cumulative_SOC_deviation: 505.7931 Fuel Consumption: 135.0200\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -5096.397523889927 Explore P: 0.0429 SOC: 1.0000 Cumulative_SOC_deviation: 494.9296 Fuel Consumption: 147.1020\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -5068.560154318418 Explore P: 0.0420 SOC: 1.0000 Cumulative_SOC_deviation: 493.9679 Fuel Consumption: 128.8812\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -5312.628629439513 Explore P: 0.0411 SOC: 1.0000 Cumulative_SOC_deviation: 514.9251 Fuel Consumption: 163.3775\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -5162.786024757741 Explore P: 0.0403 SOC: 1.0000 Cumulative_SOC_deviation: 502.0332 Fuel Consumption: 142.4539\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -5140.65701297689 Explore P: 0.0395 SOC: 1.0000 Cumulative_SOC_deviation: 501.4449 Fuel Consumption: 126.2077\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -5137.419277926817 Explore P: 0.0387 SOC: 1.0000 Cumulative_SOC_deviation: 501.0493 Fuel Consumption: 126.9262\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -5281.6447734394405 Explore P: 0.0379 SOC: 1.0000 Cumulative_SOC_deviation: 511.1219 Fuel Consumption: 170.4253\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -5209.788283092186 Explore P: 0.0371 SOC: 1.0000 Cumulative_SOC_deviation: 507.5814 Fuel Consumption: 133.9740\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -5030.573659231265 Explore P: 0.0364 SOC: 1.0000 Cumulative_SOC_deviation: 490.6287 Fuel Consumption: 124.2869\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -4737.6759137161835 Explore P: 0.0357 SOC: 1.0000 Cumulative_SOC_deviation: 464.4816 Fuel Consumption: 92.8601\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -5230.543008519899 Explore P: 0.0350 SOC: 1.0000 Cumulative_SOC_deviation: 509.3803 Fuel Consumption: 136.7404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -5136.158631369275 Explore P: 0.0343 SOC: 1.0000 Cumulative_SOC_deviation: 502.4403 Fuel Consumption: 111.7553\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -5196.017424923245 Explore P: 0.0336 SOC: 1.0000 Cumulative_SOC_deviation: 505.2439 Fuel Consumption: 143.5781\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -5080.667530306796 Explore P: 0.0330 SOC: 1.0000 Cumulative_SOC_deviation: 494.1048 Fuel Consumption: 139.6191\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -5298.715752779755 Explore P: 0.0324 SOC: 1.0000 Cumulative_SOC_deviation: 514.4727 Fuel Consumption: 153.9885\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -5308.008491390559 Explore P: 0.0318 SOC: 1.0000 Cumulative_SOC_deviation: 513.4939 Fuel Consumption: 173.0695\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -5079.287643714443 Explore P: 0.0312 SOC: 1.0000 Cumulative_SOC_deviation: 494.3153 Fuel Consumption: 136.1343\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -5140.517950864392 Explore P: 0.0306 SOC: 1.0000 Cumulative_SOC_deviation: 500.3357 Fuel Consumption: 137.1607\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -4884.444653856259 Explore P: 0.0301 SOC: 1.0000 Cumulative_SOC_deviation: 476.8022 Fuel Consumption: 116.4229\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -3827.0559570867576 Explore P: 0.0295 SOC: 1.0000 Cumulative_SOC_deviation: 373.7930 Fuel Consumption: 89.1260\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -2012.9749710405272 Explore P: 0.0290 SOC: 0.8930 Cumulative_SOC_deviation: 193.2256 Fuel Consumption: 80.7194\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -1639.1484790586098 Explore P: 0.0285 SOC: 0.9437 Cumulative_SOC_deviation: 155.4167 Fuel Consumption: 84.9813\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -1173.1753077854496 Explore P: 0.0280 SOC: 0.8592 Cumulative_SOC_deviation: 109.4729 Fuel Consumption: 78.4467\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -1337.8642929230252 Explore P: 0.0275 SOC: 0.8535 Cumulative_SOC_deviation: 126.0234 Fuel Consumption: 77.6305\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -1251.09606185459 Explore P: 0.0270 SOC: 0.8521 Cumulative_SOC_deviation: 117.3500 Fuel Consumption: 77.5963\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -1212.8774018342576 Explore P: 0.0265 SOC: 0.7897 Cumulative_SOC_deviation: 113.9959 Fuel Consumption: 72.9189\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -1011.2472882967968 Explore P: 0.0261 SOC: 0.7609 Cumulative_SOC_deviation: 94.1364 Fuel Consumption: 69.8837\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -1144.2717570622317 Explore P: 0.0257 SOC: 0.7858 Cumulative_SOC_deviation: 107.1886 Fuel Consumption: 72.3862\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -977.6186862715984 Explore P: 0.0252 SOC: 0.7086 Cumulative_SOC_deviation: 91.1860 Fuel Consumption: 65.7587\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -1119.679446876196 Explore P: 0.0248 SOC: 0.8225 Cumulative_SOC_deviation: 104.4757 Fuel Consumption: 74.9228\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -862.611523900571 Explore P: 0.0244 SOC: 0.6311 Cumulative_SOC_deviation: 80.3231 Fuel Consumption: 59.3804\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -1752.8759479057996 Explore P: 0.0240 SOC: 0.5350 Cumulative_SOC_deviation: 169.9859 Fuel Consumption: 53.0168\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -2003.802859440814 Explore P: 0.0237 SOC: 0.5374 Cumulative_SOC_deviation: 195.1226 Fuel Consumption: 52.5770\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -2342.896257498879 Explore P: 0.0233 SOC: 0.5021 Cumulative_SOC_deviation: 229.3565 Fuel Consumption: 49.3316\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -1971.6096584197696 Explore P: 0.0229 SOC: 0.4763 Cumulative_SOC_deviation: 192.4824 Fuel Consumption: 46.7852\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -1811.7539970866478 Explore P: 0.0226 SOC: 0.5457 Cumulative_SOC_deviation: 175.9021 Fuel Consumption: 52.7334\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -2127.5596126982646 Explore P: 0.0222 SOC: 0.5225 Cumulative_SOC_deviation: 207.6498 Fuel Consumption: 51.0618\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -3074.1950405895136 Explore P: 0.0219 SOC: 0.4154 Cumulative_SOC_deviation: 303.1378 Fuel Consumption: 42.8165\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -4387.582336150743 Explore P: 0.0216 SOC: 0.9619 Cumulative_SOC_deviation: 428.8657 Fuel Consumption: 98.9255\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -2575.448314844329 Explore P: 0.0213 SOC: 0.4716 Cumulative_SOC_deviation: 252.9025 Fuel Consumption: 46.4235\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -2231.3743764854353 Explore P: 0.0210 SOC: 0.7665 Cumulative_SOC_deviation: 216.4800 Fuel Consumption: 66.5749\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -4745.284153341657 Explore P: 0.0207 SOC: 1.0000 Cumulative_SOC_deviation: 462.4907 Fuel Consumption: 120.3769\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -2175.5716069003265 Explore P: 0.0204 SOC: 1.0000 Cumulative_SOC_deviation: 208.2521 Fuel Consumption: 93.0507\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -2214.520360887041 Explore P: 0.0201 SOC: 0.9573 Cumulative_SOC_deviation: 214.0409 Fuel Consumption: 74.1115\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -1774.8574571728982 Explore P: 0.0198 SOC: 0.8909 Cumulative_SOC_deviation: 170.5824 Fuel Consumption: 69.0333\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -1835.2648371271027 Explore P: 0.0196 SOC: 0.9519 Cumulative_SOC_deviation: 176.1246 Fuel Consumption: 74.0186\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -1804.3126820431664 Explore P: 0.0193 SOC: 0.9080 Cumulative_SOC_deviation: 173.3696 Fuel Consumption: 70.6169\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -2192.2980039729996 Explore P: 0.0190 SOC: 0.9921 Cumulative_SOC_deviation: 211.3827 Fuel Consumption: 78.4711\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -1926.6357305602744 Explore P: 0.0188 SOC: 0.9154 Cumulative_SOC_deviation: 185.5491 Fuel Consumption: 71.1447\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -2598.6977181971024 Explore P: 0.0186 SOC: 1.0000 Cumulative_SOC_deviation: 251.5065 Fuel Consumption: 83.6324\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -3592.107274580878 Explore P: 0.0183 SOC: 1.0000 Cumulative_SOC_deviation: 350.4208 Fuel Consumption: 87.8992\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -3911.9095903186667 Explore P: 0.0181 SOC: 1.0000 Cumulative_SOC_deviation: 381.3316 Fuel Consumption: 98.5931\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -3585.5179445772173 Explore P: 0.0179 SOC: 1.0000 Cumulative_SOC_deviation: 349.4637 Fuel Consumption: 90.8806\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -4085.6713407356033 Explore P: 0.0177 SOC: 1.0000 Cumulative_SOC_deviation: 398.6194 Fuel Consumption: 99.4778\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -4923.927118404336 Explore P: 0.0175 SOC: 1.0000 Cumulative_SOC_deviation: 481.4586 Fuel Consumption: 109.3408\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -5156.7651818610775 Explore P: 0.0173 SOC: 1.0000 Cumulative_SOC_deviation: 502.3637 Fuel Consumption: 133.1285\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -5065.0215954563155 Explore P: 0.0171 SOC: 1.0000 Cumulative_SOC_deviation: 492.2954 Fuel Consumption: 142.0678\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -5044.189742384416 Explore P: 0.0169 SOC: 1.0000 Cumulative_SOC_deviation: 491.2102 Fuel Consumption: 132.0875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -4777.900675307931 Explore P: 0.0167 SOC: 1.0000 Cumulative_SOC_deviation: 465.8228 Fuel Consumption: 119.6731\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -5094.1318251718785 Explore P: 0.0165 SOC: 1.0000 Cumulative_SOC_deviation: 496.3569 Fuel Consumption: 130.5625\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -5009.753148516908 Explore P: 0.0163 SOC: 1.0000 Cumulative_SOC_deviation: 487.9313 Fuel Consumption: 130.4403\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -5012.27492057289 Explore P: 0.0162 SOC: 1.0000 Cumulative_SOC_deviation: 488.9288 Fuel Consumption: 122.9869\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -5127.878855751823 Explore P: 0.0160 SOC: 1.0000 Cumulative_SOC_deviation: 498.7199 Fuel Consumption: 140.6797\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -5313.58659085316 Explore P: 0.0158 SOC: 1.0000 Cumulative_SOC_deviation: 514.1412 Fuel Consumption: 172.1750\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -5118.307653042677 Explore P: 0.0157 SOC: 1.0000 Cumulative_SOC_deviation: 497.4935 Fuel Consumption: 143.3728\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -4885.862639492311 Explore P: 0.0155 SOC: 1.0000 Cumulative_SOC_deviation: 476.8946 Fuel Consumption: 116.9165\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -4977.186214391043 Explore P: 0.0154 SOC: 0.9919 Cumulative_SOC_deviation: 487.4659 Fuel Consumption: 102.5276\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -5016.139213127651 Explore P: 0.0152 SOC: 1.0000 Cumulative_SOC_deviation: 489.2131 Fuel Consumption: 124.0083\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -5111.8575243966825 Explore P: 0.0151 SOC: 1.0000 Cumulative_SOC_deviation: 499.5904 Fuel Consumption: 115.9537\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -5144.462879386715 Explore P: 0.0149 SOC: 1.0000 Cumulative_SOC_deviation: 501.1984 Fuel Consumption: 132.4785\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -5095.085620980685 Explore P: 0.0148 SOC: 1.0000 Cumulative_SOC_deviation: 495.2055 Fuel Consumption: 143.0307\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -5052.659041925693 Explore P: 0.0147 SOC: 1.0000 Cumulative_SOC_deviation: 492.7952 Fuel Consumption: 124.7073\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -5206.766463212022 Explore P: 0.0146 SOC: 1.0000 Cumulative_SOC_deviation: 507.0549 Fuel Consumption: 136.2174\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -5153.008139001552 Explore P: 0.0144 SOC: 1.0000 Cumulative_SOC_deviation: 500.1463 Fuel Consumption: 151.5447\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -5184.779881797711 Explore P: 0.0143 SOC: 1.0000 Cumulative_SOC_deviation: 504.2189 Fuel Consumption: 142.5908\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -5027.864824938971 Explore P: 0.0142 SOC: 1.0000 Cumulative_SOC_deviation: 488.6388 Fuel Consumption: 141.4764\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -3530.758140164063 Explore P: 0.0141 SOC: 0.9270 Cumulative_SOC_deviation: 345.4262 Fuel Consumption: 76.4966\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN3_1_mass1200.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

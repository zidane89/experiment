{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN3_2 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 2\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -5083.57874923413 Explore P: 0.9217 SOC: 1.0000 Cumulative_SOC_deviation: 493.9717 Fuel Consumption: 143.8615\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -5058.176189618673 Explore P: 0.8970 SOC: 1.0000 Cumulative_SOC_deviation: 491.6240 Fuel Consumption: 141.9358\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -5093.274509670551 Explore P: 0.8730 SOC: 1.0000 Cumulative_SOC_deviation: 494.8157 Fuel Consumption: 145.1176\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -5072.891201488819 Explore P: 0.8496 SOC: 1.0000 Cumulative_SOC_deviation: 493.0535 Fuel Consumption: 142.3562\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -4991.360982404151 Explore P: 0.8269 SOC: 1.0000 Cumulative_SOC_deviation: 485.1742 Fuel Consumption: 139.6191\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -5055.244267483037 Explore P: 0.8048 SOC: 1.0000 Cumulative_SOC_deviation: 491.4838 Fuel Consumption: 140.4060\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -4909.715146624223 Explore P: 0.7832 SOC: 1.0000 Cumulative_SOC_deviation: 477.2413 Fuel Consumption: 137.3025\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -4943.011913398548 Explore P: 0.7623 SOC: 1.0000 Cumulative_SOC_deviation: 480.4273 Fuel Consumption: 138.7394\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -4880.945550708177 Explore P: 0.7419 SOC: 1.0000 Cumulative_SOC_deviation: 474.1287 Fuel Consumption: 139.6582\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -4947.26591705389 Explore P: 0.7221 SOC: 1.0000 Cumulative_SOC_deviation: 481.0198 Fuel Consumption: 137.0679\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -4943.98802292836 Explore P: 0.7028 SOC: 1.0000 Cumulative_SOC_deviation: 480.7702 Fuel Consumption: 136.2858\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -4872.793617842042 Explore P: 0.6840 SOC: 1.0000 Cumulative_SOC_deviation: 473.1708 Fuel Consumption: 141.0854\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -4840.612011120347 Explore P: 0.6658 SOC: 1.0000 Cumulative_SOC_deviation: 470.7821 Fuel Consumption: 132.7913\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -4922.195711353657 Explore P: 0.6480 SOC: 1.0000 Cumulative_SOC_deviation: 478.1848 Fuel Consumption: 140.3474\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -5050.172006074625 Explore P: 0.6307 SOC: 1.0000 Cumulative_SOC_deviation: 490.1330 Fuel Consumption: 148.8419\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -4893.78970307637 Explore P: 0.6139 SOC: 1.0000 Cumulative_SOC_deviation: 477.2152 Fuel Consumption: 121.6379\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -4839.88795531096 Explore P: 0.5976 SOC: 1.0000 Cumulative_SOC_deviation: 470.2561 Fuel Consumption: 137.3269\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -4859.32900859938 Explore P: 0.5816 SOC: 1.0000 Cumulative_SOC_deviation: 471.9519 Fuel Consumption: 139.8098\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -4673.298035481165 Explore P: 0.5662 SOC: 1.0000 Cumulative_SOC_deviation: 455.1020 Fuel Consumption: 122.2782\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -4835.3132711097405 Explore P: 0.5511 SOC: 1.0000 Cumulative_SOC_deviation: 470.3715 Fuel Consumption: 131.5987\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -4791.3878443262665 Explore P: 0.5364 SOC: 1.0000 Cumulative_SOC_deviation: 466.3137 Fuel Consumption: 128.2507\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -4741.561327572602 Explore P: 0.5222 SOC: 1.0000 Cumulative_SOC_deviation: 460.6879 Fuel Consumption: 134.6827\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -4844.722654463803 Explore P: 0.5083 SOC: 1.0000 Cumulative_SOC_deviation: 471.2181 Fuel Consumption: 132.5420\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -4819.996561776127 Explore P: 0.4948 SOC: 1.0000 Cumulative_SOC_deviation: 469.1521 Fuel Consumption: 128.4756\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -4707.845265658739 Explore P: 0.4817 SOC: 1.0000 Cumulative_SOC_deviation: 456.2718 Fuel Consumption: 145.1274\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -4785.277417528128 Explore P: 0.4689 SOC: 1.0000 Cumulative_SOC_deviation: 465.9329 Fuel Consumption: 125.9487\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -4742.717852718861 Explore P: 0.4565 SOC: 1.0000 Cumulative_SOC_deviation: 462.4047 Fuel Consumption: 118.6712\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -4657.991905790087 Explore P: 0.4444 SOC: 1.0000 Cumulative_SOC_deviation: 454.4296 Fuel Consumption: 113.6956\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -4575.537980882869 Explore P: 0.4326 SOC: 1.0000 Cumulative_SOC_deviation: 445.6124 Fuel Consumption: 119.4141\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -4629.475147015935 Explore P: 0.4212 SOC: 1.0000 Cumulative_SOC_deviation: 451.2676 Fuel Consumption: 116.7992\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -4874.640934037056 Explore P: 0.4100 SOC: 1.0000 Cumulative_SOC_deviation: 475.7504 Fuel Consumption: 117.1365\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -4689.864057019374 Explore P: 0.3992 SOC: 1.0000 Cumulative_SOC_deviation: 456.8387 Fuel Consumption: 121.4766\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -4569.004278940668 Explore P: 0.3887 SOC: 1.0000 Cumulative_SOC_deviation: 445.0392 Fuel Consumption: 118.6125\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -4536.200896608696 Explore P: 0.3784 SOC: 1.0000 Cumulative_SOC_deviation: 442.9441 Fuel Consumption: 106.7602\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -4625.969185813842 Explore P: 0.3684 SOC: 1.0000 Cumulative_SOC_deviation: 451.5993 Fuel Consumption: 109.9762\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -4863.827532315541 Explore P: 0.3587 SOC: 1.0000 Cumulative_SOC_deviation: 473.0318 Fuel Consumption: 133.5097\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -4465.7164268377255 Explore P: 0.3493 SOC: 1.0000 Cumulative_SOC_deviation: 434.2461 Fuel Consumption: 123.2557\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -4378.3636110456255 Explore P: 0.3401 SOC: 1.0000 Cumulative_SOC_deviation: 426.5919 Fuel Consumption: 112.4444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -4437.474725850111 Explore P: 0.3311 SOC: 1.0000 Cumulative_SOC_deviation: 432.4493 Fuel Consumption: 112.9821\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -4452.143777038592 Explore P: 0.3224 SOC: 1.0000 Cumulative_SOC_deviation: 433.0061 Fuel Consumption: 122.0827\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -4414.728084435655 Explore P: 0.3140 SOC: 1.0000 Cumulative_SOC_deviation: 427.2690 Fuel Consumption: 142.0385\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -4814.7380218345415 Explore P: 0.3057 SOC: 1.0000 Cumulative_SOC_deviation: 467.7538 Fuel Consumption: 137.1998\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -4351.482100584269 Explore P: 0.2977 SOC: 1.0000 Cumulative_SOC_deviation: 422.3041 Fuel Consumption: 128.4413\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -4844.056157586563 Explore P: 0.2899 SOC: 1.0000 Cumulative_SOC_deviation: 471.7555 Fuel Consumption: 126.5010\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -4784.073836352404 Explore P: 0.2824 SOC: 1.0000 Cumulative_SOC_deviation: 465.5818 Fuel Consumption: 128.2556\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -4477.469903205212 Explore P: 0.2750 SOC: 1.0000 Cumulative_SOC_deviation: 435.5133 Fuel Consumption: 122.3368\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -4437.879159738166 Explore P: 0.2678 SOC: 1.0000 Cumulative_SOC_deviation: 431.6583 Fuel Consumption: 121.2958\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -4768.204586767172 Explore P: 0.2608 SOC: 1.0000 Cumulative_SOC_deviation: 466.2075 Fuel Consumption: 106.1297\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -4487.664641803338 Explore P: 0.2540 SOC: 1.0000 Cumulative_SOC_deviation: 438.4854 Fuel Consumption: 102.8111\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -4236.035558530465 Explore P: 0.2474 SOC: 1.0000 Cumulative_SOC_deviation: 413.1294 Fuel Consumption: 104.7417\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -4589.9872728519 Explore P: 0.2410 SOC: 1.0000 Cumulative_SOC_deviation: 446.9254 Fuel Consumption: 120.7337\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -4087.9317744000027 Explore P: 0.2347 SOC: 1.0000 Cumulative_SOC_deviation: 397.7662 Fuel Consumption: 110.2695\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -4448.5615668052105 Explore P: 0.2286 SOC: 1.0000 Cumulative_SOC_deviation: 432.8654 Fuel Consumption: 119.9077\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -4739.020746894356 Explore P: 0.2227 SOC: 1.0000 Cumulative_SOC_deviation: 461.5501 Fuel Consumption: 123.5196\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -4073.6370444036756 Explore P: 0.2170 SOC: 1.0000 Cumulative_SOC_deviation: 397.4878 Fuel Consumption: 98.7593\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -3585.991057263824 Explore P: 0.2114 SOC: 1.0000 Cumulative_SOC_deviation: 349.5932 Fuel Consumption: 90.0595\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -3249.961328973476 Explore P: 0.2059 SOC: 1.0000 Cumulative_SOC_deviation: 316.0356 Fuel Consumption: 89.6050\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -2729.9395118880416 Explore P: 0.2006 SOC: 0.9833 Cumulative_SOC_deviation: 265.0706 Fuel Consumption: 79.2336\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -3424.653474250181 Explore P: 0.1954 SOC: 1.0000 Cumulative_SOC_deviation: 332.2097 Fuel Consumption: 102.5569\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -2432.153579233868 Explore P: 0.1904 SOC: 0.9816 Cumulative_SOC_deviation: 235.2558 Fuel Consumption: 79.5953\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -2747.0048356572966 Explore P: 0.1855 SOC: 1.0000 Cumulative_SOC_deviation: 266.6320 Fuel Consumption: 80.6852\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -2741.615226038179 Explore P: 0.1808 SOC: 1.0000 Cumulative_SOC_deviation: 265.8100 Fuel Consumption: 83.5151\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -3178.3421811501266 Explore P: 0.1761 SOC: 1.0000 Cumulative_SOC_deviation: 309.5482 Fuel Consumption: 82.8602\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -3467.3545514661237 Explore P: 0.1716 SOC: 1.0000 Cumulative_SOC_deviation: 334.9446 Fuel Consumption: 117.9087\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -3332.904717422779 Explore P: 0.1673 SOC: 1.0000 Cumulative_SOC_deviation: 323.4830 Fuel Consumption: 98.0751\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -3004.7380249762946 Explore P: 0.1630 SOC: 1.0000 Cumulative_SOC_deviation: 291.1086 Fuel Consumption: 93.6518\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -2149.5928105273715 Explore P: 0.1589 SOC: 1.0000 Cumulative_SOC_deviation: 205.7720 Fuel Consumption: 91.8728\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -2690.474929252401 Explore P: 0.1548 SOC: 1.0000 Cumulative_SOC_deviation: 258.7126 Fuel Consumption: 103.3487\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -2532.074697050587 Explore P: 0.1509 SOC: 1.0000 Cumulative_SOC_deviation: 243.3125 Fuel Consumption: 98.9499\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -1970.7999862928382 Explore P: 0.1471 SOC: 1.0000 Cumulative_SOC_deviation: 188.6004 Fuel Consumption: 84.7956\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -2905.179459981467 Explore P: 0.1434 SOC: 1.0000 Cumulative_SOC_deviation: 280.7427 Fuel Consumption: 97.7525\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -1961.6616094448186 Explore P: 0.1398 SOC: 1.0000 Cumulative_SOC_deviation: 186.6812 Fuel Consumption: 94.8493\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -1893.1733942310457 Explore P: 0.1362 SOC: 1.0000 Cumulative_SOC_deviation: 179.7860 Fuel Consumption: 95.3136\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -1445.7860001145154 Explore P: 0.1328 SOC: 0.9035 Cumulative_SOC_deviation: 137.1425 Fuel Consumption: 74.3607\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -2128.044832395615 Explore P: 0.1295 SOC: 1.0000 Cumulative_SOC_deviation: 204.5566 Fuel Consumption: 82.4789\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -1065.0595801911088 Explore P: 0.1263 SOC: 0.8845 Cumulative_SOC_deviation: 99.1926 Fuel Consumption: 73.1340\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -570.9656836860004 Explore P: 0.1231 SOC: 0.6917 Cumulative_SOC_deviation: 51.2323 Fuel Consumption: 58.6424\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -917.354627104728 Explore P: 0.1200 SOC: 0.8701 Cumulative_SOC_deviation: 84.4924 Fuel Consumption: 72.4301\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -420.15527522997604 Explore P: 0.1171 SOC: 0.6628 Cumulative_SOC_deviation: 36.3214 Fuel Consumption: 56.9415\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -492.387320117556 Explore P: 0.1142 SOC: 0.6344 Cumulative_SOC_deviation: 43.7279 Fuel Consumption: 55.1087\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -394.40931585257164 Explore P: 0.1113 SOC: 0.6247 Cumulative_SOC_deviation: 33.9154 Fuel Consumption: 55.2553\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -435.93457831515656 Explore P: 0.1086 SOC: 0.6420 Cumulative_SOC_deviation: 38.1197 Fuel Consumption: 54.7373\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -315.9258433521666 Explore P: 0.1059 SOC: 0.6308 Cumulative_SOC_deviation: 26.2200 Fuel Consumption: 53.7255\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -284.55248882358 Explore P: 0.1033 SOC: 0.6048 Cumulative_SOC_deviation: 23.2103 Fuel Consumption: 52.4499\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -432.83987938692366 Explore P: 0.1008 SOC: 0.5941 Cumulative_SOC_deviation: 38.0996 Fuel Consumption: 51.8438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -564.3868450367846 Explore P: 0.0983 SOC: 0.5829 Cumulative_SOC_deviation: 51.2533 Fuel Consumption: 51.8536\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -665.2395209206163 Explore P: 0.0960 SOC: 0.5863 Cumulative_SOC_deviation: 61.3684 Fuel Consumption: 51.5555\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -738.8606470314396 Explore P: 0.0936 SOC: 0.5527 Cumulative_SOC_deviation: 68.9172 Fuel Consumption: 49.6884\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -677.5496224030111 Explore P: 0.0914 SOC: 0.5532 Cumulative_SOC_deviation: 62.8555 Fuel Consumption: 48.9944\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -665.726669521258 Explore P: 0.0892 SOC: 0.5616 Cumulative_SOC_deviation: 61.6776 Fuel Consumption: 48.9504\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -742.4204016381269 Explore P: 0.0870 SOC: 0.5527 Cumulative_SOC_deviation: 69.3768 Fuel Consumption: 48.6523\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -752.4475582937046 Explore P: 0.0849 SOC: 0.5538 Cumulative_SOC_deviation: 70.3619 Fuel Consumption: 48.8282\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -830.6476658421768 Explore P: 0.0829 SOC: 0.5479 Cumulative_SOC_deviation: 78.2802 Fuel Consumption: 47.8458\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -837.2467409833991 Explore P: 0.0809 SOC: 0.5483 Cumulative_SOC_deviation: 78.8687 Fuel Consumption: 48.5594\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -826.7693930000017 Explore P: 0.0790 SOC: 0.5472 Cumulative_SOC_deviation: 77.8840 Fuel Consumption: 47.9289\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -858.8526355201756 Explore P: 0.0771 SOC: 0.5401 Cumulative_SOC_deviation: 81.0220 Fuel Consumption: 48.6327\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -852.5571445884133 Explore P: 0.0753 SOC: 0.5401 Cumulative_SOC_deviation: 80.4027 Fuel Consumption: 48.5301\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -852.7958093947949 Explore P: 0.0735 SOC: 0.5427 Cumulative_SOC_deviation: 80.4970 Fuel Consumption: 47.8263\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -886.7718086710424 Explore P: 0.0718 SOC: 0.5406 Cumulative_SOC_deviation: 83.8085 Fuel Consumption: 48.6865\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -874.4625572149888 Explore P: 0.0701 SOC: 0.5441 Cumulative_SOC_deviation: 82.6123 Fuel Consumption: 48.3395\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -889.801244336719 Explore P: 0.0685 SOC: 0.5452 Cumulative_SOC_deviation: 84.1066 Fuel Consumption: 48.7354\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -870.269870850651 Explore P: 0.0669 SOC: 0.5383 Cumulative_SOC_deviation: 82.2610 Fuel Consumption: 47.6601\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -894.2434186778903 Explore P: 0.0654 SOC: 0.5457 Cumulative_SOC_deviation: 84.7155 Fuel Consumption: 47.0883\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -888.9576662080999 Explore P: 0.0639 SOC: 0.5532 Cumulative_SOC_deviation: 84.0398 Fuel Consumption: 48.5594\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -893.8876184609252 Explore P: 0.0624 SOC: 0.5483 Cumulative_SOC_deviation: 84.5822 Fuel Consumption: 48.0658\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -889.8952962916013 Explore P: 0.0610 SOC: 0.5629 Cumulative_SOC_deviation: 83.9659 Fuel Consumption: 50.2358\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -896.0023458695745 Explore P: 0.0596 SOC: 0.5378 Cumulative_SOC_deviation: 84.7472 Fuel Consumption: 48.5301\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -939.7141441186351 Explore P: 0.0583 SOC: 0.5370 Cumulative_SOC_deviation: 89.1834 Fuel Consumption: 47.8800\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -954.1797100304373 Explore P: 0.0570 SOC: 0.5376 Cumulative_SOC_deviation: 90.6461 Fuel Consumption: 47.7187\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -887.5742542871429 Explore P: 0.0557 SOC: 0.5386 Cumulative_SOC_deviation: 83.9689 Fuel Consumption: 47.8849\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -925.9905166247647 Explore P: 0.0545 SOC: 0.5379 Cumulative_SOC_deviation: 87.8643 Fuel Consumption: 47.3473\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -920.8293244551833 Explore P: 0.0533 SOC: 0.5395 Cumulative_SOC_deviation: 87.2847 Fuel Consumption: 47.9827\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -919.977471209951 Explore P: 0.0521 SOC: 0.5449 Cumulative_SOC_deviation: 87.1081 Fuel Consumption: 48.8966\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -903.2700544654584 Explore P: 0.0510 SOC: 0.5426 Cumulative_SOC_deviation: 85.3240 Fuel Consumption: 50.0306\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -911.4276903979022 Explore P: 0.0498 SOC: 0.5412 Cumulative_SOC_deviation: 86.3318 Fuel Consumption: 48.1097\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -907.4443840118299 Explore P: 0.0488 SOC: 0.5424 Cumulative_SOC_deviation: 85.8479 Fuel Consumption: 48.9651\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -915.8758785048966 Explore P: 0.0477 SOC: 0.5420 Cumulative_SOC_deviation: 86.7385 Fuel Consumption: 48.4910\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -903.4787367363351 Explore P: 0.0467 SOC: 0.5448 Cumulative_SOC_deviation: 85.5139 Fuel Consumption: 48.3395\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -895.6065114947199 Explore P: 0.0457 SOC: 0.5429 Cumulative_SOC_deviation: 84.9354 Fuel Consumption: 46.2525\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -899.3099691431337 Explore P: 0.0447 SOC: 0.5427 Cumulative_SOC_deviation: 85.1347 Fuel Consumption: 47.9631\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -911.9527231165944 Explore P: 0.0438 SOC: 0.5469 Cumulative_SOC_deviation: 86.4342 Fuel Consumption: 47.6112\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -885.2693365525339 Explore P: 0.0429 SOC: 0.5424 Cumulative_SOC_deviation: 83.8323 Fuel Consumption: 46.9465\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -872.0044423827366 Explore P: 0.0420 SOC: 0.5489 Cumulative_SOC_deviation: 82.4946 Fuel Consumption: 47.0589\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -905.6544087992332 Explore P: 0.0411 SOC: 0.5437 Cumulative_SOC_deviation: 85.8928 Fuel Consumption: 46.7266\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -915.2831589426779 Explore P: 0.0403 SOC: 0.5460 Cumulative_SOC_deviation: 86.7784 Fuel Consumption: 47.4988\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -844.0054385794932 Explore P: 0.0395 SOC: 0.5480 Cumulative_SOC_deviation: 79.5035 Fuel Consumption: 48.9700\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -827.6717165047579 Explore P: 0.0387 SOC: 0.5454 Cumulative_SOC_deviation: 77.7670 Fuel Consumption: 50.0012\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -876.7548030042158 Explore P: 0.0379 SOC: 0.5437 Cumulative_SOC_deviation: 83.0214 Fuel Consumption: 46.5408\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -854.2944383769495 Explore P: 0.0371 SOC: 0.5507 Cumulative_SOC_deviation: 80.6458 Fuel Consumption: 47.8360\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -840.238274572708 Explore P: 0.0364 SOC: 0.5478 Cumulative_SOC_deviation: 79.2339 Fuel Consumption: 47.8996\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -813.7241365725374 Explore P: 0.0357 SOC: 0.5459 Cumulative_SOC_deviation: 76.5101 Fuel Consumption: 48.6229\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -801.3847186529852 Explore P: 0.0350 SOC: 0.5492 Cumulative_SOC_deviation: 75.1887 Fuel Consumption: 49.4978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -797.6772760243053 Explore P: 0.0343 SOC: 0.5500 Cumulative_SOC_deviation: 74.9059 Fuel Consumption: 48.6181\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -783.819400451901 Explore P: 0.0336 SOC: 0.5473 Cumulative_SOC_deviation: 73.4463 Fuel Consumption: 49.3561\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -768.7463674380273 Explore P: 0.0330 SOC: 0.5503 Cumulative_SOC_deviation: 72.0099 Fuel Consumption: 48.6474\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -783.7494630677761 Explore P: 0.0324 SOC: 0.5505 Cumulative_SOC_deviation: 73.6368 Fuel Consumption: 47.3815\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -769.790416490691 Explore P: 0.0318 SOC: 0.5505 Cumulative_SOC_deviation: 72.1627 Fuel Consumption: 48.1635\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -747.5297589222446 Explore P: 0.0312 SOC: 0.5532 Cumulative_SOC_deviation: 69.8457 Fuel Consumption: 49.0726\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -723.5897784322518 Explore P: 0.0306 SOC: 0.5528 Cumulative_SOC_deviation: 67.6071 Fuel Consumption: 47.5184\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -725.9157025941187 Explore P: 0.0301 SOC: 0.5518 Cumulative_SOC_deviation: 67.7043 Fuel Consumption: 48.8722\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -714.3160911258076 Explore P: 0.0295 SOC: 0.5574 Cumulative_SOC_deviation: 66.6055 Fuel Consumption: 48.2613\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -739.6258624605348 Explore P: 0.0290 SOC: 0.5567 Cumulative_SOC_deviation: 69.1291 Fuel Consumption: 48.3346\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -708.2217307354216 Explore P: 0.0285 SOC: 0.5542 Cumulative_SOC_deviation: 65.9164 Fuel Consumption: 49.0579\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -721.4362681423625 Explore P: 0.0280 SOC: 0.5564 Cumulative_SOC_deviation: 67.2940 Fuel Consumption: 48.4959\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -678.0171692920972 Explore P: 0.0275 SOC: 0.5581 Cumulative_SOC_deviation: 62.9702 Fuel Consumption: 48.3150\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -667.5964056681039 Explore P: 0.0270 SOC: 0.5595 Cumulative_SOC_deviation: 61.8235 Fuel Consumption: 49.3610\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -662.9421770719146 Explore P: 0.0265 SOC: 0.5630 Cumulative_SOC_deviation: 61.3630 Fuel Consumption: 49.3121\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -650.3207304821561 Explore P: 0.0261 SOC: 0.5614 Cumulative_SOC_deviation: 60.0065 Fuel Consumption: 50.2554\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -707.0990926703644 Explore P: 0.0257 SOC: 0.5622 Cumulative_SOC_deviation: 65.8735 Fuel Consumption: 48.3639\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -655.1606299562862 Explore P: 0.0252 SOC: 0.5637 Cumulative_SOC_deviation: 60.7530 Fuel Consumption: 47.6308\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -649.5426137736205 Explore P: 0.0248 SOC: 0.5600 Cumulative_SOC_deviation: 60.2454 Fuel Consumption: 47.0883\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -612.897776777527 Explore P: 0.0244 SOC: 0.5653 Cumulative_SOC_deviation: 56.4993 Fuel Consumption: 47.9045\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -583.7779744773363 Explore P: 0.0240 SOC: 0.5644 Cumulative_SOC_deviation: 53.5370 Fuel Consumption: 48.4079\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -544.230735500177 Explore P: 0.0237 SOC: 0.5668 Cumulative_SOC_deviation: 49.3228 Fuel Consumption: 51.0032\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -558.2031964469646 Explore P: 0.0233 SOC: 0.5678 Cumulative_SOC_deviation: 50.8280 Fuel Consumption: 49.9230\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -571.2068257123298 Explore P: 0.0229 SOC: 0.5670 Cumulative_SOC_deviation: 52.2618 Fuel Consumption: 48.5887\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -523.7627371782473 Explore P: 0.0226 SOC: 0.5698 Cumulative_SOC_deviation: 47.3454 Fuel Consumption: 50.3091\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -484.6907520232601 Explore P: 0.0222 SOC: 0.5730 Cumulative_SOC_deviation: 43.4914 Fuel Consumption: 49.7764\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -476.8061981424267 Explore P: 0.0219 SOC: 0.5713 Cumulative_SOC_deviation: 42.7714 Fuel Consumption: 49.0921\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -453.3868004973678 Explore P: 0.0216 SOC: 0.5754 Cumulative_SOC_deviation: 40.3273 Fuel Consumption: 50.1136\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -408.3031281636281 Explore P: 0.0213 SOC: 0.5751 Cumulative_SOC_deviation: 35.9402 Fuel Consumption: 48.9015\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -419.5734259623054 Explore P: 0.0210 SOC: 0.5751 Cumulative_SOC_deviation: 37.0994 Fuel Consumption: 48.5790\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -391.3755718203309 Explore P: 0.0207 SOC: 0.5811 Cumulative_SOC_deviation: 34.2195 Fuel Consumption: 49.1801\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -342.14748257147306 Explore P: 0.0204 SOC: 0.5844 Cumulative_SOC_deviation: 29.1179 Fuel Consumption: 50.9690\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -340.3024930742345 Explore P: 0.0201 SOC: 0.5852 Cumulative_SOC_deviation: 28.9094 Fuel Consumption: 51.2084\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -295.6495895282007 Explore P: 0.0198 SOC: 0.5861 Cumulative_SOC_deviation: 24.5052 Fuel Consumption: 50.5975\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -274.65705858846457 Explore P: 0.0196 SOC: 0.5899 Cumulative_SOC_deviation: 22.3781 Fuel Consumption: 50.8761\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -282.8139721504609 Explore P: 0.0193 SOC: 0.5915 Cumulative_SOC_deviation: 23.2153 Fuel Consumption: 50.6610\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -294.06576527027016 Explore P: 0.0190 SOC: 0.5858 Cumulative_SOC_deviation: 24.4578 Fuel Consumption: 49.4880\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -291.25766827091553 Explore P: 0.0188 SOC: 0.5885 Cumulative_SOC_deviation: 24.2762 Fuel Consumption: 48.4959\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -274.80716352104076 Explore P: 0.0186 SOC: 0.5895 Cumulative_SOC_deviation: 22.5441 Fuel Consumption: 49.3658\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -276.44309180288514 Explore P: 0.0183 SOC: 0.5917 Cumulative_SOC_deviation: 22.7805 Fuel Consumption: 48.6376\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -261.8082577223908 Explore P: 0.0181 SOC: 0.5849 Cumulative_SOC_deviation: 21.3156 Fuel Consumption: 48.6523\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -292.06841843363236 Explore P: 0.0179 SOC: 0.5898 Cumulative_SOC_deviation: 24.3133 Fuel Consumption: 48.9357\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -273.9653019385976 Explore P: 0.0177 SOC: 0.5882 Cumulative_SOC_deviation: 22.4028 Fuel Consumption: 49.9377\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -252.74051720265638 Explore P: 0.0175 SOC: 0.5992 Cumulative_SOC_deviation: 20.2138 Fuel Consumption: 50.6024\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -181.19738359343305 Explore P: 0.0173 SOC: 0.5985 Cumulative_SOC_deviation: 12.8865 Fuel Consumption: 52.3326\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -173.97813731953474 Explore P: 0.0171 SOC: 0.5977 Cumulative_SOC_deviation: 12.1514 Fuel Consumption: 52.4645\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -158.24148850656803 Explore P: 0.0169 SOC: 0.5950 Cumulative_SOC_deviation: 10.7488 Fuel Consumption: 50.7539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -146.06786146325112 Explore P: 0.0167 SOC: 0.6033 Cumulative_SOC_deviation: 9.4635 Fuel Consumption: 51.4333\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -150.9523148744065 Explore P: 0.0165 SOC: 0.6059 Cumulative_SOC_deviation: 9.9211 Fuel Consumption: 51.7412\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -156.26426252241887 Explore P: 0.0163 SOC: 0.6092 Cumulative_SOC_deviation: 10.2861 Fuel Consumption: 53.4030\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -171.0258488344473 Explore P: 0.0162 SOC: 0.6057 Cumulative_SOC_deviation: 11.9260 Fuel Consumption: 51.7656\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -311.034938375568 Explore P: 0.0160 SOC: 0.6205 Cumulative_SOC_deviation: 25.7862 Fuel Consumption: 53.1732\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -167.82957043905031 Explore P: 0.0158 SOC: 0.6140 Cumulative_SOC_deviation: 11.5458 Fuel Consumption: 52.3717\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -194.31887501938223 Explore P: 0.0157 SOC: 0.6255 Cumulative_SOC_deviation: 14.0232 Fuel Consumption: 54.0872\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -474.01946376986916 Explore P: 0.0155 SOC: 0.8694 Cumulative_SOC_deviation: 40.2792 Fuel Consumption: 71.2278\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -329.2703167338358 Explore P: 0.0154 SOC: 0.6431 Cumulative_SOC_deviation: 27.5613 Fuel Consumption: 53.6571\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -446.871641443135 Explore P: 0.0152 SOC: 0.6563 Cumulative_SOC_deviation: 39.1387 Fuel Consumption: 55.4850\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -260.57293615640504 Explore P: 0.0151 SOC: 0.6175 Cumulative_SOC_deviation: 20.8617 Fuel Consumption: 51.9562\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -206.28367580835567 Explore P: 0.0149 SOC: 0.6146 Cumulative_SOC_deviation: 15.4049 Fuel Consumption: 52.2348\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -3165.8060851422156 Explore P: 0.0148 SOC: 1.0000 Cumulative_SOC_deviation: 303.9569 Fuel Consumption: 126.2371\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -195.1970022998312 Explore P: 0.0147 SOC: 0.6057 Cumulative_SOC_deviation: 14.4663 Fuel Consumption: 50.5340\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -192.71012135649022 Explore P: 0.0146 SOC: 0.5995 Cumulative_SOC_deviation: 14.2308 Fuel Consumption: 50.4020\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -207.01312950789134 Explore P: 0.0144 SOC: 0.5957 Cumulative_SOC_deviation: 15.6416 Fuel Consumption: 50.5975\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -222.79995555262155 Explore P: 0.0143 SOC: 0.6064 Cumulative_SOC_deviation: 17.2105 Fuel Consumption: 50.6953\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -256.3868471615504 Explore P: 0.0142 SOC: 0.5941 Cumulative_SOC_deviation: 20.6444 Fuel Consumption: 49.9426\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -231.59775002727574 Explore P: 0.0141 SOC: 0.5948 Cumulative_SOC_deviation: 18.2638 Fuel Consumption: 48.9602\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN3_2_mass1200.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

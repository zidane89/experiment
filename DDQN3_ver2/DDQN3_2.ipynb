{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN3_2 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 2\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -4893.4991342546255 Explore P: 0.9217 SOC: 1.0000 Cumulative_SOC_deviation: 476.6733 Fuel Consumption: 126.7666\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -4937.093054559408 Explore P: 0.8970 SOC: 1.0000 Cumulative_SOC_deviation: 480.5977 Fuel Consumption: 131.1165\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -4858.562350807958 Explore P: 0.8730 SOC: 1.0000 Cumulative_SOC_deviation: 473.3848 Fuel Consumption: 124.7139\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -4758.352614904821 Explore P: 0.8496 SOC: 1.0000 Cumulative_SOC_deviation: 463.7461 Fuel Consumption: 120.8918\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -4687.061846193589 Explore P: 0.8269 SOC: 1.0000 Cumulative_SOC_deviation: 456.8614 Fuel Consumption: 118.4480\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -4721.42297818245 Explore P: 0.8048 SOC: 1.0000 Cumulative_SOC_deviation: 460.4441 Fuel Consumption: 116.9818\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -4633.9262920835035 Explore P: 0.7832 SOC: 1.0000 Cumulative_SOC_deviation: 451.7551 Fuel Consumption: 116.3757\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -4622.9253868502055 Explore P: 0.7623 SOC: 1.0000 Cumulative_SOC_deviation: 450.7371 Fuel Consumption: 115.5546\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -4662.476638874854 Explore P: 0.7419 SOC: 1.0000 Cumulative_SOC_deviation: 454.8867 Fuel Consumption: 113.6094\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -4624.207606617536 Explore P: 0.7221 SOC: 1.0000 Cumulative_SOC_deviation: 451.6180 Fuel Consumption: 108.0278\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -4558.816824351614 Explore P: 0.7028 SOC: 1.0000 Cumulative_SOC_deviation: 444.5149 Fuel Consumption: 113.6680\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -4480.438956887699 Explore P: 0.6840 SOC: 1.0000 Cumulative_SOC_deviation: 436.9919 Fuel Consumption: 110.5204\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -4425.00667707619 Explore P: 0.6658 SOC: 1.0000 Cumulative_SOC_deviation: 431.6666 Fuel Consumption: 108.3406\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -4522.681851622225 Explore P: 0.6480 SOC: 1.0000 Cumulative_SOC_deviation: 441.2132 Fuel Consumption: 110.5498\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -4094.4868617480806 Explore P: 0.6307 SOC: 1.0000 Cumulative_SOC_deviation: 398.3996 Fuel Consumption: 110.4911\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -4007.2479653521555 Explore P: 0.6139 SOC: 1.0000 Cumulative_SOC_deviation: 390.0901 Fuel Consumption: 106.3465\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -4455.425505666418 Explore P: 0.5976 SOC: 1.0000 Cumulative_SOC_deviation: 434.0516 Fuel Consumption: 114.9094\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -4065.004293910044 Explore P: 0.5816 SOC: 1.0000 Cumulative_SOC_deviation: 395.3839 Fuel Consumption: 111.1656\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -4137.578637523524 Explore P: 0.5662 SOC: 1.0000 Cumulative_SOC_deviation: 402.7127 Fuel Consumption: 110.4520\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -3944.6007396232703 Explore P: 0.5511 SOC: 1.0000 Cumulative_SOC_deviation: 383.9320 Fuel Consumption: 105.2810\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -3941.798426088803 Explore P: 0.5364 SOC: 1.0000 Cumulative_SOC_deviation: 385.2470 Fuel Consumption: 89.3281\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -4203.440912813312 Explore P: 0.5222 SOC: 1.0000 Cumulative_SOC_deviation: 409.5570 Fuel Consumption: 107.8714\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -3883.6148950481597 Explore P: 0.5083 SOC: 1.0000 Cumulative_SOC_deviation: 378.3055 Fuel Consumption: 100.5596\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -3245.8685498167138 Explore P: 0.4948 SOC: 1.0000 Cumulative_SOC_deviation: 315.5749 Fuel Consumption: 90.1199\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -3554.9342300885883 Explore P: 0.4817 SOC: 1.0000 Cumulative_SOC_deviation: 345.9575 Fuel Consumption: 95.3593\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -3559.7699915209714 Explore P: 0.4689 SOC: 1.0000 Cumulative_SOC_deviation: 346.5232 Fuel Consumption: 94.5382\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -3243.2951449384514 Explore P: 0.4565 SOC: 1.0000 Cumulative_SOC_deviation: 315.3742 Fuel Consumption: 89.5529\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -3887.515665672324 Explore P: 0.4444 SOC: 1.0000 Cumulative_SOC_deviation: 377.6135 Fuel Consumption: 111.3806\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -3702.015403725302 Explore P: 0.4326 SOC: 1.0000 Cumulative_SOC_deviation: 360.5327 Fuel Consumption: 96.6887\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -3125.282743285148 Explore P: 0.4212 SOC: 1.0000 Cumulative_SOC_deviation: 304.3081 Fuel Consumption: 82.2020\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -3513.147817825906 Explore P: 0.4100 SOC: 1.0000 Cumulative_SOC_deviation: 342.9401 Fuel Consumption: 83.7465\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -3281.047842921077 Explore P: 0.3992 SOC: 1.0000 Cumulative_SOC_deviation: 319.0508 Fuel Consumption: 90.5402\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -4037.1471295706087 Explore P: 0.3887 SOC: 1.0000 Cumulative_SOC_deviation: 394.1006 Fuel Consumption: 96.1413\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -3405.018413384918 Explore P: 0.3784 SOC: 1.0000 Cumulative_SOC_deviation: 331.8075 Fuel Consumption: 86.9429\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -3552.2088551378724 Explore P: 0.3684 SOC: 1.0000 Cumulative_SOC_deviation: 346.9674 Fuel Consumption: 82.5344\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -2440.772689188628 Explore P: 0.3587 SOC: 1.0000 Cumulative_SOC_deviation: 236.3312 Fuel Consumption: 77.4611\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -3020.950345889645 Explore P: 0.3493 SOC: 1.0000 Cumulative_SOC_deviation: 292.2668 Fuel Consumption: 98.2820\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -3035.0329524142862 Explore P: 0.3401 SOC: 1.0000 Cumulative_SOC_deviation: 295.0807 Fuel Consumption: 84.2255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -2687.5049246382464 Explore P: 0.3311 SOC: 1.0000 Cumulative_SOC_deviation: 260.0513 Fuel Consumption: 86.9918\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -3424.726119699494 Explore P: 0.3224 SOC: 1.0000 Cumulative_SOC_deviation: 332.8233 Fuel Consumption: 96.4932\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -3052.0367584755036 Explore P: 0.3140 SOC: 1.0000 Cumulative_SOC_deviation: 295.2816 Fuel Consumption: 99.2204\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -3579.8306859170502 Explore P: 0.3057 SOC: 1.0000 Cumulative_SOC_deviation: 348.8166 Fuel Consumption: 91.6643\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -3313.1304188583126 Explore P: 0.2977 SOC: 1.0000 Cumulative_SOC_deviation: 320.0968 Fuel Consumption: 112.1627\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -2825.831524743373 Explore P: 0.2899 SOC: 1.0000 Cumulative_SOC_deviation: 274.6015 Fuel Consumption: 79.8169\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -2487.7606858677204 Explore P: 0.2824 SOC: 1.0000 Cumulative_SOC_deviation: 239.5129 Fuel Consumption: 92.6320\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -2670.9275687782024 Explore P: 0.2750 SOC: 1.0000 Cumulative_SOC_deviation: 257.6037 Fuel Consumption: 94.8901\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -2504.63540717013 Explore P: 0.2678 SOC: 1.0000 Cumulative_SOC_deviation: 241.4936 Fuel Consumption: 89.6995\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -2446.1444889026016 Explore P: 0.2608 SOC: 1.0000 Cumulative_SOC_deviation: 235.9211 Fuel Consumption: 86.9332\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -1843.372146839106 Explore P: 0.2540 SOC: 1.0000 Cumulative_SOC_deviation: 176.9518 Fuel Consumption: 73.8541\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -2333.1091143193416 Explore P: 0.2474 SOC: 1.0000 Cumulative_SOC_deviation: 224.4495 Fuel Consumption: 88.6145\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -1720.0918554243403 Explore P: 0.2410 SOC: 1.0000 Cumulative_SOC_deviation: 164.0627 Fuel Consumption: 79.4650\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -2177.9100495103808 Explore P: 0.2347 SOC: 1.0000 Cumulative_SOC_deviation: 209.6011 Fuel Consumption: 81.8990\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -1978.8649967562899 Explore P: 0.2286 SOC: 1.0000 Cumulative_SOC_deviation: 189.7924 Fuel Consumption: 80.9411\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -2158.7207822458713 Explore P: 0.2227 SOC: 1.0000 Cumulative_SOC_deviation: 207.9803 Fuel Consumption: 78.9176\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -2046.3120887186635 Explore P: 0.2170 SOC: 1.0000 Cumulative_SOC_deviation: 196.7649 Fuel Consumption: 78.6635\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -2417.4715015550696 Explore P: 0.2114 SOC: 1.0000 Cumulative_SOC_deviation: 232.7977 Fuel Consumption: 89.4942\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -1388.1747157625234 Explore P: 0.2059 SOC: 0.9008 Cumulative_SOC_deviation: 132.4672 Fuel Consumption: 63.5023\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -1553.8968035306123 Explore P: 0.2006 SOC: 0.9452 Cumulative_SOC_deviation: 148.7912 Fuel Consumption: 65.9852\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -1578.747499952306 Explore P: 0.1954 SOC: 0.9515 Cumulative_SOC_deviation: 151.2068 Fuel Consumption: 66.6792\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -1776.404834582576 Explore P: 0.1904 SOC: 1.0000 Cumulative_SOC_deviation: 170.1348 Fuel Consumption: 75.0565\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -1550.6406855338769 Explore P: 0.1855 SOC: 0.9357 Cumulative_SOC_deviation: 148.5623 Fuel Consumption: 65.0174\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -1144.896437775776 Explore P: 0.1808 SOC: 0.8855 Cumulative_SOC_deviation: 108.3056 Fuel Consumption: 61.8405\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -1513.7441476808162 Explore P: 0.1761 SOC: 1.0000 Cumulative_SOC_deviation: 143.9783 Fuel Consumption: 73.9616\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -1735.9911054620065 Explore P: 0.1716 SOC: 1.0000 Cumulative_SOC_deviation: 164.9205 Fuel Consumption: 86.7865\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -2280.204136821157 Explore P: 0.1673 SOC: 1.0000 Cumulative_SOC_deviation: 218.7953 Fuel Consumption: 92.2508\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -1242.3123834178384 Explore P: 0.1630 SOC: 0.9354 Cumulative_SOC_deviation: 117.6415 Fuel Consumption: 65.8972\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -915.6551395632287 Explore P: 0.1589 SOC: 0.8677 Cumulative_SOC_deviation: 85.5164 Fuel Consumption: 60.4916\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -866.1066480352708 Explore P: 0.1548 SOC: 0.8369 Cumulative_SOC_deviation: 80.7531 Fuel Consumption: 58.5757\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -708.0480132760168 Explore P: 0.1509 SOC: 0.7952 Cumulative_SOC_deviation: 65.3050 Fuel Consumption: 54.9980\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -1529.9248358828 Explore P: 0.1471 SOC: 1.0000 Cumulative_SOC_deviation: 144.8143 Fuel Consumption: 81.7817\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -768.9911220822233 Explore P: 0.1434 SOC: 0.8957 Cumulative_SOC_deviation: 70.6984 Fuel Consumption: 62.0067\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -1289.3190734970017 Explore P: 0.1398 SOC: 1.0000 Cumulative_SOC_deviation: 121.9365 Fuel Consumption: 69.9539\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -1166.926858628618 Explore P: 0.1362 SOC: 0.9993 Cumulative_SOC_deviation: 109.6406 Fuel Consumption: 70.5208\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -536.2293756874236 Explore P: 0.1328 SOC: 0.7394 Cumulative_SOC_deviation: 48.4702 Fuel Consumption: 51.5278\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -830.6614639981821 Explore P: 0.1295 SOC: 0.8852 Cumulative_SOC_deviation: 76.9163 Fuel Consumption: 61.4984\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -1019.4690831618642 Explore P: 0.1263 SOC: 0.9452 Cumulative_SOC_deviation: 95.2888 Fuel Consumption: 66.5815\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -698.8804841233291 Explore P: 0.1231 SOC: 0.7373 Cumulative_SOC_deviation: 64.7714 Fuel Consumption: 51.1662\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -457.0920775618924 Explore P: 0.1200 SOC: 0.8258 Cumulative_SOC_deviation: 39.9377 Fuel Consumption: 57.7155\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -506.8463968444627 Explore P: 0.1171 SOC: 0.7247 Cumulative_SOC_deviation: 45.8095 Fuel Consumption: 48.7517\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -1447.7706925548698 Explore P: 0.1142 SOC: 1.0000 Cumulative_SOC_deviation: 136.1258 Fuel Consumption: 86.5128\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -363.7254980850339 Explore P: 0.1113 SOC: 0.6934 Cumulative_SOC_deviation: 31.6665 Fuel Consumption: 47.0606\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -1068.2492232855107 Explore P: 0.1086 SOC: 1.0000 Cumulative_SOC_deviation: 99.7377 Fuel Consumption: 70.8727\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -379.6658471824822 Explore P: 0.1059 SOC: 0.6931 Cumulative_SOC_deviation: 33.2732 Fuel Consumption: 46.9336\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -402.9242581908271 Explore P: 0.1033 SOC: 0.6619 Cumulative_SOC_deviation: 35.6470 Fuel Consumption: 46.4546\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -489.3973625998254 Explore P: 0.1008 SOC: 0.6868 Cumulative_SOC_deviation: 44.1301 Fuel Consumption: 48.0968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -372.53468045701896 Explore P: 0.0983 SOC: 0.6608 Cumulative_SOC_deviation: 32.6745 Fuel Consumption: 45.7899\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -360.9921455604211 Explore P: 0.0960 SOC: 0.6527 Cumulative_SOC_deviation: 31.5593 Fuel Consumption: 45.3989\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -326.56956005758593 Explore P: 0.0936 SOC: 0.6470 Cumulative_SOC_deviation: 28.1972 Fuel Consumption: 44.5973\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -418.6192110136738 Explore P: 0.0914 SOC: 0.6702 Cumulative_SOC_deviation: 37.1959 Fuel Consumption: 46.6599\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -346.802261038443 Explore P: 0.0892 SOC: 0.6637 Cumulative_SOC_deviation: 30.0876 Fuel Consumption: 45.9267\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -277.08330422248633 Explore P: 0.0870 SOC: 0.6374 Cumulative_SOC_deviation: 23.3444 Fuel Consumption: 43.6394\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -328.47127208096765 Explore P: 0.0849 SOC: 0.6708 Cumulative_SOC_deviation: 28.1665 Fuel Consumption: 46.8065\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -296.9834477601729 Explore P: 0.0829 SOC: 0.6150 Cumulative_SOC_deviation: 25.3168 Fuel Consumption: 43.8153\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -264.92936535690956 Explore P: 0.0809 SOC: 0.6072 Cumulative_SOC_deviation: 22.2668 Fuel Consumption: 42.2611\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -282.5292642703962 Explore P: 0.0790 SOC: 0.6198 Cumulative_SOC_deviation: 24.0122 Fuel Consumption: 42.4077\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -237.88560358319853 Explore P: 0.0771 SOC: 0.6036 Cumulative_SOC_deviation: 19.6837 Fuel Consumption: 41.0490\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -358.9946969421309 Explore P: 0.0753 SOC: 0.6810 Cumulative_SOC_deviation: 31.1787 Fuel Consumption: 47.2073\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -301.29813494382597 Explore P: 0.0735 SOC: 0.6043 Cumulative_SOC_deviation: 26.0005 Fuel Consumption: 41.2933\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -357.053764436097 Explore P: 0.0718 SOC: 0.5944 Cumulative_SOC_deviation: 31.6239 Fuel Consumption: 40.8144\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -364.2850678102364 Explore P: 0.0701 SOC: 0.5972 Cumulative_SOC_deviation: 32.3129 Fuel Consumption: 41.1565\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -352.04396960243264 Explore P: 0.0685 SOC: 0.6179 Cumulative_SOC_deviation: 30.8590 Fuel Consumption: 43.4536\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -247.56678012634362 Explore P: 0.0669 SOC: 0.6092 Cumulative_SOC_deviation: 20.5716 Fuel Consumption: 41.8505\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -268.55620171467484 Explore P: 0.0654 SOC: 0.6271 Cumulative_SOC_deviation: 22.5220 Fuel Consumption: 43.3363\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -260.9502598475674 Explore P: 0.0639 SOC: 0.6204 Cumulative_SOC_deviation: 21.7282 Fuel Consumption: 43.6687\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -248.11902718310944 Explore P: 0.0624 SOC: 0.6046 Cumulative_SOC_deviation: 20.6679 Fuel Consumption: 41.4400\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -291.2945094689875 Explore P: 0.0610 SOC: 0.6074 Cumulative_SOC_deviation: 24.9669 Fuel Consumption: 41.6257\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -330.6266311897678 Explore P: 0.0596 SOC: 0.6311 Cumulative_SOC_deviation: 28.6899 Fuel Consumption: 43.7273\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -223.13345858473048 Explore P: 0.0583 SOC: 0.6273 Cumulative_SOC_deviation: 17.9866 Fuel Consumption: 43.2679\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -235.04692576433177 Explore P: 0.0570 SOC: 0.6120 Cumulative_SOC_deviation: 19.2092 Fuel Consumption: 42.9551\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -283.6198503326985 Explore P: 0.0557 SOC: 0.6177 Cumulative_SOC_deviation: 24.1066 Fuel Consumption: 42.5543\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -314.09110632404304 Explore P: 0.0545 SOC: 0.6156 Cumulative_SOC_deviation: 27.1253 Fuel Consumption: 42.8378\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -380.82827254497477 Explore P: 0.0533 SOC: 0.6255 Cumulative_SOC_deviation: 33.6544 Fuel Consumption: 44.2845\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -293.67866174893965 Explore P: 0.0521 SOC: 0.6081 Cumulative_SOC_deviation: 25.1672 Fuel Consumption: 42.0069\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -285.48434186720044 Explore P: 0.0510 SOC: 0.6119 Cumulative_SOC_deviation: 24.2461 Fuel Consumption: 43.0235\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -334.5742079609552 Explore P: 0.0498 SOC: 0.6220 Cumulative_SOC_deviation: 29.1795 Fuel Consumption: 42.7791\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -318.0301711956048 Explore P: 0.0488 SOC: 0.6045 Cumulative_SOC_deviation: 27.5769 Fuel Consumption: 42.2611\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -367.679761703931 Explore P: 0.0477 SOC: 0.5949 Cumulative_SOC_deviation: 32.6074 Fuel Consumption: 41.6061\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -334.6129147748793 Explore P: 0.0467 SOC: 0.6117 Cumulative_SOC_deviation: 29.1922 Fuel Consumption: 42.6912\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -383.286731176303 Explore P: 0.0457 SOC: 0.6011 Cumulative_SOC_deviation: 34.1876 Fuel Consumption: 41.4106\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -340.1656381070792 Explore P: 0.0447 SOC: 0.6068 Cumulative_SOC_deviation: 29.7719 Fuel Consumption: 42.4468\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -357.76835629996486 Explore P: 0.0438 SOC: 0.5932 Cumulative_SOC_deviation: 31.7091 Fuel Consumption: 40.6775\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -450.4426050184483 Explore P: 0.0429 SOC: 0.6027 Cumulative_SOC_deviation: 40.8006 Fuel Consumption: 42.4370\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -384.79694864903706 Explore P: 0.0420 SOC: 0.5900 Cumulative_SOC_deviation: 34.4051 Fuel Consumption: 40.7459\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -472.07867687126685 Explore P: 0.0411 SOC: 0.5958 Cumulative_SOC_deviation: 43.0981 Fuel Consumption: 41.0978\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -376.47312982227044 Explore P: 0.0403 SOC: 0.5964 Cumulative_SOC_deviation: 33.5336 Fuel Consumption: 41.1369\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -462.26619879666345 Explore P: 0.0395 SOC: 0.5943 Cumulative_SOC_deviation: 42.0846 Fuel Consumption: 41.4204\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -453.6847695717222 Explore P: 0.0387 SOC: 0.5824 Cumulative_SOC_deviation: 41.3496 Fuel Consumption: 40.1887\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -448.8450529938131 Explore P: 0.0379 SOC: 0.5930 Cumulative_SOC_deviation: 40.8001 Fuel Consumption: 40.8437\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -509.02803143144945 Explore P: 0.0371 SOC: 0.5765 Cumulative_SOC_deviation: 46.8693 Fuel Consumption: 40.3354\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -556.1272592674849 Explore P: 0.0364 SOC: 0.5870 Cumulative_SOC_deviation: 51.4228 Fuel Consumption: 41.8994\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -476.4216163957663 Explore P: 0.0357 SOC: 0.5854 Cumulative_SOC_deviation: 43.5568 Fuel Consumption: 40.8535\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -568.6814285265943 Explore P: 0.0350 SOC: 0.5919 Cumulative_SOC_deviation: 52.8424 Fuel Consumption: 40.2572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -683.2072145032258 Explore P: 0.0343 SOC: 0.5797 Cumulative_SOC_deviation: 64.2794 Fuel Consumption: 40.4136\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -569.983764922515 Explore P: 0.0336 SOC: 0.5864 Cumulative_SOC_deviation: 52.9629 Fuel Consumption: 40.3549\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -552.3062921476009 Explore P: 0.0330 SOC: 0.5736 Cumulative_SOC_deviation: 51.2900 Fuel Consumption: 39.4067\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -529.8880895484969 Explore P: 0.0324 SOC: 0.5745 Cumulative_SOC_deviation: 48.9778 Fuel Consumption: 40.1105\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -558.3753849041592 Explore P: 0.0318 SOC: 0.5797 Cumulative_SOC_deviation: 51.8314 Fuel Consumption: 40.0617\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -568.658444723443 Explore P: 0.0312 SOC: 0.5822 Cumulative_SOC_deviation: 52.7834 Fuel Consumption: 40.8241\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -548.861437204862 Explore P: 0.0306 SOC: 0.5849 Cumulative_SOC_deviation: 50.7793 Fuel Consumption: 41.0685\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -548.752914095907 Explore P: 0.0301 SOC: 0.5844 Cumulative_SOC_deviation: 50.8242 Fuel Consumption: 40.5113\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -585.8966162505833 Explore P: 0.0295 SOC: 0.5835 Cumulative_SOC_deviation: 54.5141 Fuel Consumption: 40.7557\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -609.0137443601336 Explore P: 0.0290 SOC: 0.5746 Cumulative_SOC_deviation: 56.9529 Fuel Consumption: 39.4849\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -605.9602173606768 Explore P: 0.0285 SOC: 0.5737 Cumulative_SOC_deviation: 56.6182 Fuel Consumption: 39.7782\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -600.0250413100094 Explore P: 0.0280 SOC: 0.5730 Cumulative_SOC_deviation: 56.0530 Fuel Consumption: 39.4947\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -584.4580928393021 Explore P: 0.0275 SOC: 0.5755 Cumulative_SOC_deviation: 54.4787 Fuel Consumption: 39.6707\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -568.2092669214003 Explore P: 0.0270 SOC: 0.5709 Cumulative_SOC_deviation: 52.7893 Fuel Consumption: 40.3158\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -464.08142436915904 Explore P: 0.0265 SOC: 0.5983 Cumulative_SOC_deviation: 42.2143 Fuel Consumption: 41.9385\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -525.6305215231469 Explore P: 0.0261 SOC: 0.5734 Cumulative_SOC_deviation: 48.6009 Fuel Consumption: 39.6218\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -548.3250788400939 Explore P: 0.0257 SOC: 0.5777 Cumulative_SOC_deviation: 50.8733 Fuel Consumption: 39.5925\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -509.176587665096 Explore P: 0.0252 SOC: 0.5830 Cumulative_SOC_deviation: 46.8460 Fuel Consumption: 40.7166\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -565.438153392814 Explore P: 0.0248 SOC: 0.5769 Cumulative_SOC_deviation: 52.5758 Fuel Consumption: 39.6804\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -501.2787711311933 Explore P: 0.0244 SOC: 0.5854 Cumulative_SOC_deviation: 46.1188 Fuel Consumption: 40.0910\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -489.9218170256484 Explore P: 0.0240 SOC: 0.5882 Cumulative_SOC_deviation: 44.9743 Fuel Consumption: 40.1790\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -445.84580532246287 Explore P: 0.0237 SOC: 0.5862 Cumulative_SOC_deviation: 40.5872 Fuel Consumption: 39.9737\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -440.9564671075893 Explore P: 0.0233 SOC: 0.5822 Cumulative_SOC_deviation: 40.0602 Fuel Consumption: 40.3549\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -427.0182457683004 Explore P: 0.0229 SOC: 0.5784 Cumulative_SOC_deviation: 38.7103 Fuel Consumption: 39.9150\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -409.7457000870875 Explore P: 0.0226 SOC: 0.5801 Cumulative_SOC_deviation: 36.9743 Fuel Consumption: 40.0030\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -412.3363431957196 Explore P: 0.0222 SOC: 0.5859 Cumulative_SOC_deviation: 37.2040 Fuel Consumption: 40.2963\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -357.15299786379217 Explore P: 0.0219 SOC: 0.5920 Cumulative_SOC_deviation: 31.6231 Fuel Consumption: 40.9219\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -353.8293897331182 Explore P: 0.0216 SOC: 0.5971 Cumulative_SOC_deviation: 31.2360 Fuel Consumption: 41.4693\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -275.26079479734005 Explore P: 0.0213 SOC: 0.5962 Cumulative_SOC_deviation: 23.4486 Fuel Consumption: 40.7753\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -283.9133611737138 Explore P: 0.0210 SOC: 0.5962 Cumulative_SOC_deviation: 24.2894 Fuel Consumption: 41.0196\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -279.2861188810228 Explore P: 0.0207 SOC: 0.6108 Cumulative_SOC_deviation: 23.6986 Fuel Consumption: 42.3002\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -246.86639793189357 Explore P: 0.0204 SOC: 0.6010 Cumulative_SOC_deviation: 20.4820 Fuel Consumption: 42.0460\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -204.56763100916575 Explore P: 0.0201 SOC: 0.6063 Cumulative_SOC_deviation: 16.2052 Fuel Consumption: 42.5152\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -184.181602235708 Explore P: 0.0198 SOC: 0.6030 Cumulative_SOC_deviation: 14.3250 Fuel Consumption: 40.9317\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -178.4839492795077 Explore P: 0.0196 SOC: 0.6042 Cumulative_SOC_deviation: 13.6848 Fuel Consumption: 41.6355\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -202.69274367841496 Explore P: 0.0193 SOC: 0.6067 Cumulative_SOC_deviation: 16.1585 Fuel Consumption: 41.1076\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -178.71783206769834 Explore P: 0.0190 SOC: 0.6105 Cumulative_SOC_deviation: 13.6467 Fuel Consumption: 42.2513\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -252.91199349382188 Explore P: 0.0188 SOC: 0.6256 Cumulative_SOC_deviation: 20.9957 Fuel Consumption: 42.9551\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -189.7074204583551 Explore P: 0.0186 SOC: 0.6193 Cumulative_SOC_deviation: 14.7417 Fuel Consumption: 42.2904\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -179.8781998798001 Explore P: 0.0183 SOC: 0.6197 Cumulative_SOC_deviation: 13.6552 Fuel Consumption: 43.3265\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -2735.443189821207 Explore P: 0.0181 SOC: 1.0000 Cumulative_SOC_deviation: 262.4160 Fuel Consumption: 111.2829\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -2572.08091708835 Explore P: 0.0179 SOC: 1.0000 Cumulative_SOC_deviation: 247.8481 Fuel Consumption: 93.5998\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -2599.717894590628 Explore P: 0.0177 SOC: 1.0000 Cumulative_SOC_deviation: 250.1543 Fuel Consumption: 98.1745\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -185.39501344882237 Explore P: 0.0175 SOC: 0.6209 Cumulative_SOC_deviation: 14.3026 Fuel Consumption: 42.3686\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -217.64388270275674 Explore P: 0.0173 SOC: 0.6211 Cumulative_SOC_deviation: 17.5363 Fuel Consumption: 42.2806\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -1237.1444315447886 Explore P: 0.0171 SOC: 0.9052 Cumulative_SOC_deviation: 117.4307 Fuel Consumption: 62.8376\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -290.9444581038118 Explore P: 0.0169 SOC: 0.6612 Cumulative_SOC_deviation: 24.5594 Fuel Consumption: 45.3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -244.14416480088153 Explore P: 0.0167 SOC: 0.6339 Cumulative_SOC_deviation: 20.0642 Fuel Consumption: 43.5025\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -217.8291828845337 Explore P: 0.0165 SOC: 0.6354 Cumulative_SOC_deviation: 17.4014 Fuel Consumption: 43.8153\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -240.44751650898456 Explore P: 0.0163 SOC: 0.6409 Cumulative_SOC_deviation: 19.6798 Fuel Consumption: 43.6491\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -246.03838434308562 Explore P: 0.0162 SOC: 0.6507 Cumulative_SOC_deviation: 20.1607 Fuel Consumption: 44.4311\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -217.78156121889617 Explore P: 0.0160 SOC: 0.6353 Cumulative_SOC_deviation: 17.3712 Fuel Consumption: 44.0695\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -290.0151528760377 Explore P: 0.0158 SOC: 0.6771 Cumulative_SOC_deviation: 24.2935 Fuel Consumption: 47.0802\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -156.6449507719879 Explore P: 0.0157 SOC: 0.6235 Cumulative_SOC_deviation: 11.3377 Fuel Consumption: 43.2679\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -192.79742402414092 Explore P: 0.0155 SOC: 0.6368 Cumulative_SOC_deviation: 15.0096 Fuel Consumption: 42.7009\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -179.2014523436516 Explore P: 0.0154 SOC: 0.6143 Cumulative_SOC_deviation: 13.7038 Fuel Consumption: 42.1633\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -123.4197399057311 Explore P: 0.0152 SOC: 0.6122 Cumulative_SOC_deviation: 8.1139 Fuel Consumption: 42.2806\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -153.66299859959918 Explore P: 0.0151 SOC: 0.6117 Cumulative_SOC_deviation: 11.2203 Fuel Consumption: 41.4595\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -286.03961215270664 Explore P: 0.0149 SOC: 0.7656 Cumulative_SOC_deviation: 23.2312 Fuel Consumption: 53.7272\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -158.10306227912204 Explore P: 0.0148 SOC: 0.6139 Cumulative_SOC_deviation: 11.5519 Fuel Consumption: 42.5836\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -125.78449111526058 Explore P: 0.0147 SOC: 0.6099 Cumulative_SOC_deviation: 8.3895 Fuel Consumption: 41.8896\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -124.75676154063261 Explore P: 0.0146 SOC: 0.6112 Cumulative_SOC_deviation: 8.2867 Fuel Consumption: 41.8896\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -144.60239100808155 Explore P: 0.0144 SOC: 0.6097 Cumulative_SOC_deviation: 10.3065 Fuel Consumption: 41.5377\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -154.459764908517 Explore P: 0.0143 SOC: 0.6344 Cumulative_SOC_deviation: 11.1446 Fuel Consumption: 43.0137\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -231.3176666609443 Explore P: 0.0142 SOC: 0.6214 Cumulative_SOC_deviation: 18.9301 Fuel Consumption: 42.0167\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -140.39392286936885 Explore P: 0.0141 SOC: 0.6101 Cumulative_SOC_deviation: 9.8954 Fuel Consumption: 41.4400\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN3_2.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

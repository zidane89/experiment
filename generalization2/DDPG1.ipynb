{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG1 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):      \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    inputs_batchnorm = layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs_batchnorm)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_input_batchnorm = layers.BatchNormalization()(state_input)\n",
    "    \n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input_batchnorm)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 200\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(actor_model, critic_model, target_actor, target_critic, root): \n",
    "    actor_model.save_weights(\"./{}/actor_model_checkpoint\".format(root))\n",
    "    critic_model.save_weights(\"./{}/critic_model_checkpoint\".format(root))\n",
    "    target_actor.save_weights(\"./{}/target_actor_checkpoint\".format(root))\n",
    "    target_critic.save_weights(\"./{}/target_critic_checkpoint\".format(root))\n",
    "    print(\"model is saved..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Trial 0\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 16.211\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -885.6240229025988 SOC: 0.7760 Cumulative_SOC_deviation: 82.6443 Fuel Consumption: 59.1808\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 16.322\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -1046.5361810815275 SOC: 0.8126 Cumulative_SOC_deviation: 98.4507 Fuel Consumption: 62.0288\n",
      "WARNING:tensorflow:Layer batch_normalization_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.234\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -852.6794627020812 SOC: 0.7754 Cumulative_SOC_deviation: 79.3469 Fuel Consumption: 59.2103\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.681\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -706.672271832264 SOC: 0.7383 Cumulative_SOC_deviation: 65.0568 Fuel Consumption: 56.1039\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.070\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -751.9796875319529 SOC: 0.7147 Cumulative_SOC_deviation: 69.7552 Fuel Consumption: 54.4274\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.485\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -764.1891879718224 SOC: 0.7024 Cumulative_SOC_deviation: 71.0843 Fuel Consumption: 53.3457\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.218\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -752.7178203934913 SOC: 0.6858 Cumulative_SOC_deviation: 70.0562 Fuel Consumption: 52.1560\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.936\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -753.4540568660188 SOC: 0.6805 Cumulative_SOC_deviation: 70.1785 Fuel Consumption: 51.6692\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.502\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -847.3556422895896 SOC: 0.6584 Cumulative_SOC_deviation: 79.7382 Fuel Consumption: 49.9735\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.170\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -873.8056298605488 SOC: 0.6390 Cumulative_SOC_deviation: 82.5289 Fuel Consumption: 48.5163\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.357\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -1018.3924302477396 SOC: 0.6300 Cumulative_SOC_deviation: 97.0442 Fuel Consumption: 47.9504\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.922\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -1182.665330649888 SOC: 0.5795 Cumulative_SOC_deviation: 113.8834 Fuel Consumption: 43.8314\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.815\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -1294.838935583843 SOC: 0.5751 Cumulative_SOC_deviation: 125.0934 Fuel Consumption: 43.9052\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.098\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -1344.3769535934164 SOC: 0.5723 Cumulative_SOC_deviation: 130.0842 Fuel Consumption: 43.5350\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.759\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -1395.6347019468064 SOC: 0.5550 Cumulative_SOC_deviation: 135.3441 Fuel Consumption: 42.1934\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.494\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -1711.6874939141142 SOC: 0.5138 Cumulative_SOC_deviation: 167.2672 Fuel Consumption: 39.0159\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.481\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -1689.657415812242 SOC: 0.5142 Cumulative_SOC_deviation: 165.0483 Fuel Consumption: 39.1742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 88.370\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -1748.2626939998154 SOC: 0.5036 Cumulative_SOC_deviation: 171.0058 Fuel Consumption: 38.2049\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.137\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -1824.3897756552044 SOC: 0.4921 Cumulative_SOC_deviation: 178.7030 Fuel Consumption: 37.3597\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.181\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -1965.5693347624983 SOC: 0.4687 Cumulative_SOC_deviation: 192.9890 Fuel Consumption: 35.6789\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.398\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -1940.6337082978205 SOC: 0.4824 Cumulative_SOC_deviation: 190.3859 Fuel Consumption: 36.7745\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.589\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -2005.7819328231833 SOC: 0.4521 Cumulative_SOC_deviation: 197.1335 Fuel Consumption: 34.4464\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.356\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -2082.3835728003587 SOC: 0.4652 Cumulative_SOC_deviation: 204.6925 Fuel Consumption: 35.4585\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.232\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -2143.0638583844325 SOC: 0.4404 Cumulative_SOC_deviation: 210.9549 Fuel Consumption: 33.5146\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.480\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -2256.805945427359 SOC: 0.4168 Cumulative_SOC_deviation: 222.5021 Fuel Consumption: 31.7846\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.214\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -2509.660705895912 SOC: 0.3823 Cumulative_SOC_deviation: 248.0310 Fuel Consumption: 29.3506\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.409\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -2500.2305689123427 SOC: 0.4001 Cumulative_SOC_deviation: 246.9390 Fuel Consumption: 30.8410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.668\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -2572.8692460769494 SOC: 0.3816 Cumulative_SOC_deviation: 254.3472 Fuel Consumption: 29.3977\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.527\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -2668.8558523329675 SOC: 0.3586 Cumulative_SOC_deviation: 264.1328 Fuel Consumption: 27.5276\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.060\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -2388.1859985606093 SOC: 0.3996 Cumulative_SOC_deviation: 235.7477 Fuel Consumption: 30.7094\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.517\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -2664.7562093266993 SOC: 0.3644 Cumulative_SOC_deviation: 263.6527 Fuel Consumption: 28.2294\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.472\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -2801.549674598616 SOC: 0.3486 Cumulative_SOC_deviation: 277.4624 Fuel Consumption: 26.9252\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.473\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -2915.1496815400783 SOC: 0.3444 Cumulative_SOC_deviation: 288.8339 Fuel Consumption: 26.8108\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.749\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -3009.269342954208 SOC: 0.3263 Cumulative_SOC_deviation: 298.3685 Fuel Consumption: 25.5847\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.463\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -3036.738912715198 SOC: 0.3125 Cumulative_SOC_deviation: 301.2280 Fuel Consumption: 24.4592\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.326\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -3166.9958714976533 SOC: 0.2982 Cumulative_SOC_deviation: 314.3558 Fuel Consumption: 23.4374\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.640\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -3132.891334587297 SOC: 0.3002 Cumulative_SOC_deviation: 310.9201 Fuel Consumption: 23.6899\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.794\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -3129.6737017162864 SOC: 0.2886 Cumulative_SOC_deviation: 310.6950 Fuel Consumption: 22.7238\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.216\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -3188.6974370043554 SOC: 0.2999 Cumulative_SOC_deviation: 316.5033 Fuel Consumption: 23.6643\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.100\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -3035.593760128331 SOC: 0.3064 Cumulative_SOC_deviation: 301.1663 Fuel Consumption: 23.9307\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.347\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -3147.925550966625 SOC: 0.2897 Cumulative_SOC_deviation: 312.5133 Fuel Consumption: 22.7923\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.196\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -3181.62220871328 SOC: 0.2876 Cumulative_SOC_deviation: 315.8768 Fuel Consumption: 22.8544\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.023\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -3379.559407853939 SOC: 0.2719 Cumulative_SOC_deviation: 335.7859 Fuel Consumption: 21.7000\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.888\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -3143.1420926490414 SOC: 0.2937 Cumulative_SOC_deviation: 311.9945 Fuel Consumption: 23.1967\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.113\n",
      "Episode: 45 Exploration P: 0.2977 Total reward: -3734.12262838049 SOC: 0.2717 Cumulative_SOC_deviation: 371.2136 Fuel Consumption: 21.9863\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.117\n",
      "Episode: 46 Exploration P: 0.2899 Total reward: -312.12827848044356 SOC: 0.6177 Cumulative_SOC_deviation: 26.6838 Fuel Consumption: 45.2908\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.537\n",
      "Episode: 47 Exploration P: 0.2824 Total reward: -345.9028491927803 SOC: 0.6137 Cumulative_SOC_deviation: 30.0204 Fuel Consumption: 45.6988\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.319\n",
      "Episode: 48 Exploration P: 0.2750 Total reward: -332.0311932222814 SOC: 0.6116 Cumulative_SOC_deviation: 28.6632 Fuel Consumption: 45.3993\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.229\n",
      "Episode: 49 Exploration P: 0.2678 Total reward: -319.75558686285615 SOC: 0.6116 Cumulative_SOC_deviation: 27.4524 Fuel Consumption: 45.2318\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.296\n",
      "Episode: 50 Exploration P: 0.2608 Total reward: -347.79541233991694 SOC: 0.6119 Cumulative_SOC_deviation: 30.2484 Fuel Consumption: 45.3109\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.233\n",
      "Episode: 51 Exploration P: 0.2540 Total reward: -291.9311886493602 SOC: 0.6066 Cumulative_SOC_deviation: 24.7010 Fuel Consumption: 44.9208\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.034\n",
      "Episode: 52 Exploration P: 0.2474 Total reward: -309.9525472022367 SOC: 0.6099 Cumulative_SOC_deviation: 26.4868 Fuel Consumption: 45.0841\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.056\n",
      "Episode: 53 Exploration P: 0.2410 Total reward: -295.22169098037836 SOC: 0.6072 Cumulative_SOC_deviation: 25.0141 Fuel Consumption: 45.0805\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 86.125\n",
      "Episode: 54 Exploration P: 0.2347 Total reward: -276.8528763844167 SOC: 0.6041 Cumulative_SOC_deviation: 23.2110 Fuel Consumption: 44.7428\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.366\n",
      "Episode: 55 Exploration P: 0.2286 Total reward: -334.1102985851377 SOC: 0.6060 Cumulative_SOC_deviation: 28.9307 Fuel Consumption: 44.8038\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.417\n",
      "Episode: 56 Exploration P: 0.2227 Total reward: -301.81234016871576 SOC: 0.6061 Cumulative_SOC_deviation: 25.6922 Fuel Consumption: 44.8900\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.431\n",
      "Episode: 57 Exploration P: 0.2170 Total reward: -301.2582154572296 SOC: 0.6065 Cumulative_SOC_deviation: 25.6172 Fuel Consumption: 45.0862\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.609\n",
      "Episode: 58 Exploration P: 0.2114 Total reward: -302.138236319292 SOC: 0.6060 Cumulative_SOC_deviation: 25.7140 Fuel Consumption: 44.9984\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.383\n",
      "Episode: 59 Exploration P: 0.2059 Total reward: -305.53114246785157 SOC: 0.6096 Cumulative_SOC_deviation: 26.0402 Fuel Consumption: 45.1294\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.468\n",
      "Episode: 60 Exploration P: 0.2006 Total reward: -272.50411513545015 SOC: 0.6036 Cumulative_SOC_deviation: 22.7627 Fuel Consumption: 44.8776\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.310\n",
      "Episode: 61 Exploration P: 0.1954 Total reward: -295.8180057614603 SOC: 0.6079 Cumulative_SOC_deviation: 25.0534 Fuel Consumption: 45.2842\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.497\n",
      "Episode: 62 Exploration P: 0.1904 Total reward: -292.3768485896009 SOC: 0.6058 Cumulative_SOC_deviation: 24.7214 Fuel Consumption: 45.1626\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.480\n",
      "Episode: 63 Exploration P: 0.1855 Total reward: -287.021245251765 SOC: 0.6072 Cumulative_SOC_deviation: 24.1743 Fuel Consumption: 45.2781\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.412\n",
      "Episode: 64 Exploration P: 0.1808 Total reward: -270.20517393067297 SOC: 0.6017 Cumulative_SOC_deviation: 22.5405 Fuel Consumption: 44.7999\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.450\n",
      "Episode: 65 Exploration P: 0.1761 Total reward: -279.0604681968779 SOC: 0.6015 Cumulative_SOC_deviation: 23.4303 Fuel Consumption: 44.7579\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.757\n",
      "Episode: 66 Exploration P: 0.1716 Total reward: -272.1540634921042 SOC: 0.6024 Cumulative_SOC_deviation: 22.7340 Fuel Consumption: 44.8143\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.130\n",
      "Episode: 67 Exploration P: 0.1673 Total reward: -277.31943463862575 SOC: 0.6045 Cumulative_SOC_deviation: 23.2396 Fuel Consumption: 44.9232\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.493\n",
      "Episode: 68 Exploration P: 0.1630 Total reward: -256.35760553759496 SOC: 0.6009 Cumulative_SOC_deviation: 21.1583 Fuel Consumption: 44.7748\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.561\n",
      "Episode: 69 Exploration P: 0.1589 Total reward: -269.3831551560693 SOC: 0.6014 Cumulative_SOC_deviation: 22.4553 Fuel Consumption: 44.8305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.751\n",
      "Episode: 70 Exploration P: 0.1548 Total reward: -268.9687101072317 SOC: 0.6027 Cumulative_SOC_deviation: 22.4135 Fuel Consumption: 44.8342\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.775\n",
      "Episode: 71 Exploration P: 0.1509 Total reward: -290.6629113481004 SOC: 0.6027 Cumulative_SOC_deviation: 24.5831 Fuel Consumption: 44.8322\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.959\n",
      "Episode: 72 Exploration P: 0.1471 Total reward: -293.10938598315516 SOC: 0.6017 Cumulative_SOC_deviation: 24.8359 Fuel Consumption: 44.7501\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.928\n",
      "Episode: 73 Exploration P: 0.1434 Total reward: -265.65560259728386 SOC: 0.6008 Cumulative_SOC_deviation: 22.0953 Fuel Consumption: 44.7024\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.887\n",
      "Episode: 74 Exploration P: 0.1398 Total reward: -264.41317948139596 SOC: 0.6006 Cumulative_SOC_deviation: 21.9615 Fuel Consumption: 44.7980\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.150\n",
      "Episode: 75 Exploration P: 0.1362 Total reward: -293.0280159207683 SOC: 0.6000 Cumulative_SOC_deviation: 24.8279 Fuel Consumption: 44.7487\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.956\n",
      "Episode: 76 Exploration P: 0.1328 Total reward: -275.42287361307206 SOC: 0.5980 Cumulative_SOC_deviation: 23.0785 Fuel Consumption: 44.6379\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.195\n",
      "Episode: 77 Exploration P: 0.1295 Total reward: -276.6798764683632 SOC: 0.6001 Cumulative_SOC_deviation: 23.1952 Fuel Consumption: 44.7279\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.143\n",
      "Episode: 78 Exploration P: 0.1263 Total reward: -271.1306223153596 SOC: 0.5997 Cumulative_SOC_deviation: 22.6466 Fuel Consumption: 44.6644\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.772\n",
      "Episode: 79 Exploration P: 0.1231 Total reward: -295.4650475651271 SOC: 0.6002 Cumulative_SOC_deviation: 25.0885 Fuel Consumption: 44.5801\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.220\n",
      "Episode: 80 Exploration P: 0.1200 Total reward: -266.95302818675594 SOC: 0.6007 Cumulative_SOC_deviation: 22.2168 Fuel Consumption: 44.7846\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.719\n",
      "Episode: 81 Exploration P: 0.1171 Total reward: -263.82184972455974 SOC: 0.5969 Cumulative_SOC_deviation: 21.9329 Fuel Consumption: 44.4930\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.825\n",
      "Episode: 82 Exploration P: 0.1142 Total reward: -260.04819342729957 SOC: 0.5991 Cumulative_SOC_deviation: 21.5307 Fuel Consumption: 44.7409\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.065\n",
      "Episode: 83 Exploration P: 0.1113 Total reward: -265.5329819072937 SOC: 0.5984 Cumulative_SOC_deviation: 22.0933 Fuel Consumption: 44.6000\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 87.873\n",
      "Episode: 84 Exploration P: 0.1086 Total reward: -272.3967900351827 SOC: 0.5980 Cumulative_SOC_deviation: 22.7749 Fuel Consumption: 44.6473\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 86.130\n",
      "Episode: 85 Exploration P: 0.1059 Total reward: -277.8327253289464 SOC: 0.5969 Cumulative_SOC_deviation: 23.3494 Fuel Consumption: 44.3386\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.927\n",
      "Episode: 86 Exploration P: 0.1033 Total reward: -264.0333158707953 SOC: 0.6003 Cumulative_SOC_deviation: 21.9349 Fuel Consumption: 44.6843\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.132\n",
      "Episode: 87 Exploration P: 0.1008 Total reward: -279.30211356777386 SOC: 0.5979 Cumulative_SOC_deviation: 23.4801 Fuel Consumption: 44.5016\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.085\n",
      "Episode: 88 Exploration P: 0.0983 Total reward: -282.436358613323 SOC: 0.5960 Cumulative_SOC_deviation: 23.8282 Fuel Consumption: 44.1541\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.131\n",
      "Episode: 89 Exploration P: 0.0960 Total reward: -271.49111812239977 SOC: 0.5958 Cumulative_SOC_deviation: 22.7146 Fuel Consumption: 44.3455\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.963\n",
      "Episode: 90 Exploration P: 0.0936 Total reward: -280.8577785220557 SOC: 0.5979 Cumulative_SOC_deviation: 23.6426 Fuel Consumption: 44.4318\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.375\n",
      "Episode: 91 Exploration P: 0.0914 Total reward: -291.2353426753803 SOC: 0.5973 Cumulative_SOC_deviation: 24.6782 Fuel Consumption: 44.4529\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.812\n",
      "Episode: 92 Exploration P: 0.0892 Total reward: -265.76149288688737 SOC: 0.5975 Cumulative_SOC_deviation: 22.1345 Fuel Consumption: 44.4170\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.301\n",
      "Episode: 93 Exploration P: 0.0870 Total reward: -282.80917494037567 SOC: 0.5972 Cumulative_SOC_deviation: 23.8453 Fuel Consumption: 44.3560\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.939\n",
      "Episode: 94 Exploration P: 0.0849 Total reward: -279.97167674087757 SOC: 0.5973 Cumulative_SOC_deviation: 23.5637 Fuel Consumption: 44.3351\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.137\n",
      "Episode: 95 Exploration P: 0.0829 Total reward: -261.6661664300775 SOC: 0.5952 Cumulative_SOC_deviation: 21.7504 Fuel Consumption: 44.1618\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.378\n",
      "Episode: 96 Exploration P: 0.0809 Total reward: -296.3068370718298 SOC: 0.6012 Cumulative_SOC_deviation: 25.1674 Fuel Consumption: 44.6333\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.929\n",
      "Episode: 97 Exploration P: 0.0790 Total reward: -291.5201474731563 SOC: 0.5976 Cumulative_SOC_deviation: 24.7240 Fuel Consumption: 44.2802\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 72.983\n",
      "Episode: 98 Exploration P: 0.0771 Total reward: -261.47232068170155 SOC: 0.5961 Cumulative_SOC_deviation: 21.7018 Fuel Consumption: 44.4544\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.677\n",
      "Episode: 99 Exploration P: 0.0753 Total reward: -271.3915758789743 SOC: 0.5967 Cumulative_SOC_deviation: 22.6994 Fuel Consumption: 44.3975\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.860\n",
      "Episode: 100 Exploration P: 0.0735 Total reward: -259.38139930682524 SOC: 0.5968 Cumulative_SOC_deviation: 21.5020 Fuel Consumption: 44.3615\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.249\n",
      "Episode: 101 Exploration P: 0.0718 Total reward: -270.96938378748155 SOC: 0.5990 Cumulative_SOC_deviation: 22.6493 Fuel Consumption: 44.4761\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.178\n",
      "Episode: 102 Exploration P: 0.0701 Total reward: -265.59718355362827 SOC: 0.5980 Cumulative_SOC_deviation: 22.1136 Fuel Consumption: 44.4617\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.435\n",
      "Episode: 103 Exploration P: 0.0685 Total reward: -266.9653449157672 SOC: 0.5991 Cumulative_SOC_deviation: 22.2461 Fuel Consumption: 44.5040\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.655\n",
      "Episode: 104 Exploration P: 0.0669 Total reward: -262.26478157210494 SOC: 0.5986 Cumulative_SOC_deviation: 21.7793 Fuel Consumption: 44.4718\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.439\n",
      "Episode: 105 Exploration P: 0.0654 Total reward: -270.1414070933988 SOC: 0.6002 Cumulative_SOC_deviation: 22.5549 Fuel Consumption: 44.5925\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.311\n",
      "Episode: 106 Exploration P: 0.0639 Total reward: -303.423366788171 SOC: 0.5984 Cumulative_SOC_deviation: 25.9091 Fuel Consumption: 44.3320\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.768\n",
      "Episode: 107 Exploration P: 0.0624 Total reward: -253.87091649664853 SOC: 0.6002 Cumulative_SOC_deviation: 20.9304 Fuel Consumption: 44.5674\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.963\n",
      "Episode: 108 Exploration P: 0.0610 Total reward: -256.16059171027075 SOC: 0.6011 Cumulative_SOC_deviation: 21.1508 Fuel Consumption: 44.6526\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.952\n",
      "Episode: 109 Exploration P: 0.0596 Total reward: -255.84363192711513 SOC: 0.5988 Cumulative_SOC_deviation: 21.1437 Fuel Consumption: 44.4070\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.704\n",
      "Episode: 110 Exploration P: 0.0583 Total reward: -257.0199693090979 SOC: 0.6021 Cumulative_SOC_deviation: 21.2257 Fuel Consumption: 44.7629\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.098\n",
      "Episode: 111 Exploration P: 0.0570 Total reward: -272.68181475261474 SOC: 0.6006 Cumulative_SOC_deviation: 22.8075 Fuel Consumption: 44.6065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.494\n",
      "Episode: 112 Exploration P: 0.0557 Total reward: -266.836619848691 SOC: 0.5954 Cumulative_SOC_deviation: 22.2672 Fuel Consumption: 44.1647\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.340\n",
      "Episode: 113 Exploration P: 0.0545 Total reward: -296.68402006438185 SOC: 0.5949 Cumulative_SOC_deviation: 25.2681 Fuel Consumption: 44.0026\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.589\n",
      "Episode: 114 Exploration P: 0.0533 Total reward: -270.7472403645104 SOC: 0.6009 Cumulative_SOC_deviation: 22.6251 Fuel Consumption: 44.4964\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.213\n",
      "Episode: 115 Exploration P: 0.0521 Total reward: -240.07629279229454 SOC: 0.6015 Cumulative_SOC_deviation: 19.5523 Fuel Consumption: 44.5535\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.140\n",
      "Episode: 116 Exploration P: 0.0510 Total reward: -291.1375327239198 SOC: 0.5979 Cumulative_SOC_deviation: 24.7048 Fuel Consumption: 44.0897\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.156\n",
      "Episode: 117 Exploration P: 0.0498 Total reward: -284.46095983362 SOC: 0.5985 Cumulative_SOC_deviation: 24.0318 Fuel Consumption: 44.1432\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.355\n",
      "Episode: 118 Exploration P: 0.0488 Total reward: -285.2302138450726 SOC: 0.6003 Cumulative_SOC_deviation: 24.0890 Fuel Consumption: 44.3402\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.383\n",
      "Episode: 119 Exploration P: 0.0477 Total reward: -236.45187261560486 SOC: 0.6015 Cumulative_SOC_deviation: 19.1804 Fuel Consumption: 44.6477\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.138\n",
      "Episode: 120 Exploration P: 0.0467 Total reward: -237.24021051614807 SOC: 0.6036 Cumulative_SOC_deviation: 19.2394 Fuel Consumption: 44.8464\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.725\n",
      "Episode: 121 Exploration P: 0.0457 Total reward: -233.14925336040235 SOC: 0.6039 Cumulative_SOC_deviation: 18.8219 Fuel Consumption: 44.9298\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.551\n",
      "Episode: 122 Exploration P: 0.0447 Total reward: -245.574109232851 SOC: 0.6027 Cumulative_SOC_deviation: 20.0774 Fuel Consumption: 44.8002\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.389\n",
      "Episode: 123 Exploration P: 0.0438 Total reward: -247.53249782597487 SOC: 0.5941 Cumulative_SOC_deviation: 20.3434 Fuel Consumption: 44.0988\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.950\n",
      "Episode: 124 Exploration P: 0.0429 Total reward: -303.3403778940225 SOC: 0.5959 Cumulative_SOC_deviation: 25.9340 Fuel Consumption: 44.0002\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.991\n",
      "Episode: 125 Exploration P: 0.0420 Total reward: -282.0591117879417 SOC: 0.5968 Cumulative_SOC_deviation: 23.7934 Fuel Consumption: 44.1248\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.263\n",
      "Episode: 126 Exploration P: 0.0411 Total reward: -271.5448814491174 SOC: 0.6025 Cumulative_SOC_deviation: 22.6867 Fuel Consumption: 44.6777\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.260\n",
      "Episode: 127 Exploration P: 0.0403 Total reward: -243.36567602929819 SOC: 0.5983 Cumulative_SOC_deviation: 19.9130 Fuel Consumption: 44.2353\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.910\n",
      "Episode: 128 Exploration P: 0.0395 Total reward: -254.27796279955356 SOC: 0.6046 Cumulative_SOC_deviation: 20.9516 Fuel Consumption: 44.7623\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 85.024\n",
      "Episode: 129 Exploration P: 0.0387 Total reward: -248.91776152313216 SOC: 0.5993 Cumulative_SOC_deviation: 20.4617 Fuel Consumption: 44.3004\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 90.667\n",
      "Episode: 130 Exploration P: 0.0379 Total reward: -217.0232955527911 SOC: 0.6013 Cumulative_SOC_deviation: 17.2431 Fuel Consumption: 44.5926\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.078\n",
      "Episode: 131 Exploration P: 0.0371 Total reward: -279.5274637807748 SOC: 0.5938 Cumulative_SOC_deviation: 23.5659 Fuel Consumption: 43.8689\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 93.915\n",
      "Episode: 132 Exploration P: 0.0364 Total reward: -302.16034661614094 SOC: 0.5948 Cumulative_SOC_deviation: 25.8190 Fuel Consumption: 43.9706\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 88.159\n",
      "Episode: 133 Exploration P: 0.0357 Total reward: -297.1788875850107 SOC: 0.5929 Cumulative_SOC_deviation: 25.3445 Fuel Consumption: 43.7342\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.649\n",
      "Episode: 134 Exploration P: 0.0350 Total reward: -302.1221382643855 SOC: 0.5933 Cumulative_SOC_deviation: 25.8303 Fuel Consumption: 43.8193\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 152.442\n",
      "Episode: 135 Exploration P: 0.0343 Total reward: -263.94584660789747 SOC: 0.6004 Cumulative_SOC_deviation: 21.9425 Fuel Consumption: 44.5211\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 96.422\n",
      "Episode: 136 Exploration P: 0.0336 Total reward: -278.31557142321105 SOC: 0.6008 Cumulative_SOC_deviation: 23.3861 Fuel Consumption: 44.4546\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 238.235\n",
      "Episode: 137 Exploration P: 0.0330 Total reward: -241.2519010228987 SOC: 0.6033 Cumulative_SOC_deviation: 19.6484 Fuel Consumption: 44.7675\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 113.759\n",
      "Episode: 138 Exploration P: 0.0324 Total reward: -225.64296009412075 SOC: 0.5965 Cumulative_SOC_deviation: 18.1361 Fuel Consumption: 44.2820\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.404\n",
      "Episode: 139 Exploration P: 0.0318 Total reward: -237.09158003170865 SOC: 0.6032 Cumulative_SOC_deviation: 19.2186 Fuel Consumption: 44.9051\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.454\n",
      "Episode: 140 Exploration P: 0.0312 Total reward: -232.09070431596408 SOC: 0.6028 Cumulative_SOC_deviation: 18.7278 Fuel Consumption: 44.8127\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.423\n",
      "Episode: 141 Exploration P: 0.0306 Total reward: -231.87865246664475 SOC: 0.6027 Cumulative_SOC_deviation: 18.7132 Fuel Consumption: 44.7464\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.158\n",
      "Episode: 142 Exploration P: 0.0301 Total reward: -273.73241039916087 SOC: 0.6012 Cumulative_SOC_deviation: 22.9257 Fuel Consumption: 44.4751\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 213.961\n",
      "Episode: 143 Exploration P: 0.0295 Total reward: -219.16424944941835 SOC: 0.6002 Cumulative_SOC_deviation: 17.4619 Fuel Consumption: 44.5456\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 135.309\n",
      "Episode: 144 Exploration P: 0.0290 Total reward: -268.44568959813023 SOC: 0.6017 Cumulative_SOC_deviation: 22.4018 Fuel Consumption: 44.4272\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 131.873\n",
      "Episode: 145 Exploration P: 0.0285 Total reward: -228.00226934833378 SOC: 0.6014 Cumulative_SOC_deviation: 18.3459 Fuel Consumption: 44.5437\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 137.612\n",
      "Episode: 146 Exploration P: 0.0280 Total reward: -256.20218085821983 SOC: 0.5959 Cumulative_SOC_deviation: 21.2321 Fuel Consumption: 43.8816\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 133.669\n",
      "Episode: 147 Exploration P: 0.0275 Total reward: -266.2807980078455 SOC: 0.5943 Cumulative_SOC_deviation: 22.2428 Fuel Consumption: 43.8524\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 132.449\n",
      "Episode: 148 Exploration P: 0.0270 Total reward: -295.7505557857151 SOC: 0.5974 Cumulative_SOC_deviation: 25.1733 Fuel Consumption: 44.0172\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 114.980\n",
      "Episode: 149 Exploration P: 0.0265 Total reward: -280.12847325919773 SOC: 0.6026 Cumulative_SOC_deviation: 23.5634 Fuel Consumption: 44.4943\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.260\n",
      "Episode: 150 Exploration P: 0.0261 Total reward: -219.46266743862236 SOC: 0.6039 Cumulative_SOC_deviation: 17.4799 Fuel Consumption: 44.6634\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.264\n",
      "Episode: 151 Exploration P: 0.0257 Total reward: -213.6670418468973 SOC: 0.6004 Cumulative_SOC_deviation: 16.9275 Fuel Consumption: 44.3922\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.343\n",
      "Episode: 152 Exploration P: 0.0252 Total reward: -241.9525942798717 SOC: 0.6056 Cumulative_SOC_deviation: 19.7083 Fuel Consumption: 44.8696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.497\n",
      "Episode: 153 Exploration P: 0.0248 Total reward: -218.93856159686595 SOC: 0.6031 Cumulative_SOC_deviation: 17.4194 Fuel Consumption: 44.7449\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.609\n",
      "Episode: 154 Exploration P: 0.0244 Total reward: -236.03521172531944 SOC: 0.6048 Cumulative_SOC_deviation: 19.1210 Fuel Consumption: 44.8249\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.599\n",
      "Episode: 155 Exploration P: 0.0240 Total reward: -203.0522599951894 SOC: 0.6046 Cumulative_SOC_deviation: 15.8255 Fuel Consumption: 44.7973\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.233\n",
      "Episode: 156 Exploration P: 0.0237 Total reward: -199.3022994980784 SOC: 0.6041 Cumulative_SOC_deviation: 15.4479 Fuel Consumption: 44.8234\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.330\n",
      "Episode: 157 Exploration P: 0.0233 Total reward: -231.2137624267459 SOC: 0.6047 Cumulative_SOC_deviation: 18.6298 Fuel Consumption: 44.9153\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.177\n",
      "Episode: 158 Exploration P: 0.0229 Total reward: -216.90064194806146 SOC: 0.6049 Cumulative_SOC_deviation: 17.2016 Fuel Consumption: 44.8843\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.150\n",
      "Episode: 159 Exploration P: 0.0226 Total reward: -204.61651952131646 SOC: 0.6022 Cumulative_SOC_deviation: 16.0100 Fuel Consumption: 44.5162\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.138\n",
      "Episode: 160 Exploration P: 0.0222 Total reward: -197.86420677186206 SOC: 0.6034 Cumulative_SOC_deviation: 15.3161 Fuel Consumption: 44.7034\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.171\n",
      "Episode: 161 Exploration P: 0.0219 Total reward: -238.97235434199058 SOC: 0.6003 Cumulative_SOC_deviation: 19.4605 Fuel Consumption: 44.3678\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.251\n",
      "Episode: 162 Exploration P: 0.0216 Total reward: -234.02397739780773 SOC: 0.6002 Cumulative_SOC_deviation: 18.9607 Fuel Consumption: 44.4168\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.656\n",
      "Episode: 163 Exploration P: 0.0213 Total reward: -208.03863295904958 SOC: 0.6036 Cumulative_SOC_deviation: 16.3325 Fuel Consumption: 44.7134\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 86.839\n",
      "Episode: 164 Exploration P: 0.0210 Total reward: -239.75090057871645 SOC: 0.6025 Cumulative_SOC_deviation: 19.5120 Fuel Consumption: 44.6308\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 87.719\n",
      "Episode: 165 Exploration P: 0.0207 Total reward: -224.66224581141333 SOC: 0.6045 Cumulative_SOC_deviation: 17.9892 Fuel Consumption: 44.7702\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 94.391\n",
      "Episode: 166 Exploration P: 0.0204 Total reward: -220.6113346777895 SOC: 0.6035 Cumulative_SOC_deviation: 17.5919 Fuel Consumption: 44.6926\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.190\n",
      "Episode: 167 Exploration P: 0.0201 Total reward: -222.59011554432035 SOC: 0.5998 Cumulative_SOC_deviation: 17.8291 Fuel Consumption: 44.2989\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 94.531\n",
      "Episode: 168 Exploration P: 0.0198 Total reward: -215.30051187995036 SOC: 0.6017 Cumulative_SOC_deviation: 17.0794 Fuel Consumption: 44.5068\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 97.888\n",
      "Episode: 169 Exploration P: 0.0196 Total reward: -239.90881621160688 SOC: 0.6020 Cumulative_SOC_deviation: 19.5315 Fuel Consumption: 44.5942\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 93.053\n",
      "Episode: 170 Exploration P: 0.0193 Total reward: -203.56799201952305 SOC: 0.6036 Cumulative_SOC_deviation: 15.8851 Fuel Consumption: 44.7174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.271\n",
      "Episode: 171 Exploration P: 0.0190 Total reward: -200.9701044421765 SOC: 0.6041 Cumulative_SOC_deviation: 15.6194 Fuel Consumption: 44.7763\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 87.620\n",
      "Episode: 172 Exploration P: 0.0188 Total reward: -200.10297089585896 SOC: 0.6036 Cumulative_SOC_deviation: 15.5385 Fuel Consumption: 44.7183\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 90.604\n",
      "Episode: 173 Exploration P: 0.0186 Total reward: -201.4111792581534 SOC: 0.6040 Cumulative_SOC_deviation: 15.6597 Fuel Consumption: 44.8144\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.520\n",
      "Episode: 174 Exploration P: 0.0183 Total reward: -217.59784452477007 SOC: 0.6027 Cumulative_SOC_deviation: 17.2877 Fuel Consumption: 44.7208\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.029\n",
      "Episode: 175 Exploration P: 0.0181 Total reward: -212.1002948309499 SOC: 0.6030 Cumulative_SOC_deviation: 16.7426 Fuel Consumption: 44.6748\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.484\n",
      "Episode: 176 Exploration P: 0.0179 Total reward: -222.030441489839 SOC: 0.6033 Cumulative_SOC_deviation: 17.7247 Fuel Consumption: 44.7830\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.385\n",
      "Episode: 177 Exploration P: 0.0177 Total reward: -210.3800473010774 SOC: 0.6032 Cumulative_SOC_deviation: 16.5720 Fuel Consumption: 44.6601\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.139\n",
      "Episode: 178 Exploration P: 0.0175 Total reward: -215.7726003243812 SOC: 0.6020 Cumulative_SOC_deviation: 17.1225 Fuel Consumption: 44.5472\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.635\n",
      "Episode: 179 Exploration P: 0.0173 Total reward: -203.7422511927445 SOC: 0.6016 Cumulative_SOC_deviation: 15.9288 Fuel Consumption: 44.4543\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.235\n",
      "Episode: 180 Exploration P: 0.0171 Total reward: -238.51342619572358 SOC: 0.6049 Cumulative_SOC_deviation: 19.3849 Fuel Consumption: 44.6640\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.494\n",
      "Episode: 181 Exploration P: 0.0169 Total reward: -205.43588843595228 SOC: 0.6038 Cumulative_SOC_deviation: 16.0697 Fuel Consumption: 44.7388\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.318\n",
      "Episode: 182 Exploration P: 0.0167 Total reward: -210.01217924366847 SOC: 0.6045 Cumulative_SOC_deviation: 16.5247 Fuel Consumption: 44.7655\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.977\n",
      "Episode: 183 Exploration P: 0.0165 Total reward: -200.86803818342256 SOC: 0.6032 Cumulative_SOC_deviation: 15.6192 Fuel Consumption: 44.6761\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.325\n",
      "Episode: 184 Exploration P: 0.0163 Total reward: -202.72740237251008 SOC: 0.6029 Cumulative_SOC_deviation: 15.8046 Fuel Consumption: 44.6810\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.114\n",
      "Episode: 185 Exploration P: 0.0162 Total reward: -216.40072521192405 SOC: 0.6040 Cumulative_SOC_deviation: 17.1764 Fuel Consumption: 44.6368\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.303\n",
      "Episode: 186 Exploration P: 0.0160 Total reward: -199.81362295751805 SOC: 0.6035 Cumulative_SOC_deviation: 15.5121 Fuel Consumption: 44.6931\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.174\n",
      "Episode: 187 Exploration P: 0.0158 Total reward: -203.28955336209114 SOC: 0.6022 Cumulative_SOC_deviation: 15.8689 Fuel Consumption: 44.6007\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.218\n",
      "Episode: 188 Exploration P: 0.0157 Total reward: -205.62506557579295 SOC: 0.6041 Cumulative_SOC_deviation: 16.0893 Fuel Consumption: 44.7316\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.546\n",
      "Episode: 189 Exploration P: 0.0155 Total reward: -195.56591116132196 SOC: 0.6025 Cumulative_SOC_deviation: 15.0913 Fuel Consumption: 44.6531\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.325\n",
      "Episode: 190 Exploration P: 0.0154 Total reward: -199.92169104265454 SOC: 0.6034 Cumulative_SOC_deviation: 15.5248 Fuel Consumption: 44.6735\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.931\n",
      "Episode: 191 Exploration P: 0.0152 Total reward: -198.3232137104371 SOC: 0.6037 Cumulative_SOC_deviation: 15.3527 Fuel Consumption: 44.7964\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.390\n",
      "Episode: 192 Exploration P: 0.0151 Total reward: -200.59921817699723 SOC: 0.6035 Cumulative_SOC_deviation: 15.5810 Fuel Consumption: 44.7892\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.401\n",
      "Episode: 193 Exploration P: 0.0149 Total reward: -196.44128277414154 SOC: 0.6019 Cumulative_SOC_deviation: 15.1776 Fuel Consumption: 44.6651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.457\n",
      "Episode: 194 Exploration P: 0.0148 Total reward: -199.08282703529127 SOC: 0.6040 Cumulative_SOC_deviation: 15.4218 Fuel Consumption: 44.8653\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.291\n",
      "Episode: 195 Exploration P: 0.0147 Total reward: -202.03201524614371 SOC: 0.6030 Cumulative_SOC_deviation: 15.7295 Fuel Consumption: 44.7373\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.441\n",
      "Episode: 196 Exploration P: 0.0146 Total reward: -196.36690795049137 SOC: 0.6020 Cumulative_SOC_deviation: 15.1761 Fuel Consumption: 44.6055\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.190\n",
      "Episode: 197 Exploration P: 0.0144 Total reward: -203.86692197414928 SOC: 0.6046 Cumulative_SOC_deviation: 15.9081 Fuel Consumption: 44.7854\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.133\n",
      "Episode: 198 Exploration P: 0.0143 Total reward: -194.34098287018506 SOC: 0.6029 Cumulative_SOC_deviation: 14.9707 Fuel Consumption: 44.6341\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.587\n",
      "Episode: 199 Exploration P: 0.0142 Total reward: -201.29765074984064 SOC: 0.6039 Cumulative_SOC_deviation: 15.6501 Fuel Consumption: 44.7966\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.329\n",
      "Episode: 200 Exploration P: 0.0141 Total reward: -200.9515437925841 SOC: 0.6043 Cumulative_SOC_deviation: 15.6199 Fuel Consumption: 44.7522\n",
      "model is saved..\n",
      "\n",
      "Trial 1\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 17.228\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -891.4641549231349 SOC: 0.7810 Cumulative_SOC_deviation: 83.2058 Fuel Consumption: 59.4066\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 16.694\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -969.8604796083296 SOC: 0.8028 Cumulative_SOC_deviation: 90.8514 Fuel Consumption: 61.3462\n",
      "WARNING:tensorflow:Layer batch_normalization_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 62.814\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -866.4219296467096 SOC: 0.7676 Cumulative_SOC_deviation: 80.8038 Fuel Consumption: 58.3840\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.443\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -762.1556263854611 SOC: 0.7336 Cumulative_SOC_deviation: 70.6527 Fuel Consumption: 55.6288\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.117\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -789.6487339681771 SOC: 0.7352 Cumulative_SOC_deviation: 73.3696 Fuel Consumption: 55.9530\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.864\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -833.3015743148794 SOC: 0.6656 Cumulative_SOC_deviation: 78.2708 Fuel Consumption: 50.5940\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.033\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -811.8038739264371 SOC: 0.6659 Cumulative_SOC_deviation: 76.1387 Fuel Consumption: 50.4164\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.930\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -793.7659188110354 SOC: 0.6601 Cumulative_SOC_deviation: 74.3602 Fuel Consumption: 50.1639\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.328\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -964.2620604155334 SOC: 0.6322 Cumulative_SOC_deviation: 91.6280 Fuel Consumption: 47.9825\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.954\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -915.5206829547262 SOC: 0.6322 Cumulative_SOC_deviation: 86.7639 Fuel Consumption: 47.8819\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.232\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -1083.4117828497274 SOC: 0.6058 Cumulative_SOC_deviation: 103.7388 Fuel Consumption: 46.0235\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.114\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -1323.3171647913769 SOC: 0.5594 Cumulative_SOC_deviation: 128.1130 Fuel Consumption: 42.1870\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.962\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -1133.4036674712836 SOC: 0.6064 Cumulative_SOC_deviation: 108.7419 Fuel Consumption: 45.9850\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.142\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -1329.7033642793172 SOC: 0.5600 Cumulative_SOC_deviation: 128.7186 Fuel Consumption: 42.5176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.007\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -1271.3721045016812 SOC: 0.5737 Cumulative_SOC_deviation: 122.7856 Fuel Consumption: 43.5157\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.398\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -1570.4919431701667 SOC: 0.5322 Cumulative_SOC_deviation: 152.9989 Fuel Consumption: 40.5030\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 87.919\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -1753.0785922966502 SOC: 0.5221 Cumulative_SOC_deviation: 171.3578 Fuel Consumption: 39.5005\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.712\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -1738.608332159657 SOC: 0.5109 Cumulative_SOC_deviation: 169.9812 Fuel Consumption: 38.7965\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 88.841\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -1712.8681092561524 SOC: 0.4952 Cumulative_SOC_deviation: 167.5216 Fuel Consumption: 37.6518\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.849\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -2060.698376305387 SOC: 0.4752 Cumulative_SOC_deviation: 202.4520 Fuel Consumption: 36.1786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.349\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -2061.4163167342717 SOC: 0.4590 Cumulative_SOC_deviation: 202.6544 Fuel Consumption: 34.8722\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.416\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -2094.8116327678504 SOC: 0.4526 Cumulative_SOC_deviation: 206.0350 Fuel Consumption: 34.4614\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.315\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -2140.1109590409383 SOC: 0.4464 Cumulative_SOC_deviation: 210.6133 Fuel Consumption: 33.9778\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.032\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -2192.2509371580327 SOC: 0.4342 Cumulative_SOC_deviation: 215.9070 Fuel Consumption: 33.1808\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.636\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -1417.116707995286 SOC: 0.6714 Cumulative_SOC_deviation: 136.6182 Fuel Consumption: 50.9344\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.393\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -404.8438913204394 SOC: 0.6271 Cumulative_SOC_deviation: 35.7836 Fuel Consumption: 47.0079\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 93.184\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -400.9411848723921 SOC: 0.6509 Cumulative_SOC_deviation: 35.1415 Fuel Consumption: 49.5260\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 87.979\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -345.62854130131 SOC: 0.6346 Cumulative_SOC_deviation: 29.7462 Fuel Consumption: 48.1668\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.123\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -303.220196619493 SOC: 0.6202 Cumulative_SOC_deviation: 25.6366 Fuel Consumption: 46.8542\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.765\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -313.2651601866731 SOC: 0.6245 Cumulative_SOC_deviation: 26.5787 Fuel Consumption: 47.4781\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.569\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -295.7147386557962 SOC: 0.6293 Cumulative_SOC_deviation: 24.8035 Fuel Consumption: 47.6800\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.075\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -317.8448994225821 SOC: 0.6274 Cumulative_SOC_deviation: 27.0426 Fuel Consumption: 47.4189\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.863\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -366.4676507285788 SOC: 0.6227 Cumulative_SOC_deviation: 31.9259 Fuel Consumption: 47.2084\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.875\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -284.2652012195329 SOC: 0.6230 Cumulative_SOC_deviation: 23.7111 Fuel Consumption: 47.1541\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.817\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -318.25009566917083 SOC: 0.6241 Cumulative_SOC_deviation: 27.1134 Fuel Consumption: 47.1162\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.152\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -299.0865924015817 SOC: 0.6158 Cumulative_SOC_deviation: 25.2532 Fuel Consumption: 46.5549\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.272\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -323.9821945811437 SOC: 0.6099 Cumulative_SOC_deviation: 27.8083 Fuel Consumption: 45.8995\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.513\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -335.66135465331763 SOC: 0.6095 Cumulative_SOC_deviation: 28.9877 Fuel Consumption: 45.7844\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.569\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -321.80117100917 SOC: 0.6118 Cumulative_SOC_deviation: 27.5755 Fuel Consumption: 46.0461\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.770\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -309.58005598147196 SOC: 0.6133 Cumulative_SOC_deviation: 26.3343 Fuel Consumption: 46.2372\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.826\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -266.9559545137596 SOC: 0.6076 Cumulative_SOC_deviation: 22.1314 Fuel Consumption: 45.6420\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.380\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -307.4469286736338 SOC: 0.6119 Cumulative_SOC_deviation: 26.1553 Fuel Consumption: 45.8938\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.789\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -305.3790904193969 SOC: 0.6139 Cumulative_SOC_deviation: 25.9187 Fuel Consumption: 46.1916\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.296\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -261.39764744734595 SOC: 0.6091 Cumulative_SOC_deviation: 21.5679 Fuel Consumption: 45.7188\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 3\n",
    "results_dict = {} \n",
    "for trial in range(num_trials): \n",
    "    print()\n",
    "    print(\"Trial {}\".format(trial))\n",
    "    \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer = initialization()\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    for ep in range(total_episodes): \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "        )\n",
    "    \n",
    "    root = \"DDPG1_trial{}\".format(trial+1)\n",
    "    save_weights(actor_model, critic_model, target_actor, target_critic, root)\n",
    "    \n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG1.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

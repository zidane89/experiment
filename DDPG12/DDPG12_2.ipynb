{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG12_2 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):      \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    inputs_batchnorm = layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs_batchnorm)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_input_batchnorm = layers.BatchNormalization()(state_input)\n",
    "    \n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input_batchnorm)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 28.732\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -943.8410256002068 SOC: 0.7979 Cumulative_SOC_deviation: 88.3043 Fuel Consumption: 60.7985\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 29.871\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -1008.8101509087568 SOC: 0.8165 Cumulative_SOC_deviation: 94.6425 Fuel Consumption: 62.3851\n",
      "WARNING:tensorflow:Layer batch_normalization_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 179.675\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -838.5890290516504 SOC: 0.7753 Cumulative_SOC_deviation: 77.9646 Fuel Consumption: 58.9433\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 209.891\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -774.5027622448171 SOC: 0.7368 Cumulative_SOC_deviation: 71.8320 Fuel Consumption: 56.1831\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 209.752\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -783.9408381280521 SOC: 0.7233 Cumulative_SOC_deviation: 72.8918 Fuel Consumption: 55.0226\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 208.859\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -756.3437117944898 SOC: 0.6967 Cumulative_SOC_deviation: 70.3401 Fuel Consumption: 52.9431\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 209.031\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -761.2812736172056 SOC: 0.7041 Cumulative_SOC_deviation: 70.7995 Fuel Consumption: 53.2861\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 208.788\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -722.6938367535772 SOC: 0.6659 Cumulative_SOC_deviation: 67.2157 Fuel Consumption: 50.5371\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 208.781\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -834.9764183292272 SOC: 0.6436 Cumulative_SOC_deviation: 78.6170 Fuel Consumption: 48.8060\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 209.600\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -1006.7415833587194 SOC: 0.6005 Cumulative_SOC_deviation: 96.1311 Fuel Consumption: 45.4302\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 209.710\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -1154.3176330691351 SOC: 0.6157 Cumulative_SOC_deviation: 110.7666 Fuel Consumption: 46.6521\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 211.742\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -649.9252481720425 SOC: 0.6934 Cumulative_SOC_deviation: 59.7680 Fuel Consumption: 52.2453\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 209.289\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -1226.2618119545375 SOC: 0.5821 Cumulative_SOC_deviation: 118.2197 Fuel Consumption: 44.0653\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 209.587\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -1261.3712181556998 SOC: 0.5641 Cumulative_SOC_deviation: 121.8553 Fuel Consumption: 42.8182\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 210.714\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -1467.0533569023037 SOC: 0.5533 Cumulative_SOC_deviation: 142.4829 Fuel Consumption: 42.2244\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 209.390\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -1379.9450881398718 SOC: 0.5451 Cumulative_SOC_deviation: 133.8670 Fuel Consumption: 41.2754\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 209.425\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -1565.7345950388585 SOC: 0.5227 Cumulative_SOC_deviation: 152.5996 Fuel Consumption: 39.7391\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 209.814\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -1663.3594997712787 SOC: 0.5169 Cumulative_SOC_deviation: 162.4007 Fuel Consumption: 39.3529\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 213.735\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -1765.2880427346613 SOC: 0.5014 Cumulative_SOC_deviation: 172.7013 Fuel Consumption: 38.2755\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 214.358\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -1691.3467572460834 SOC: 0.5127 Cumulative_SOC_deviation: 165.2281 Fuel Consumption: 39.0661\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 213.464\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -2001.8378839882741 SOC: 0.4680 Cumulative_SOC_deviation: 196.6187 Fuel Consumption: 35.6511\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 213.218\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -1877.3176046312278 SOC: 0.4706 Cumulative_SOC_deviation: 184.1592 Fuel Consumption: 35.7260\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 215.199\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -1500.4372296167921 SOC: 0.6232 Cumulative_SOC_deviation: 145.2706 Fuel Consumption: 47.7312\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 214.674\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -446.2759758499513 SOC: 0.6400 Cumulative_SOC_deviation: 39.8059 Fuel Consumption: 48.2171\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 215.648\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -443.5804709675649 SOC: 0.6532 Cumulative_SOC_deviation: 39.4154 Fuel Consumption: 49.4262\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 213.732\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -350.2398964157785 SOC: 0.6314 Cumulative_SOC_deviation: 30.2862 Fuel Consumption: 47.3775\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 214.257\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -400.1232552481597 SOC: 0.6379 Cumulative_SOC_deviation: 35.2123 Fuel Consumption: 48.0005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 215.789\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -379.020813052396 SOC: 0.6343 Cumulative_SOC_deviation: 33.1313 Fuel Consumption: 47.7079\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 216.578\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -376.8070276904804 SOC: 0.6241 Cumulative_SOC_deviation: 32.9924 Fuel Consumption: 46.8826\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 213.448\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -386.3163411434918 SOC: 0.6361 Cumulative_SOC_deviation: 33.8621 Fuel Consumption: 47.6951\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 216.094\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -341.86125834531725 SOC: 0.6214 Cumulative_SOC_deviation: 29.5203 Fuel Consumption: 46.6585\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 214.946\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -359.49063836298205 SOC: 0.6184 Cumulative_SOC_deviation: 31.2840 Fuel Consumption: 46.6509\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 213.908\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -382.3846667662402 SOC: 0.6247 Cumulative_SOC_deviation: 33.5424 Fuel Consumption: 46.9602\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 213.668\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -333.6838216661745 SOC: 0.6155 Cumulative_SOC_deviation: 28.7397 Fuel Consumption: 46.2868\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 215.148\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -349.95125184829305 SOC: 0.6111 Cumulative_SOC_deviation: 30.4210 Fuel Consumption: 45.7410\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 215.735\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -322.29875975596536 SOC: 0.6071 Cumulative_SOC_deviation: 27.6785 Fuel Consumption: 45.5133\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 215.040\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -315.3363274612805 SOC: 0.6143 Cumulative_SOC_deviation: 26.9260 Fuel Consumption: 46.0766\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 214.955\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -341.1904512235206 SOC: 0.6150 Cumulative_SOC_deviation: 29.5024 Fuel Consumption: 46.1668\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 216.816\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -335.5927731865988 SOC: 0.6162 Cumulative_SOC_deviation: 28.9452 Fuel Consumption: 46.1406\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 215.804\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -298.4144124112884 SOC: 0.6074 Cumulative_SOC_deviation: 25.2860 Fuel Consumption: 45.5544\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 215.701\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -280.40310047481705 SOC: 0.6076 Cumulative_SOC_deviation: 23.4880 Fuel Consumption: 45.5234\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 215.744\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -285.35322569472874 SOC: 0.6108 Cumulative_SOC_deviation: 23.9308 Fuel Consumption: 46.0450\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 215.109\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -261.6475378912083 SOC: 0.6078 Cumulative_SOC_deviation: 21.5947 Fuel Consumption: 45.7008\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 215.115\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -286.2899613615133 SOC: 0.6075 Cumulative_SOC_deviation: 24.0678 Fuel Consumption: 45.6122\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 216.073\n",
      "Episode: 45 Exploration P: 0.2977 Total reward: -258.37021425313196 SOC: 0.6044 Cumulative_SOC_deviation: 21.3070 Fuel Consumption: 45.3000\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 218.165\n",
      "Episode: 46 Exploration P: 0.2899 Total reward: -281.86333895731394 SOC: 0.6083 Cumulative_SOC_deviation: 23.6091 Fuel Consumption: 45.7726\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 215.918\n",
      "Episode: 47 Exploration P: 0.2824 Total reward: -293.98507523297735 SOC: 0.6058 Cumulative_SOC_deviation: 24.8530 Fuel Consumption: 45.4554\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 216.026\n",
      "Episode: 48 Exploration P: 0.2750 Total reward: -273.4467319635378 SOC: 0.5988 Cumulative_SOC_deviation: 22.8561 Fuel Consumption: 44.8856\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 216.186\n",
      "Episode: 49 Exploration P: 0.2678 Total reward: -285.59696014938976 SOC: 0.6053 Cumulative_SOC_deviation: 24.0131 Fuel Consumption: 45.4659\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 215.755\n",
      "Episode: 50 Exploration P: 0.2608 Total reward: -265.2255841449364 SOC: 0.6023 Cumulative_SOC_deviation: 22.0102 Fuel Consumption: 45.1237\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 215.201\n",
      "Episode: 51 Exploration P: 0.2540 Total reward: -270.3374935715116 SOC: 0.6019 Cumulative_SOC_deviation: 22.5161 Fuel Consumption: 45.1762\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 216.625\n",
      "Episode: 52 Exploration P: 0.2474 Total reward: -283.11298193128823 SOC: 0.6030 Cumulative_SOC_deviation: 23.7895 Fuel Consumption: 45.2181\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 217.269\n",
      "Episode: 53 Exploration P: 0.2410 Total reward: -284.40283668015775 SOC: 0.5976 Cumulative_SOC_deviation: 23.9513 Fuel Consumption: 44.8900\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 216.003\n",
      "Episode: 54 Exploration P: 0.2347 Total reward: -292.2878916351257 SOC: 0.6019 Cumulative_SOC_deviation: 24.7111 Fuel Consumption: 45.1766\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 206.994\n",
      "Episode: 55 Exploration P: 0.2286 Total reward: -273.9839219678323 SOC: 0.6011 Cumulative_SOC_deviation: 22.8913 Fuel Consumption: 45.0706\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 214.789\n",
      "Episode: 56 Exploration P: 0.2227 Total reward: -283.8841886301118 SOC: 0.5963 Cumulative_SOC_deviation: 23.9208 Fuel Consumption: 44.6761\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 218.695\n",
      "Episode: 57 Exploration P: 0.2170 Total reward: -290.78043164478413 SOC: 0.6028 Cumulative_SOC_deviation: 24.5700 Fuel Consumption: 45.0804\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 203.684\n",
      "Episode: 58 Exploration P: 0.2114 Total reward: -278.79071943644936 SOC: 0.5995 Cumulative_SOC_deviation: 23.3920 Fuel Consumption: 44.8706\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 217.541\n",
      "Episode: 59 Exploration P: 0.2059 Total reward: -291.4904463708308 SOC: 0.6021 Cumulative_SOC_deviation: 24.6426 Fuel Consumption: 45.0648\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 219.055\n",
      "Episode: 60 Exploration P: 0.2006 Total reward: -274.4990907082628 SOC: 0.5979 Cumulative_SOC_deviation: 22.9854 Fuel Consumption: 44.6453\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 205.333\n",
      "Episode: 61 Exploration P: 0.1954 Total reward: -266.0756473241433 SOC: 0.5994 Cumulative_SOC_deviation: 22.1206 Fuel Consumption: 44.8697\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 218.499\n",
      "Episode: 62 Exploration P: 0.1904 Total reward: -294.4747624913983 SOC: 0.5940 Cumulative_SOC_deviation: 24.9982 Fuel Consumption: 44.4932\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 219.266\n",
      "Episode: 63 Exploration P: 0.1855 Total reward: -333.59323495190034 SOC: 0.6074 Cumulative_SOC_deviation: 28.8134 Fuel Consumption: 45.4589\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 217.255\n",
      "Episode: 64 Exploration P: 0.1808 Total reward: -223.95107736250375 SOC: 0.5970 Cumulative_SOC_deviation: 17.9264 Fuel Consumption: 44.6868\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 218.831\n",
      "Episode: 65 Exploration P: 0.1761 Total reward: -244.81260459224126 SOC: 0.6065 Cumulative_SOC_deviation: 19.9304 Fuel Consumption: 45.5088\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 219.896\n",
      "Episode: 66 Exploration P: 0.1716 Total reward: -231.02747180833015 SOC: 0.6033 Cumulative_SOC_deviation: 18.5805 Fuel Consumption: 45.2221\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 218.152\n",
      "Episode: 67 Exploration P: 0.1673 Total reward: -223.21810332209444 SOC: 0.6028 Cumulative_SOC_deviation: 17.8065 Fuel Consumption: 45.1534\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 218.010\n",
      "Episode: 68 Exploration P: 0.1630 Total reward: -231.7894022539512 SOC: 0.6048 Cumulative_SOC_deviation: 18.6323 Fuel Consumption: 45.4668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 214.565\n",
      "Episode: 69 Exploration P: 0.1589 Total reward: -260.17069566120733 SOC: 0.6069 Cumulative_SOC_deviation: 21.4850 Fuel Consumption: 45.3212\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 207.463\n",
      "Episode: 70 Exploration P: 0.1548 Total reward: -229.3853677554245 SOC: 0.6037 Cumulative_SOC_deviation: 18.4101 Fuel Consumption: 45.2848\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 204.853\n",
      "Episode: 71 Exploration P: 0.1509 Total reward: -222.9677155807542 SOC: 0.6095 Cumulative_SOC_deviation: 17.7410 Fuel Consumption: 45.5579\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 205.392\n",
      "Episode: 72 Exploration P: 0.1471 Total reward: -230.52764715323892 SOC: 0.6020 Cumulative_SOC_deviation: 18.5472 Fuel Consumption: 45.0558\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 206.184\n",
      "Episode: 73 Exploration P: 0.1434 Total reward: -234.4263196405945 SOC: 0.6057 Cumulative_SOC_deviation: 18.8915 Fuel Consumption: 45.5115\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 206.700\n",
      "Episode: 74 Exploration P: 0.1398 Total reward: -234.7422926565809 SOC: 0.6022 Cumulative_SOC_deviation: 18.9668 Fuel Consumption: 45.0743\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 122.817\n",
      "Episode: 75 Exploration P: 0.1362 Total reward: -220.45361821507694 SOC: 0.6018 Cumulative_SOC_deviation: 17.5543 Fuel Consumption: 44.9111\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 126.434\n",
      "Episode: 76 Exploration P: 0.1328 Total reward: -235.6892634442753 SOC: 0.6019 Cumulative_SOC_deviation: 19.0864 Fuel Consumption: 44.8255\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.141\n",
      "Episode: 77 Exploration P: 0.1295 Total reward: -237.3229861055544 SOC: 0.6011 Cumulative_SOC_deviation: 19.2531 Fuel Consumption: 44.7921\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.939\n",
      "Episode: 78 Exploration P: 0.1263 Total reward: -228.77324671282003 SOC: 0.6016 Cumulative_SOC_deviation: 18.3916 Fuel Consumption: 44.8575\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.849\n",
      "Episode: 79 Exploration P: 0.1231 Total reward: -233.9130000191974 SOC: 0.6048 Cumulative_SOC_deviation: 18.8768 Fuel Consumption: 45.1446\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.042\n",
      "Episode: 80 Exploration P: 0.1200 Total reward: -238.171744680741 SOC: 0.5990 Cumulative_SOC_deviation: 19.3623 Fuel Consumption: 44.5490\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.392\n",
      "Episode: 81 Exploration P: 0.1171 Total reward: -244.76679248854916 SOC: 0.6027 Cumulative_SOC_deviation: 20.0125 Fuel Consumption: 44.6415\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.049\n",
      "Episode: 82 Exploration P: 0.1142 Total reward: -224.88217584721102 SOC: 0.6026 Cumulative_SOC_deviation: 18.0284 Fuel Consumption: 44.5980\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.352\n",
      "Episode: 83 Exploration P: 0.1113 Total reward: -215.57109577008586 SOC: 0.6027 Cumulative_SOC_deviation: 17.0911 Fuel Consumption: 44.6598\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.342\n",
      "Episode: 84 Exploration P: 0.1086 Total reward: -226.46605971202425 SOC: 0.6019 Cumulative_SOC_deviation: 18.1917 Fuel Consumption: 44.5487\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.036\n",
      "Episode: 85 Exploration P: 0.1059 Total reward: -212.916485365881 SOC: 0.6028 Cumulative_SOC_deviation: 16.8097 Fuel Consumption: 44.8194\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.954\n",
      "Episode: 86 Exploration P: 0.1033 Total reward: -208.51994530353508 SOC: 0.6040 Cumulative_SOC_deviation: 16.3680 Fuel Consumption: 44.8397\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.594\n",
      "Episode: 87 Exploration P: 0.1008 Total reward: -215.90047532813864 SOC: 0.6036 Cumulative_SOC_deviation: 17.1216 Fuel Consumption: 44.6850\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.429\n",
      "Episode: 88 Exploration P: 0.0983 Total reward: -212.94572991345893 SOC: 0.6034 Cumulative_SOC_deviation: 16.8263 Fuel Consumption: 44.6823\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.584\n",
      "Episode: 89 Exploration P: 0.0960 Total reward: -221.00747779314446 SOC: 0.6013 Cumulative_SOC_deviation: 17.6644 Fuel Consumption: 44.3632\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.205\n",
      "Episode: 90 Exploration P: 0.0936 Total reward: -209.43527679990672 SOC: 0.6027 Cumulative_SOC_deviation: 16.4962 Fuel Consumption: 44.4731\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.094\n",
      "Episode: 91 Exploration P: 0.0914 Total reward: -231.034125204851 SOC: 0.6002 Cumulative_SOC_deviation: 18.6597 Fuel Consumption: 44.4376\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.263\n",
      "Episode: 92 Exploration P: 0.0892 Total reward: -219.72880708584702 SOC: 0.6027 Cumulative_SOC_deviation: 17.5150 Fuel Consumption: 44.5793\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.932\n",
      "Episode: 93 Exploration P: 0.0870 Total reward: -213.99042384866436 SOC: 0.6013 Cumulative_SOC_deviation: 16.9509 Fuel Consumption: 44.4812\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.419\n",
      "Episode: 94 Exploration P: 0.0849 Total reward: -250.07640487538922 SOC: 0.6027 Cumulative_SOC_deviation: 20.5444 Fuel Consumption: 44.6323\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.162\n",
      "Episode: 95 Exploration P: 0.0829 Total reward: -264.64798801904357 SOC: 0.6026 Cumulative_SOC_deviation: 22.0062 Fuel Consumption: 44.5864\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.594\n",
      "Episode: 96 Exploration P: 0.0809 Total reward: -212.65567244537667 SOC: 0.6023 Cumulative_SOC_deviation: 16.8088 Fuel Consumption: 44.5675\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 120.479\n",
      "Episode: 97 Exploration P: 0.0790 Total reward: -210.81190025353098 SOC: 0.6011 Cumulative_SOC_deviation: 16.6311 Fuel Consumption: 44.5009\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.575\n",
      "Episode: 98 Exploration P: 0.0771 Total reward: -204.5274552575872 SOC: 0.6010 Cumulative_SOC_deviation: 16.0028 Fuel Consumption: 44.4991\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.168\n",
      "Episode: 99 Exploration P: 0.0753 Total reward: -210.04459055814934 SOC: 0.6014 Cumulative_SOC_deviation: 16.5492 Fuel Consumption: 44.5529\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.119\n",
      "Episode: 100 Exploration P: 0.0735 Total reward: -215.96314750396817 SOC: 0.5997 Cumulative_SOC_deviation: 17.1513 Fuel Consumption: 44.4500\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 22.016\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -943.1745605298413 SOC: 0.7952 Cumulative_SOC_deviation: 88.2593 Fuel Consumption: 60.5813\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 22.407\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -914.5872417460189 SOC: 0.7943 Cumulative_SOC_deviation: 85.4110 Fuel Consumption: 60.4775\n",
      "WARNING:tensorflow:Layer batch_normalization_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer batch_normalization_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 99.143\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -804.754974455981 SOC: 0.7511 Cumulative_SOC_deviation: 74.7633 Fuel Consumption: 57.1218\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 116.188\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -807.4095891315015 SOC: 0.7250 Cumulative_SOC_deviation: 75.2326 Fuel Consumption: 55.0832\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 115.680\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -766.9636911142325 SOC: 0.7186 Cumulative_SOC_deviation: 71.2326 Fuel Consumption: 54.6381\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 116.848\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -797.4048970647623 SOC: 0.6878 Cumulative_SOC_deviation: 74.5298 Fuel Consumption: 52.1068\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 116.177\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -766.9064497403585 SOC: 0.6719 Cumulative_SOC_deviation: 71.5921 Fuel Consumption: 50.9856\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 116.347\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -906.7665781188415 SOC: 0.6616 Cumulative_SOC_deviation: 85.6543 Fuel Consumption: 50.2238\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 116.071\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -841.2647256126265 SOC: 0.6566 Cumulative_SOC_deviation: 79.1352 Fuel Consumption: 49.9125\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 117.305\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -787.1766353560079 SOC: 0.6481 Cumulative_SOC_deviation: 73.7872 Fuel Consumption: 49.3048\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 117.799\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -1085.1903267742505 SOC: 0.6032 Cumulative_SOC_deviation: 103.9464 Fuel Consumption: 45.7261\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 117.122\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -1004.5918065445259 SOC: 0.6220 Cumulative_SOC_deviation: 95.7207 Fuel Consumption: 47.3844\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 115.285\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -1078.437770066974 SOC: 0.5937 Cumulative_SOC_deviation: 103.3507 Fuel Consumption: 44.9312\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 114.323\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -1173.388591303199 SOC: 0.5821 Cumulative_SOC_deviation: 112.9257 Fuel Consumption: 44.1320\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 116.725\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -1398.395052760622 SOC: 0.5546 Cumulative_SOC_deviation: 135.6433 Fuel Consumption: 41.9623\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 117.939\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -1601.7520533891752 SOC: 0.5280 Cumulative_SOC_deviation: 156.1582 Fuel Consumption: 40.1703\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 113.252\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -1705.628188024173 SOC: 0.5118 Cumulative_SOC_deviation: 166.6871 Fuel Consumption: 38.7570\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 116.970\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -1973.653523567464 SOC: 0.4809 Cumulative_SOC_deviation: 193.7104 Fuel Consumption: 36.5498\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 117.103\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -1705.6095991354518 SOC: 0.5084 Cumulative_SOC_deviation: 166.6833 Fuel Consumption: 38.7762\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 116.888\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -1828.6452170503198 SOC: 0.4850 Cumulative_SOC_deviation: 179.1777 Fuel Consumption: 36.8686\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 117.615\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -2026.554757657316 SOC: 0.4705 Cumulative_SOC_deviation: 199.0787 Fuel Consumption: 35.7677\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 113.925\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -2260.0884324717576 SOC: 0.4346 Cumulative_SOC_deviation: 222.6917 Fuel Consumption: 33.1711\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.053\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -2136.784873889618 SOC: 0.4365 Cumulative_SOC_deviation: 210.3439 Fuel Consumption: 33.3455\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 117.777\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -2413.9692222591625 SOC: 0.4201 Cumulative_SOC_deviation: 238.1803 Fuel Consumption: 32.1665\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 117.724\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -2209.7576150576588 SOC: 0.4388 Cumulative_SOC_deviation: 217.6353 Fuel Consumption: 33.4044\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 117.359\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -2230.5577807376776 SOC: 0.4296 Cumulative_SOC_deviation: 219.7730 Fuel Consumption: 32.8277\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 117.855\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -2482.5555036618025 SOC: 0.4000 Cumulative_SOC_deviation: 245.1895 Fuel Consumption: 30.6602\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 117.838\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -2611.5052098999668 SOC: 0.3910 Cumulative_SOC_deviation: 258.1438 Fuel Consumption: 30.0674\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.156\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -2576.3606579894076 SOC: 0.3897 Cumulative_SOC_deviation: 254.6279 Fuel Consumption: 30.0814\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 117.812\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -2514.923026992016 SOC: 0.3872 Cumulative_SOC_deviation: 248.5180 Fuel Consumption: 29.7433\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.172\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -2780.9186129219743 SOC: 0.3475 Cumulative_SOC_deviation: 275.3875 Fuel Consumption: 27.0440\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.112\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -2828.8857625623878 SOC: 0.3460 Cumulative_SOC_deviation: 280.2002 Fuel Consumption: 26.8835\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.835\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -2485.066237603596 SOC: 0.5637 Cumulative_SOC_deviation: 244.1096 Fuel Consumption: 43.9706\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.628\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -238.28043116825424 SOC: 0.6090 Cumulative_SOC_deviation: 19.2805 Fuel Consumption: 45.4756\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.826\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -331.1817647487163 SOC: 0.6204 Cumulative_SOC_deviation: 28.4071 Fuel Consumption: 47.1110\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.371\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -289.3334420368647 SOC: 0.6144 Cumulative_SOC_deviation: 24.2951 Fuel Consumption: 46.3829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.239\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -282.4113726620317 SOC: 0.6120 Cumulative_SOC_deviation: 23.6158 Fuel Consumption: 46.2532\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.352\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -283.532270480619 SOC: 0.6075 Cumulative_SOC_deviation: 23.7495 Fuel Consumption: 46.0368\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.304\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -279.62093144728277 SOC: 0.6157 Cumulative_SOC_deviation: 23.3037 Fuel Consumption: 46.5837\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.435\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -262.8484756687511 SOC: 0.6091 Cumulative_SOC_deviation: 21.6883 Fuel Consumption: 45.9653\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.444\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -266.973116971779 SOC: 0.6057 Cumulative_SOC_deviation: 22.1190 Fuel Consumption: 45.7836\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.576\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -271.468224467972 SOC: 0.6069 Cumulative_SOC_deviation: 22.5634 Fuel Consumption: 45.8344\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.404\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -268.3059503983985 SOC: 0.6067 Cumulative_SOC_deviation: 22.2599 Fuel Consumption: 45.7073\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.879\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -261.18130651085494 SOC: 0.6111 Cumulative_SOC_deviation: 21.5066 Fuel Consumption: 46.1154\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.542\n",
      "Episode: 45 Exploration P: 0.2977 Total reward: -243.03450138024098 SOC: 0.6032 Cumulative_SOC_deviation: 19.7792 Fuel Consumption: 45.2422\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.278\n",
      "Episode: 46 Exploration P: 0.2899 Total reward: -248.55862362875732 SOC: 0.6071 Cumulative_SOC_deviation: 20.2898 Fuel Consumption: 45.6605\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.509\n",
      "Episode: 47 Exploration P: 0.2824 Total reward: -264.22297784369675 SOC: 0.6065 Cumulative_SOC_deviation: 21.8590 Fuel Consumption: 45.6327\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.102\n",
      "Episode: 48 Exploration P: 0.2750 Total reward: -257.50949793041315 SOC: 0.6044 Cumulative_SOC_deviation: 21.1848 Fuel Consumption: 45.6611\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.606\n",
      "Episode: 49 Exploration P: 0.2678 Total reward: -250.60173752855496 SOC: 0.5994 Cumulative_SOC_deviation: 20.5392 Fuel Consumption: 45.2094\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.714\n",
      "Episode: 50 Exploration P: 0.2608 Total reward: -260.612407917727 SOC: 0.6054 Cumulative_SOC_deviation: 21.5072 Fuel Consumption: 45.5407\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.705\n",
      "Episode: 51 Exploration P: 0.2540 Total reward: -247.18718715241872 SOC: 0.6024 Cumulative_SOC_deviation: 20.1969 Fuel Consumption: 45.2183\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.768\n",
      "Episode: 52 Exploration P: 0.2474 Total reward: -244.8130125693266 SOC: 0.6026 Cumulative_SOC_deviation: 19.9596 Fuel Consumption: 45.2172\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.177\n",
      "Episode: 53 Exploration P: 0.2410 Total reward: -238.8265859488007 SOC: 0.6069 Cumulative_SOC_deviation: 19.3325 Fuel Consumption: 45.5020\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.288\n",
      "Episode: 54 Exploration P: 0.2347 Total reward: -253.74663033859656 SOC: 0.6063 Cumulative_SOC_deviation: 20.8302 Fuel Consumption: 45.4445\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.993\n",
      "Episode: 55 Exploration P: 0.2286 Total reward: -237.69700945496294 SOC: 0.6021 Cumulative_SOC_deviation: 19.2582 Fuel Consumption: 45.1153\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 120.007\n",
      "Episode: 56 Exploration P: 0.2227 Total reward: -249.5819373307711 SOC: 0.6021 Cumulative_SOC_deviation: 20.4476 Fuel Consumption: 45.1056\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.745\n",
      "Episode: 57 Exploration P: 0.2170 Total reward: -258.0439689573505 SOC: 0.6011 Cumulative_SOC_deviation: 21.3000 Fuel Consumption: 45.0437\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.107\n",
      "Episode: 58 Exploration P: 0.2114 Total reward: -251.66250033095753 SOC: 0.6070 Cumulative_SOC_deviation: 20.6215 Fuel Consumption: 45.4471\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.784\n",
      "Episode: 59 Exploration P: 0.2059 Total reward: -257.24521622675354 SOC: 0.6016 Cumulative_SOC_deviation: 21.2212 Fuel Consumption: 45.0337\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.948\n",
      "Episode: 60 Exploration P: 0.2006 Total reward: -253.17407894698152 SOC: 0.6039 Cumulative_SOC_deviation: 20.7850 Fuel Consumption: 45.3236\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.089\n",
      "Episode: 61 Exploration P: 0.1954 Total reward: -260.9912901934077 SOC: 0.6006 Cumulative_SOC_deviation: 21.5956 Fuel Consumption: 45.0358\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.751\n",
      "Episode: 62 Exploration P: 0.1904 Total reward: -247.68865365929963 SOC: 0.6007 Cumulative_SOC_deviation: 20.2614 Fuel Consumption: 45.0742\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.646\n",
      "Episode: 63 Exploration P: 0.1855 Total reward: -246.53470721940744 SOC: 0.6002 Cumulative_SOC_deviation: 20.1633 Fuel Consumption: 44.9016\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.714\n",
      "Episode: 64 Exploration P: 0.1808 Total reward: -256.26423316723935 SOC: 0.6019 Cumulative_SOC_deviation: 21.1136 Fuel Consumption: 45.1278\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.742\n",
      "Episode: 65 Exploration P: 0.1761 Total reward: -247.71781690537773 SOC: 0.6025 Cumulative_SOC_deviation: 20.2637 Fuel Consumption: 45.0808\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.250\n",
      "Episode: 66 Exploration P: 0.1716 Total reward: -249.33595351107948 SOC: 0.6005 Cumulative_SOC_deviation: 20.4477 Fuel Consumption: 44.8585\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.970\n",
      "Episode: 67 Exploration P: 0.1673 Total reward: -251.7714802726357 SOC: 0.5996 Cumulative_SOC_deviation: 20.6957 Fuel Consumption: 44.8149\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.836\n",
      "Episode: 68 Exploration P: 0.1630 Total reward: -253.5570255775699 SOC: 0.6010 Cumulative_SOC_deviation: 20.8586 Fuel Consumption: 44.9712\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.959\n",
      "Episode: 69 Exploration P: 0.1589 Total reward: -238.52939600925268 SOC: 0.5991 Cumulative_SOC_deviation: 19.3871 Fuel Consumption: 44.6580\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.913\n",
      "Episode: 70 Exploration P: 0.1548 Total reward: -252.91401126212563 SOC: 0.5998 Cumulative_SOC_deviation: 20.8198 Fuel Consumption: 44.7159\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.633\n",
      "Episode: 71 Exploration P: 0.1509 Total reward: -248.57237717480183 SOC: 0.5986 Cumulative_SOC_deviation: 20.3896 Fuel Consumption: 44.6759\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 114.347\n",
      "Episode: 72 Exploration P: 0.1471 Total reward: -260.4921041545787 SOC: 0.6013 Cumulative_SOC_deviation: 21.5593 Fuel Consumption: 44.8992\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 115.917\n",
      "Episode: 73 Exploration P: 0.1434 Total reward: -235.15547675978218 SOC: 0.5975 Cumulative_SOC_deviation: 19.0735 Fuel Consumption: 44.4202\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 114.798\n",
      "Episode: 74 Exploration P: 0.1398 Total reward: -271.7963772438689 SOC: 0.5947 Cumulative_SOC_deviation: 22.7327 Fuel Consumption: 44.4690\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 114.746\n",
      "Episode: 75 Exploration P: 0.1362 Total reward: -268.4693257589503 SOC: 0.5962 Cumulative_SOC_deviation: 22.3934 Fuel Consumption: 44.5357\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 114.608\n",
      "Episode: 76 Exploration P: 0.1328 Total reward: -255.99694394474074 SOC: 0.6001 Cumulative_SOC_deviation: 21.1200 Fuel Consumption: 44.7969\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.381\n",
      "Episode: 77 Exploration P: 0.1295 Total reward: -247.96011405814804 SOC: 0.5968 Cumulative_SOC_deviation: 20.3353 Fuel Consumption: 44.6069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 105.794\n",
      "Episode: 78 Exploration P: 0.1263 Total reward: -257.66896870759336 SOC: 0.5966 Cumulative_SOC_deviation: 21.3180 Fuel Consumption: 44.4890\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 105.623\n",
      "Episode: 79 Exploration P: 0.1231 Total reward: -260.38182822186536 SOC: 0.5983 Cumulative_SOC_deviation: 21.5706 Fuel Consumption: 44.6754\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 105.851\n",
      "Episode: 80 Exploration P: 0.1200 Total reward: -255.6544072738537 SOC: 0.5979 Cumulative_SOC_deviation: 21.1050 Fuel Consumption: 44.6043\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 105.867\n",
      "Episode: 81 Exploration P: 0.1171 Total reward: -261.88868696667697 SOC: 0.5983 Cumulative_SOC_deviation: 21.7156 Fuel Consumption: 44.7327\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 105.969\n",
      "Episode: 82 Exploration P: 0.1142 Total reward: -259.87088134977347 SOC: 0.5958 Cumulative_SOC_deviation: 21.5427 Fuel Consumption: 44.4435\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 100.774\n",
      "Episode: 83 Exploration P: 0.1113 Total reward: -248.8886193004028 SOC: 0.5980 Cumulative_SOC_deviation: 20.4342 Fuel Consumption: 44.5466\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 107.125\n",
      "Episode: 84 Exploration P: 0.1086 Total reward: -260.53593089168083 SOC: 0.5971 Cumulative_SOC_deviation: 21.6041 Fuel Consumption: 44.4947\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 107.016\n",
      "Episode: 85 Exploration P: 0.1059 Total reward: -267.56148277271507 SOC: 0.5974 Cumulative_SOC_deviation: 22.3162 Fuel Consumption: 44.3996\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.273\n",
      "Episode: 86 Exploration P: 0.1033 Total reward: -239.7157491169381 SOC: 0.5994 Cumulative_SOC_deviation: 19.5062 Fuel Consumption: 44.6542\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.124\n",
      "Episode: 87 Exploration P: 0.1008 Total reward: -242.71023336668182 SOC: 0.6002 Cumulative_SOC_deviation: 19.8005 Fuel Consumption: 44.7054\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 105.432\n",
      "Episode: 88 Exploration P: 0.0983 Total reward: -252.16029960092908 SOC: 0.5958 Cumulative_SOC_deviation: 20.7804 Fuel Consumption: 44.3558\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.025\n",
      "Episode: 89 Exploration P: 0.0960 Total reward: -232.9420850934727 SOC: 0.5987 Cumulative_SOC_deviation: 18.8377 Fuel Consumption: 44.5653\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.191\n",
      "Episode: 90 Exploration P: 0.0936 Total reward: -223.05212561622562 SOC: 0.6015 Cumulative_SOC_deviation: 17.8367 Fuel Consumption: 44.6855\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 105.904\n",
      "Episode: 91 Exploration P: 0.0914 Total reward: -220.57340968596895 SOC: 0.6018 Cumulative_SOC_deviation: 17.5641 Fuel Consumption: 44.9319\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.102\n",
      "Episode: 92 Exploration P: 0.0892 Total reward: -207.38751227609265 SOC: 0.6023 Cumulative_SOC_deviation: 16.2447 Fuel Consumption: 44.9406\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.661\n",
      "Episode: 93 Exploration P: 0.0870 Total reward: -202.65920293119692 SOC: 0.6056 Cumulative_SOC_deviation: 15.7419 Fuel Consumption: 45.2401\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.204\n",
      "Episode: 94 Exploration P: 0.0849 Total reward: -203.0618352160129 SOC: 0.6037 Cumulative_SOC_deviation: 15.8052 Fuel Consumption: 45.0097\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 105.692\n",
      "Episode: 95 Exploration P: 0.0829 Total reward: -200.1365061710454 SOC: 0.6065 Cumulative_SOC_deviation: 15.4937 Fuel Consumption: 45.1993\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.248\n",
      "Episode: 96 Exploration P: 0.0809 Total reward: -200.68228529939302 SOC: 0.6057 Cumulative_SOC_deviation: 15.5518 Fuel Consumption: 45.1640\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.677\n",
      "Episode: 97 Exploration P: 0.0790 Total reward: -199.0382902786742 SOC: 0.6038 Cumulative_SOC_deviation: 15.4075 Fuel Consumption: 44.9637\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.105\n",
      "Episode: 98 Exploration P: 0.0771 Total reward: -201.1900582954068 SOC: 0.6036 Cumulative_SOC_deviation: 15.6258 Fuel Consumption: 44.9322\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.376\n",
      "Episode: 99 Exploration P: 0.0753 Total reward: -207.4872349807858 SOC: 0.6048 Cumulative_SOC_deviation: 16.2457 Fuel Consumption: 45.0306\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.439\n",
      "Episode: 100 Exploration P: 0.0735 Total reward: -205.31321089977567 SOC: 0.6043 Cumulative_SOC_deviation: 16.0307 Fuel Consumption: 45.0058\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 17.759\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -1015.8745266779079 SOC: 0.8128 Cumulative_SOC_deviation: 95.3789 Fuel Consumption: 62.0855\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 17.779\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -973.4956863464142 SOC: 0.7995 Cumulative_SOC_deviation: 91.2493 Fuel Consumption: 61.0028\n",
      "WARNING:tensorflow:Layer batch_normalization_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_11 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 87.259\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -804.4509393438861 SOC: 0.7578 Cumulative_SOC_deviation: 74.6903 Fuel Consumption: 57.5482\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 102.740\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -787.530226853376 SOC: 0.7359 Cumulative_SOC_deviation: 73.1488 Fuel Consumption: 56.0418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 103.310\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -852.1571816969532 SOC: 0.6935 Cumulative_SOC_deviation: 79.9478 Fuel Consumption: 52.6792\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 103.504\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -770.2019261841501 SOC: 0.7128 Cumulative_SOC_deviation: 71.6093 Fuel Consumption: 54.1086\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 103.865\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -837.0485488127127 SOC: 0.6822 Cumulative_SOC_deviation: 78.5302 Fuel Consumption: 51.7463\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 103.981\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -848.6171592884332 SOC: 0.6701 Cumulative_SOC_deviation: 79.7662 Fuel Consumption: 50.9556\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.471\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -928.6298404849157 SOC: 0.6483 Cumulative_SOC_deviation: 87.9369 Fuel Consumption: 49.2610\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.510\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -1146.448017874695 SOC: 0.5907 Cumulative_SOC_deviation: 110.1632 Fuel Consumption: 44.8156\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 103.824\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -967.975434405199 SOC: 0.6053 Cumulative_SOC_deviation: 92.2205 Fuel Consumption: 45.7700\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 103.993\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -1051.9355525462097 SOC: 0.6010 Cumulative_SOC_deviation: 100.6371 Fuel Consumption: 45.5646\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.935\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -1444.311565569167 SOC: 0.5444 Cumulative_SOC_deviation: 140.2931 Fuel Consumption: 41.3803\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.193\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -1158.906048695965 SOC: 0.6050 Cumulative_SOC_deviation: 111.2885 Fuel Consumption: 46.0214\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.225\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -1459.1490978723189 SOC: 0.5472 Cumulative_SOC_deviation: 141.7861 Fuel Consumption: 41.2883\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.153\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -1440.0479151912834 SOC: 0.5529 Cumulative_SOC_deviation: 139.8215 Fuel Consumption: 41.8328\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.343\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -1746.4834784913387 SOC: 0.5107 Cumulative_SOC_deviation: 170.7786 Fuel Consumption: 38.6970\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.886\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -1810.2496004650181 SOC: 0.5033 Cumulative_SOC_deviation: 177.2040 Fuel Consumption: 38.2092\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.882\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -1806.7252443414488 SOC: 0.4964 Cumulative_SOC_deviation: 176.8845 Fuel Consumption: 37.8807\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.707\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -1954.7626706593915 SOC: 0.4674 Cumulative_SOC_deviation: 191.9214 Fuel Consumption: 35.5484\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 105.077\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -1896.6181151662681 SOC: 0.4740 Cumulative_SOC_deviation: 186.0538 Fuel Consumption: 36.0801\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.805\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -2049.1360066708685 SOC: 0.4663 Cumulative_SOC_deviation: 201.3602 Fuel Consumption: 35.5345\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.884\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -2239.3553040484667 SOC: 0.4385 Cumulative_SOC_deviation: 220.6032 Fuel Consumption: 33.3231\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 105.115\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -2311.4617944761067 SOC: 0.4235 Cumulative_SOC_deviation: 227.9128 Fuel Consumption: 32.3334\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.947\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -2252.2440187908414 SOC: 0.4173 Cumulative_SOC_deviation: 222.0396 Fuel Consumption: 31.8477\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.878\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -2374.7487580158504 SOC: 0.4016 Cumulative_SOC_deviation: 234.3943 Fuel Consumption: 30.8057\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.973\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -2540.4160786930674 SOC: 0.3886 Cumulative_SOC_deviation: 251.0456 Fuel Consumption: 29.9605\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.714\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -2595.122004163815 SOC: 0.3869 Cumulative_SOC_deviation: 256.5449 Fuel Consumption: 29.6727\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 105.308\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -2377.6598446823014 SOC: 0.3936 Cumulative_SOC_deviation: 234.7491 Fuel Consumption: 30.1691\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.782\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -2249.8180317763504 SOC: 0.6103 Cumulative_SOC_deviation: 220.2247 Fuel Consumption: 47.5714\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 99.455\n",
      "Episode: 31 Exploration P: 0.4332 Total reward: -2685.066600275528 SOC: 1.0000 Cumulative_SOC_deviation: 259.5836 Fuel Consumption: 89.2304\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 99.562\n",
      "Episode: 32 Exploration P: 0.4223 Total reward: -2595.5003804582184 SOC: 1.0000 Cumulative_SOC_deviation: 250.6965 Fuel Consumption: 88.5352\n",
      "Available condition is not avail... SOC: 0.9999410660221929\n",
      "elapsed_time: 99.653\n",
      "Episode: 33 Exploration P: 0.4117 Total reward: -2678.898567778313 SOC: 0.9999 Cumulative_SOC_deviation: 259.0055 Fuel Consumption: 88.8433\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 99.665\n",
      "Episode: 34 Exploration P: 0.4013 Total reward: -2791.8837834676187 SOC: 1.0000 Cumulative_SOC_deviation: 270.1091 Fuel Consumption: 90.7926\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 99.846\n",
      "Episode: 35 Exploration P: 0.3912 Total reward: -2711.047814522135 SOC: 1.0000 Cumulative_SOC_deviation: 262.1823 Fuel Consumption: 89.2253\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 99.651\n",
      "Episode: 36 Exploration P: 0.3814 Total reward: -2886.6302638896796 SOC: 1.0000 Cumulative_SOC_deviation: 279.4378 Fuel Consumption: 92.2519\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 99.853\n",
      "Episode: 37 Exploration P: 0.3718 Total reward: -2831.8028558630044 SOC: 1.0000 Cumulative_SOC_deviation: 273.8716 Fuel Consumption: 93.0865\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 99.980\n",
      "Episode: 38 Exploration P: 0.3625 Total reward: -2793.276181983908 SOC: 1.0000 Cumulative_SOC_deviation: 270.0853 Fuel Consumption: 92.4231\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 99.972\n",
      "Episode: 39 Exploration P: 0.3534 Total reward: -2962.0155696657293 SOC: 1.0000 Cumulative_SOC_deviation: 286.6748 Fuel Consumption: 95.2679\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 99.743\n",
      "Episode: 40 Exploration P: 0.3446 Total reward: -2825.5952120863794 SOC: 1.0000 Cumulative_SOC_deviation: 273.3674 Fuel Consumption: 91.9214\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.382\n",
      "Episode: 41 Exploration P: 0.3360 Total reward: -2929.590878382331 SOC: 1.0000 Cumulative_SOC_deviation: 283.5475 Fuel Consumption: 94.1157\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 99.933\n",
      "Episode: 42 Exploration P: 0.3276 Total reward: -2989.990005602384 SOC: 1.0000 Cumulative_SOC_deviation: 289.5089 Fuel Consumption: 94.9010\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 95.957\n",
      "Episode: 43 Exploration P: 0.3194 Total reward: -2921.060692918759 SOC: 1.0000 Cumulative_SOC_deviation: 282.7247 Fuel Consumption: 93.8140\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.204\n",
      "Episode: 44 Exploration P: 0.3114 Total reward: -2916.6163031309043 SOC: 1.0000 Cumulative_SOC_deviation: 282.1696 Fuel Consumption: 94.9202\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.673\n",
      "Episode: 45 Exploration P: 0.3037 Total reward: -2902.23436327413 SOC: 1.0000 Cumulative_SOC_deviation: 280.6565 Fuel Consumption: 95.6691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.693\n",
      "Episode: 46 Exploration P: 0.2961 Total reward: -3008.570680367199 SOC: 1.0000 Cumulative_SOC_deviation: 291.1958 Fuel Consumption: 96.6128\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.243\n",
      "Episode: 47 Exploration P: 0.2887 Total reward: -3019.6155514741245 SOC: 1.0000 Cumulative_SOC_deviation: 292.2919 Fuel Consumption: 96.6962\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.469\n",
      "Episode: 48 Exploration P: 0.2815 Total reward: -2967.4587797157924 SOC: 1.0000 Cumulative_SOC_deviation: 287.1409 Fuel Consumption: 96.0500\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.723\n",
      "Episode: 49 Exploration P: 0.2745 Total reward: -3054.7130036707367 SOC: 1.0000 Cumulative_SOC_deviation: 295.7690 Fuel Consumption: 97.0225\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.769\n",
      "Episode: 50 Exploration P: 0.2677 Total reward: -3002.1312120352977 SOC: 1.0000 Cumulative_SOC_deviation: 290.5932 Fuel Consumption: 96.1987\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.406\n",
      "Episode: 51 Exploration P: 0.2611 Total reward: -3056.413229307129 SOC: 1.0000 Cumulative_SOC_deviation: 295.9099 Fuel Consumption: 97.3146\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.596\n",
      "Episode: 52 Exploration P: 0.2546 Total reward: -3048.366432776966 SOC: 1.0000 Cumulative_SOC_deviation: 295.0131 Fuel Consumption: 98.2358\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.530\n",
      "Episode: 53 Exploration P: 0.2483 Total reward: -3058.143118828518 SOC: 1.0000 Cumulative_SOC_deviation: 296.0707 Fuel Consumption: 97.4366\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.776\n",
      "Episode: 54 Exploration P: 0.2422 Total reward: -3116.775232376733 SOC: 1.0000 Cumulative_SOC_deviation: 301.7987 Fuel Consumption: 98.7878\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.855\n",
      "Episode: 55 Exploration P: 0.2362 Total reward: -3175.9087954568918 SOC: 1.0000 Cumulative_SOC_deviation: 307.6365 Fuel Consumption: 99.5442\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.650\n",
      "Episode: 56 Exploration P: 0.2304 Total reward: -3154.9454444500034 SOC: 1.0000 Cumulative_SOC_deviation: 305.4735 Fuel Consumption: 100.2107\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.925\n",
      "Episode: 57 Exploration P: 0.2247 Total reward: -3201.8812637623114 SOC: 1.0000 Cumulative_SOC_deviation: 310.1078 Fuel Consumption: 100.8034\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.803\n",
      "Episode: 58 Exploration P: 0.2192 Total reward: -3147.54775600326 SOC: 1.0000 Cumulative_SOC_deviation: 304.7412 Fuel Consumption: 100.1358\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.666\n",
      "Episode: 59 Exploration P: 0.2138 Total reward: -3190.230963982511 SOC: 1.0000 Cumulative_SOC_deviation: 309.0233 Fuel Consumption: 99.9978\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.582\n",
      "Episode: 60 Exploration P: 0.2085 Total reward: -3142.4378962469577 SOC: 1.0000 Cumulative_SOC_deviation: 304.1919 Fuel Consumption: 100.5189\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.784\n",
      "Episode: 61 Exploration P: 0.2034 Total reward: -3271.0836501186377 SOC: 1.0000 Cumulative_SOC_deviation: 316.9594 Fuel Consumption: 101.4892\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.904\n",
      "Episode: 62 Exploration P: 0.1984 Total reward: -3133.5532102094594 SOC: 1.0000 Cumulative_SOC_deviation: 303.2290 Fuel Consumption: 101.2635\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.042\n",
      "Episode: 63 Exploration P: 0.1936 Total reward: -3245.3829141890665 SOC: 1.0000 Cumulative_SOC_deviation: 314.3556 Fuel Consumption: 101.8273\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.855\n",
      "Episode: 64 Exploration P: 0.1888 Total reward: -3222.4849697496165 SOC: 1.0000 Cumulative_SOC_deviation: 312.1433 Fuel Consumption: 101.0517\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.732\n",
      "Episode: 65 Exploration P: 0.1842 Total reward: -3259.530963296502 SOC: 1.0000 Cumulative_SOC_deviation: 315.6964 Fuel Consumption: 102.5666\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.862\n",
      "Episode: 66 Exploration P: 0.1797 Total reward: -3270.7361564452444 SOC: 1.0000 Cumulative_SOC_deviation: 316.8967 Fuel Consumption: 101.7695\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.922\n",
      "Episode: 67 Exploration P: 0.1754 Total reward: -3269.1090598616684 SOC: 1.0000 Cumulative_SOC_deviation: 316.5985 Fuel Consumption: 103.1240\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.827\n",
      "Episode: 68 Exploration P: 0.1711 Total reward: -3267.4809805650375 SOC: 1.0000 Cumulative_SOC_deviation: 316.4668 Fuel Consumption: 102.8127\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.849\n",
      "Episode: 69 Exploration P: 0.1670 Total reward: -3216.6300188075234 SOC: 1.0000 Cumulative_SOC_deviation: 311.4255 Fuel Consumption: 102.3751\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.828\n",
      "Episode: 70 Exploration P: 0.1629 Total reward: -3260.502132349381 SOC: 1.0000 Cumulative_SOC_deviation: 315.8457 Fuel Consumption: 102.0456\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.340\n",
      "Episode: 71 Exploration P: 0.1590 Total reward: -3326.9675536978425 SOC: 1.0000 Cumulative_SOC_deviation: 322.3503 Fuel Consumption: 103.4642\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.982\n",
      "Episode: 72 Exploration P: 0.1551 Total reward: -3273.3902890586724 SOC: 1.0000 Cumulative_SOC_deviation: 316.9794 Fuel Consumption: 103.5958\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.042\n",
      "Episode: 73 Exploration P: 0.1514 Total reward: -3220.9942669474513 SOC: 1.0000 Cumulative_SOC_deviation: 311.8375 Fuel Consumption: 102.6190\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.703\n",
      "Episode: 74 Exploration P: 0.1478 Total reward: -3273.336468694997 SOC: 1.0000 Cumulative_SOC_deviation: 317.0526 Fuel Consumption: 102.8105\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.931\n",
      "Episode: 75 Exploration P: 0.1442 Total reward: -3332.0028180923305 SOC: 1.0000 Cumulative_SOC_deviation: 322.6358 Fuel Consumption: 105.6446\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.047\n",
      "Episode: 76 Exploration P: 0.1408 Total reward: -3337.4956423681056 SOC: 1.0000 Cumulative_SOC_deviation: 323.3174 Fuel Consumption: 104.3212\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.446\n",
      "Episode: 77 Exploration P: 0.1374 Total reward: -3343.165954240306 SOC: 1.0000 Cumulative_SOC_deviation: 323.8561 Fuel Consumption: 104.6047\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.321\n",
      "Episode: 78 Exploration P: 0.1341 Total reward: -3349.434597978707 SOC: 1.0000 Cumulative_SOC_deviation: 324.4118 Fuel Consumption: 105.3162\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.201\n",
      "Episode: 79 Exploration P: 0.1309 Total reward: -3310.0153256843478 SOC: 1.0000 Cumulative_SOC_deviation: 320.5333 Fuel Consumption: 104.6828\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.286\n",
      "Episode: 80 Exploration P: 0.1278 Total reward: -3347.2013659785234 SOC: 1.0000 Cumulative_SOC_deviation: 324.2131 Fuel Consumption: 105.0701\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.775\n",
      "Episode: 81 Exploration P: 0.1248 Total reward: -3367.8254318489767 SOC: 1.0000 Cumulative_SOC_deviation: 326.2371 Fuel Consumption: 105.4542\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.073\n",
      "Episode: 82 Exploration P: 0.1218 Total reward: -3379.2963317694703 SOC: 1.0000 Cumulative_SOC_deviation: 327.3846 Fuel Consumption: 105.4499\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.988\n",
      "Episode: 83 Exploration P: 0.1189 Total reward: -3345.6681033005752 SOC: 1.0000 Cumulative_SOC_deviation: 324.0610 Fuel Consumption: 105.0583\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.764\n",
      "Episode: 84 Exploration P: 0.1161 Total reward: -3336.35143744248 SOC: 1.0000 Cumulative_SOC_deviation: 323.1478 Fuel Consumption: 104.8732\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.007\n",
      "Episode: 85 Exploration P: 0.1134 Total reward: -3353.738131020066 SOC: 1.0000 Cumulative_SOC_deviation: 324.7566 Fuel Consumption: 106.1721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.013\n",
      "Episode: 86 Exploration P: 0.1107 Total reward: -3341.7487396454912 SOC: 1.0000 Cumulative_SOC_deviation: 323.7324 Fuel Consumption: 104.4250\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.234\n",
      "Episode: 87 Exploration P: 0.1081 Total reward: -3425.696522009548 SOC: 1.0000 Cumulative_SOC_deviation: 331.8567 Fuel Consumption: 107.1296\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.004\n",
      "Episode: 88 Exploration P: 0.1056 Total reward: -3396.9713512978965 SOC: 1.0000 Cumulative_SOC_deviation: 329.0045 Fuel Consumption: 106.9263\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.258\n",
      "Episode: 89 Exploration P: 0.1031 Total reward: -3392.537964387631 SOC: 1.0000 Cumulative_SOC_deviation: 328.6430 Fuel Consumption: 106.1079\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.361\n",
      "Episode: 90 Exploration P: 0.1007 Total reward: -3418.1125876771534 SOC: 1.0000 Cumulative_SOC_deviation: 331.2209 Fuel Consumption: 105.9035\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 100.796\n",
      "Episode: 91 Exploration P: 0.0984 Total reward: -3407.2840102243595 SOC: 1.0000 Cumulative_SOC_deviation: 330.0736 Fuel Consumption: 106.5476\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.023\n",
      "Episode: 92 Exploration P: 0.0961 Total reward: -3382.699900253954 SOC: 1.0000 Cumulative_SOC_deviation: 327.5771 Fuel Consumption: 106.9285\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.132\n",
      "Episode: 93 Exploration P: 0.0939 Total reward: -3451.270645189637 SOC: 1.0000 Cumulative_SOC_deviation: 334.3347 Fuel Consumption: 107.9234\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.473\n",
      "Episode: 94 Exploration P: 0.0917 Total reward: -3420.6787340275196 SOC: 1.0000 Cumulative_SOC_deviation: 331.3456 Fuel Consumption: 107.2227\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.154\n",
      "Episode: 95 Exploration P: 0.0896 Total reward: -3406.030929736394 SOC: 1.0000 Cumulative_SOC_deviation: 329.9454 Fuel Consumption: 106.5765\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.099\n",
      "Episode: 96 Exploration P: 0.0876 Total reward: -3469.988988330819 SOC: 1.0000 Cumulative_SOC_deviation: 336.2275 Fuel Consumption: 107.7138\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.444\n",
      "Episode: 97 Exploration P: 0.0856 Total reward: -3430.899627297332 SOC: 1.0000 Cumulative_SOC_deviation: 332.3663 Fuel Consumption: 107.2366\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.176\n",
      "Episode: 98 Exploration P: 0.0836 Total reward: -3452.972814844448 SOC: 1.0000 Cumulative_SOC_deviation: 334.5518 Fuel Consumption: 107.4548\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 101.624\n",
      "Episode: 99 Exploration P: 0.0817 Total reward: -3421.3335305067585 SOC: 1.0000 Cumulative_SOC_deviation: 331.3216 Fuel Consumption: 108.1171\n",
      "Available condition is not avail... SOC: 1\n",
      "elapsed_time: 113.669\n",
      "Episode: 100 Exploration P: 0.0799 Total reward: -3414.6471090745117 SOC: 1.0000 Cumulative_SOC_deviation: 330.7531 Fuel Consumption: 107.1157\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 18.447\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -939.8426623477775 SOC: 0.7862 Cumulative_SOC_deviation: 87.9936 Fuel Consumption: 59.9062\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 18.621\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -995.0590705108335 SOC: 0.7988 Cumulative_SOC_deviation: 93.4165 Fuel Consumption: 60.8937\n",
      "WARNING:tensorflow:Layer batch_normalization_14 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_15 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_13 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_12 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 86.660\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -766.8889419598733 SOC: 0.7501 Cumulative_SOC_deviation: 70.9973 Fuel Consumption: 56.9163\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 107.578\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -768.6159012455622 SOC: 0.7257 Cumulative_SOC_deviation: 71.3434 Fuel Consumption: 55.1816\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 109.222\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -815.2843090302194 SOC: 0.7040 Cumulative_SOC_deviation: 76.1918 Fuel Consumption: 53.3661\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 110.452\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -774.4742224807201 SOC: 0.7003 Cumulative_SOC_deviation: 72.1179 Fuel Consumption: 53.2955\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 110.256\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -762.8307425644266 SOC: 0.6810 Cumulative_SOC_deviation: 71.1184 Fuel Consumption: 51.6468\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 109.259\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -828.5953618692571 SOC: 0.6652 Cumulative_SOC_deviation: 77.8042 Fuel Consumption: 50.5534\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 111.926\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -938.8960179761424 SOC: 0.6253 Cumulative_SOC_deviation: 89.1409 Fuel Consumption: 47.4871\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 111.656\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -930.7507745863898 SOC: 0.6285 Cumulative_SOC_deviation: 88.3079 Fuel Consumption: 47.6722\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 109.840\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -1023.0305067323541 SOC: 0.6029 Cumulative_SOC_deviation: 97.7414 Fuel Consumption: 45.6170\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 110.543\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -1041.687999965968 SOC: 0.6034 Cumulative_SOC_deviation: 99.6049 Fuel Consumption: 45.6394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 112.819\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -1426.8665529193058 SOC: 0.5610 Cumulative_SOC_deviation: 138.4379 Fuel Consumption: 42.4876\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 10\n",
    "results_dict = {} \n",
    "for trial in range(num_trials): \n",
    "    print()\n",
    "    print(\"Trial {}\".format(trial))\n",
    "    \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer = initialization()\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    for ep in range(total_episodes): \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "        )\n",
    "    \n",
    "    model.save_weights(\"./DDPG12_2_trial{}\".format(trial+1))\n",
    "    print(\"model is saved..\")\n",
    "    \n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG12_2.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN_SOC_dev import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 1000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 0\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 1 Total reward: -1007.1097181963142 Explore P: 0.9732 SOC: 0.8037 Cumulative_SOC_deviation: 94.6066 Fuel Consumption: 61.0439\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 2 Total reward: -1037.2550683756192 Explore P: 0.9471 SOC: 0.8149 Cumulative_SOC_deviation: 97.5200 Fuel Consumption: 62.0549\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -861.5289099890156 Explore P: 0.9217 SOC: 0.7661 Cumulative_SOC_deviation: 80.3539 Fuel Consumption: 57.9895\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -900.9313656684636 Explore P: 0.8970 SOC: 0.7876 Cumulative_SOC_deviation: 84.1100 Fuel Consumption: 59.8316\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -869.8696524209361 Explore P: 0.8730 SOC: 0.7674 Cumulative_SOC_deviation: 81.1642 Fuel Consumption: 58.2277\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -778.724839152508 Explore P: 0.8496 SOC: 0.7606 Cumulative_SOC_deviation: 72.1152 Fuel Consumption: 57.5724\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -765.8309896571826 Explore P: 0.8269 SOC: 0.7268 Cumulative_SOC_deviation: 71.0862 Fuel Consumption: 54.9686\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -721.3572104834145 Explore P: 0.8048 SOC: 0.7280 Cumulative_SOC_deviation: 66.6390 Fuel Consumption: 54.9670\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -809.1855059917111 Explore P: 0.7832 SOC: 0.6951 Cumulative_SOC_deviation: 75.6705 Fuel Consumption: 52.4803\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -831.9658220722079 Explore P: 0.7623 SOC: 0.7722 Cumulative_SOC_deviation: 77.3521 Fuel Consumption: 58.4451\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -818.7478120476205 Explore P: 0.7419 SOC: 0.7390 Cumulative_SOC_deviation: 76.2850 Fuel Consumption: 55.8980\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -726.6946010201286 Explore P: 0.7221 SOC: 0.7243 Cumulative_SOC_deviation: 67.2208 Fuel Consumption: 54.4861\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -748.6007790239952 Explore P: 0.7028 SOC: 0.7019 Cumulative_SOC_deviation: 69.5880 Fuel Consumption: 52.7210\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -693.4290007609156 Explore P: 0.6840 SOC: 0.6789 Cumulative_SOC_deviation: 64.2408 Fuel Consumption: 51.0212\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -713.847914802891 Explore P: 0.6658 SOC: 0.7224 Cumulative_SOC_deviation: 65.9465 Fuel Consumption: 54.3832\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -681.6704062749186 Explore P: 0.6480 SOC: 0.6905 Cumulative_SOC_deviation: 62.9640 Fuel Consumption: 52.0302\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -755.2550778238908 Explore P: 0.6307 SOC: 0.7294 Cumulative_SOC_deviation: 69.9866 Fuel Consumption: 55.3892\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -727.0669383157347 Explore P: 0.6139 SOC: 0.6614 Cumulative_SOC_deviation: 67.7475 Fuel Consumption: 49.5916\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -777.0639541909306 Explore P: 0.5976 SOC: 0.7132 Cumulative_SOC_deviation: 72.3186 Fuel Consumption: 53.8780\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -703.180091743055 Explore P: 0.5816 SOC: 0.6979 Cumulative_SOC_deviation: 65.0706 Fuel Consumption: 52.4742\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -649.8241281185212 Explore P: 0.5662 SOC: 0.6890 Cumulative_SOC_deviation: 59.8244 Fuel Consumption: 51.5802\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -665.0425781487114 Explore P: 0.5511 SOC: 0.6672 Cumulative_SOC_deviation: 61.5160 Fuel Consumption: 49.8830\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -650.2437589651585 Explore P: 0.5364 SOC: 0.6889 Cumulative_SOC_deviation: 59.8562 Fuel Consumption: 51.6816\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -736.9212105335477 Explore P: 0.5222 SOC: 0.6664 Cumulative_SOC_deviation: 68.7123 Fuel Consumption: 49.7979\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -598.0182383516029 Explore P: 0.5083 SOC: 0.6802 Cumulative_SOC_deviation: 54.7093 Fuel Consumption: 50.9255\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -650.2574611428506 Explore P: 0.4948 SOC: 0.7136 Cumulative_SOC_deviation: 59.6886 Fuel Consumption: 53.3712\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -659.8766987978538 Explore P: 0.4817 SOC: 0.6866 Cumulative_SOC_deviation: 60.8584 Fuel Consumption: 51.2924\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -558.5848906396974 Explore P: 0.4689 SOC: 0.7103 Cumulative_SOC_deviation: 50.5281 Fuel Consumption: 53.3043\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -622.7215169137323 Explore P: 0.4565 SOC: 0.6979 Cumulative_SOC_deviation: 57.0386 Fuel Consumption: 52.3353\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -612.527690459259 Explore P: 0.4444 SOC: 0.6605 Cumulative_SOC_deviation: 56.3130 Fuel Consumption: 49.3975\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -747.7052222781001 Explore P: 0.4326 SOC: 0.6877 Cumulative_SOC_deviation: 69.6041 Fuel Consumption: 51.6643\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -926.4103554359718 Explore P: 0.4212 SOC: 0.7550 Cumulative_SOC_deviation: 86.9270 Fuel Consumption: 57.1406\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -576.758604555108 Explore P: 0.4100 SOC: 0.6783 Cumulative_SOC_deviation: 52.6156 Fuel Consumption: 50.6021\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -779.0056147869891 Explore P: 0.3992 SOC: 0.7460 Cumulative_SOC_deviation: 72.2746 Fuel Consumption: 56.2593\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -593.5113759526129 Explore P: 0.3887 SOC: 0.6848 Cumulative_SOC_deviation: 54.2505 Fuel Consumption: 51.0060\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -546.2296429168528 Explore P: 0.3784 SOC: 0.6689 Cumulative_SOC_deviation: 49.5911 Fuel Consumption: 50.3188\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -444.7707151258853 Explore P: 0.3684 SOC: 0.6661 Cumulative_SOC_deviation: 39.5104 Fuel Consumption: 49.6671\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -502.83898916354 Explore P: 0.3587 SOC: 0.6501 Cumulative_SOC_deviation: 45.4147 Fuel Consumption: 48.6921\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -416.0749551801576 Explore P: 0.3493 SOC: 0.6308 Cumulative_SOC_deviation: 36.9099 Fuel Consumption: 46.9756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -381.0889470303793 Explore P: 0.3401 SOC: 0.6361 Cumulative_SOC_deviation: 33.3805 Fuel Consumption: 47.2842\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -451.5865047395877 Explore P: 0.3311 SOC: 0.6244 Cumulative_SOC_deviation: 40.4771 Fuel Consumption: 46.8160\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -495.6189252881138 Explore P: 0.3224 SOC: 0.6199 Cumulative_SOC_deviation: 44.9537 Fuel Consumption: 46.0821\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -591.1343620182736 Explore P: 0.3140 SOC: 0.6565 Cumulative_SOC_deviation: 54.2007 Fuel Consumption: 49.1274\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -683.1100009689457 Explore P: 0.3057 SOC: 0.7147 Cumulative_SOC_deviation: 62.9390 Fuel Consumption: 53.7204\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -476.16690817623953 Explore P: 0.2977 SOC: 0.6369 Cumulative_SOC_deviation: 42.8784 Fuel Consumption: 47.3831\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -516.6412995377677 Explore P: 0.2899 SOC: 0.6323 Cumulative_SOC_deviation: 46.9407 Fuel Consumption: 47.2346\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -497.16903066460713 Explore P: 0.2824 SOC: 0.6291 Cumulative_SOC_deviation: 45.0621 Fuel Consumption: 46.5479\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -409.3926956465216 Explore P: 0.2750 SOC: 0.6285 Cumulative_SOC_deviation: 36.2735 Fuel Consumption: 46.6573\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -354.54488410338115 Explore P: 0.2678 SOC: 0.6173 Cumulative_SOC_deviation: 30.8538 Fuel Consumption: 46.0071\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -475.1465918715494 Explore P: 0.2608 SOC: 0.6162 Cumulative_SOC_deviation: 42.9463 Fuel Consumption: 45.6833\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -499.5651505631933 Explore P: 0.2540 SOC: 0.6205 Cumulative_SOC_deviation: 45.3460 Fuel Consumption: 46.1049\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -339.29348236720733 Explore P: 0.2474 SOC: 0.6132 Cumulative_SOC_deviation: 29.3922 Fuel Consumption: 45.3711\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -451.2384649510846 Explore P: 0.2410 SOC: 0.6169 Cumulative_SOC_deviation: 40.5467 Fuel Consumption: 45.7710\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -345.1224201616136 Explore P: 0.2347 SOC: 0.6148 Cumulative_SOC_deviation: 29.9239 Fuel Consumption: 45.8830\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -381.1054994556697 Explore P: 0.2286 SOC: 0.6108 Cumulative_SOC_deviation: 33.5649 Fuel Consumption: 45.4568\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -316.45135271331185 Explore P: 0.2227 SOC: 0.6063 Cumulative_SOC_deviation: 27.1418 Fuel Consumption: 45.0336\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -437.67993527213576 Explore P: 0.2170 SOC: 0.6132 Cumulative_SOC_deviation: 39.1954 Fuel Consumption: 45.7259\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -488.26997743794004 Explore P: 0.2114 SOC: 0.6165 Cumulative_SOC_deviation: 44.2125 Fuel Consumption: 46.1445\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -335.6310090517517 Explore P: 0.2059 SOC: 0.6139 Cumulative_SOC_deviation: 29.0055 Fuel Consumption: 45.5759\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -363.5474228130856 Explore P: 0.2006 SOC: 0.6140 Cumulative_SOC_deviation: 31.7868 Fuel Consumption: 45.6798\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -373.1487470131855 Explore P: 0.1954 SOC: 0.6179 Cumulative_SOC_deviation: 32.7234 Fuel Consumption: 45.9144\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -431.82037139972886 Explore P: 0.1904 SOC: 0.6102 Cumulative_SOC_deviation: 38.6485 Fuel Consumption: 45.3351\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -404.1731873394705 Explore P: 0.1855 SOC: 0.6114 Cumulative_SOC_deviation: 35.8944 Fuel Consumption: 45.2292\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -478.3945197904776 Explore P: 0.1808 SOC: 0.6092 Cumulative_SOC_deviation: 43.3039 Fuel Consumption: 45.3559\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -547.5905967977124 Explore P: 0.1761 SOC: 0.6153 Cumulative_SOC_deviation: 50.2005 Fuel Consumption: 45.5855\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -345.35248938384115 Explore P: 0.1716 SOC: 0.6103 Cumulative_SOC_deviation: 29.9849 Fuel Consumption: 45.5039\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -423.2143518018975 Explore P: 0.1673 SOC: 0.6148 Cumulative_SOC_deviation: 37.7680 Fuel Consumption: 45.5348\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -380.78291368102276 Explore P: 0.1630 SOC: 0.6078 Cumulative_SOC_deviation: 33.5515 Fuel Consumption: 45.2677\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -462.78302156848986 Explore P: 0.1589 SOC: 0.6111 Cumulative_SOC_deviation: 41.7404 Fuel Consumption: 45.3792\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -360.5369318744594 Explore P: 0.1548 SOC: 0.6071 Cumulative_SOC_deviation: 31.5565 Fuel Consumption: 44.9723\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -351.9089860385179 Explore P: 0.1509 SOC: 0.6126 Cumulative_SOC_deviation: 30.6428 Fuel Consumption: 45.4811\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -353.7177620988863 Explore P: 0.1471 SOC: 0.6168 Cumulative_SOC_deviation: 30.7912 Fuel Consumption: 45.8054\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -395.5335732700297 Explore P: 0.1434 SOC: 0.6113 Cumulative_SOC_deviation: 35.0249 Fuel Consumption: 45.2850\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -444.0332474039148 Explore P: 0.1398 SOC: 0.6097 Cumulative_SOC_deviation: 39.8838 Fuel Consumption: 45.1948\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -270.5503206876188 Explore P: 0.1362 SOC: 0.6003 Cumulative_SOC_deviation: 22.5991 Fuel Consumption: 44.5598\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -262.02949072224834 Explore P: 0.1328 SOC: 0.6030 Cumulative_SOC_deviation: 21.7477 Fuel Consumption: 44.5522\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -326.81716845145377 Explore P: 0.1295 SOC: 0.6115 Cumulative_SOC_deviation: 28.1412 Fuel Consumption: 45.4056\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -314.0362336310075 Explore P: 0.1263 SOC: 0.6153 Cumulative_SOC_deviation: 26.8491 Fuel Consumption: 45.5455\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -279.7161862167084 Explore P: 0.1231 SOC: 0.6060 Cumulative_SOC_deviation: 23.4660 Fuel Consumption: 45.0559\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -273.1200891813859 Explore P: 0.1200 SOC: 0.6087 Cumulative_SOC_deviation: 22.8293 Fuel Consumption: 44.8268\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -332.99293829639447 Explore P: 0.1171 SOC: 0.6037 Cumulative_SOC_deviation: 28.8427 Fuel Consumption: 44.5659\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -327.3739686523056 Explore P: 0.1142 SOC: 0.6058 Cumulative_SOC_deviation: 28.2460 Fuel Consumption: 44.9135\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -338.02350814067506 Explore P: 0.1113 SOC: 0.6062 Cumulative_SOC_deviation: 29.2935 Fuel Consumption: 45.0889\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -321.826299450717 Explore P: 0.1086 SOC: 0.6199 Cumulative_SOC_deviation: 27.5989 Fuel Consumption: 45.8369\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -256.61416072183056 Explore P: 0.1059 SOC: 0.6067 Cumulative_SOC_deviation: 21.1872 Fuel Consumption: 44.7417\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -492.4773738955382 Explore P: 0.1033 SOC: 0.6099 Cumulative_SOC_deviation: 44.6894 Fuel Consumption: 45.5830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -514.9674512739921 Explore P: 0.1008 SOC: 0.6062 Cumulative_SOC_deviation: 46.9513 Fuel Consumption: 45.4542\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -458.03410607539206 Explore P: 0.0983 SOC: 0.6073 Cumulative_SOC_deviation: 41.2512 Fuel Consumption: 45.5216\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -308.4624996109076 Explore P: 0.0960 SOC: 0.6066 Cumulative_SOC_deviation: 26.3683 Fuel Consumption: 44.7797\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -421.7376533097492 Explore P: 0.0936 SOC: 0.6094 Cumulative_SOC_deviation: 37.6577 Fuel Consumption: 45.1603\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -322.82826248602726 Explore P: 0.0914 SOC: 0.6100 Cumulative_SOC_deviation: 27.7615 Fuel Consumption: 45.2130\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -358.29269626735675 Explore P: 0.0892 SOC: 0.6058 Cumulative_SOC_deviation: 31.3044 Fuel Consumption: 45.2485\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -347.8735079460228 Explore P: 0.0870 SOC: 0.6056 Cumulative_SOC_deviation: 30.2731 Fuel Consumption: 45.1421\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -334.7070635205805 Explore P: 0.0849 SOC: 0.6054 Cumulative_SOC_deviation: 28.9681 Fuel Consumption: 45.0265\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -302.8453360593537 Explore P: 0.0829 SOC: 0.6087 Cumulative_SOC_deviation: 25.7778 Fuel Consumption: 45.0671\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -295.6049337460052 Explore P: 0.0809 SOC: 0.6070 Cumulative_SOC_deviation: 25.0622 Fuel Consumption: 44.9829\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -357.12318187024124 Explore P: 0.0790 SOC: 0.6067 Cumulative_SOC_deviation: 31.2356 Fuel Consumption: 44.7676\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -332.5671859362012 Explore P: 0.0771 SOC: 0.6113 Cumulative_SOC_deviation: 28.7350 Fuel Consumption: 45.2171\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -345.19284083554584 Explore P: 0.0753 SOC: 0.6075 Cumulative_SOC_deviation: 30.0240 Fuel Consumption: 44.9530\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -388.6093618156768 Explore P: 0.0735 SOC: 0.6072 Cumulative_SOC_deviation: 34.3457 Fuel Consumption: 45.1527\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -327.77297349785124 Explore P: 0.0718 SOC: 0.6128 Cumulative_SOC_deviation: 28.2165 Fuel Consumption: 45.6083\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -325.45635746732296 Explore P: 0.0701 SOC: 0.6068 Cumulative_SOC_deviation: 28.0638 Fuel Consumption: 44.8187\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -379.8681266677378 Explore P: 0.0685 SOC: 0.6083 Cumulative_SOC_deviation: 33.4754 Fuel Consumption: 45.1137\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -332.3617792559998 Explore P: 0.0669 SOC: 0.6033 Cumulative_SOC_deviation: 28.7573 Fuel Consumption: 44.7883\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -394.28304595002913 Explore P: 0.0654 SOC: 0.6089 Cumulative_SOC_deviation: 34.9331 Fuel Consumption: 44.9520\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -404.0980112821388 Explore P: 0.0639 SOC: 0.6098 Cumulative_SOC_deviation: 35.9041 Fuel Consumption: 45.0574\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -322.1398686027457 Explore P: 0.0624 SOC: 0.6031 Cumulative_SOC_deviation: 27.7479 Fuel Consumption: 44.6606\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -346.71152579324274 Explore P: 0.0610 SOC: 0.6121 Cumulative_SOC_deviation: 30.1304 Fuel Consumption: 45.4071\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -375.66408581298424 Explore P: 0.0596 SOC: 0.6063 Cumulative_SOC_deviation: 33.0627 Fuel Consumption: 45.0367\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -326.6357720313076 Explore P: 0.0583 SOC: 0.6029 Cumulative_SOC_deviation: 28.1805 Fuel Consumption: 44.8304\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -386.4448125905935 Explore P: 0.0570 SOC: 0.6076 Cumulative_SOC_deviation: 34.1767 Fuel Consumption: 44.6779\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -407.20172626861466 Explore P: 0.0557 SOC: 0.6037 Cumulative_SOC_deviation: 36.2813 Fuel Consumption: 44.3885\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -324.2692477442034 Explore P: 0.0545 SOC: 0.6047 Cumulative_SOC_deviation: 27.9485 Fuel Consumption: 44.7838\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -366.60391516064755 Explore P: 0.0533 SOC: 0.6075 Cumulative_SOC_deviation: 32.1647 Fuel Consumption: 44.9566\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -469.06912580840105 Explore P: 0.0521 SOC: 0.6059 Cumulative_SOC_deviation: 42.3958 Fuel Consumption: 45.1106\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -523.5739768318819 Explore P: 0.0510 SOC: 0.6068 Cumulative_SOC_deviation: 47.8583 Fuel Consumption: 44.9910\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -335.44652164257155 Explore P: 0.0498 SOC: 0.6042 Cumulative_SOC_deviation: 29.0565 Fuel Consumption: 44.8816\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -312.6000950544465 Explore P: 0.0488 SOC: 0.6083 Cumulative_SOC_deviation: 26.7466 Fuel Consumption: 45.1340\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -378.9295584505415 Explore P: 0.0477 SOC: 0.6108 Cumulative_SOC_deviation: 33.3654 Fuel Consumption: 45.2753\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -296.4166450591591 Explore P: 0.0467 SOC: 0.6072 Cumulative_SOC_deviation: 25.1602 Fuel Consumption: 44.8147\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -306.1601649267701 Explore P: 0.0457 SOC: 0.6056 Cumulative_SOC_deviation: 26.1438 Fuel Consumption: 44.7224\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -304.3351740730404 Explore P: 0.0447 SOC: 0.6080 Cumulative_SOC_deviation: 25.9453 Fuel Consumption: 44.8826\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -385.71299949766853 Explore P: 0.0438 SOC: 0.6060 Cumulative_SOC_deviation: 34.0688 Fuel Consumption: 45.0250\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -281.2225319662832 Explore P: 0.0429 SOC: 0.6042 Cumulative_SOC_deviation: 23.6452 Fuel Consumption: 44.7706\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -277.8289796597547 Explore P: 0.0420 SOC: 0.6030 Cumulative_SOC_deviation: 23.3302 Fuel Consumption: 44.5273\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -439.00100586993045 Explore P: 0.0411 SOC: 0.6055 Cumulative_SOC_deviation: 39.3873 Fuel Consumption: 45.1279\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -558.6752777295922 Explore P: 0.0403 SOC: 0.6124 Cumulative_SOC_deviation: 51.2790 Fuel Consumption: 45.8850\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -295.86278901459957 Explore P: 0.0395 SOC: 0.6056 Cumulative_SOC_deviation: 25.1014 Fuel Consumption: 44.8491\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -346.8137329728303 Explore P: 0.0387 SOC: 0.6047 Cumulative_SOC_deviation: 30.2044 Fuel Consumption: 44.7696\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -301.9043893600377 Explore P: 0.0379 SOC: 0.6075 Cumulative_SOC_deviation: 25.6870 Fuel Consumption: 45.0346\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -282.16671640423044 Explore P: 0.0371 SOC: 0.6052 Cumulative_SOC_deviation: 23.7237 Fuel Consumption: 44.9297\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -337.1998983631857 Explore P: 0.0364 SOC: 0.6126 Cumulative_SOC_deviation: 29.1844 Fuel Consumption: 45.3554\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -293.87323357190786 Explore P: 0.0357 SOC: 0.6091 Cumulative_SOC_deviation: 24.8782 Fuel Consumption: 45.0909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -220.39975021871862 Explore P: 0.0350 SOC: 0.6080 Cumulative_SOC_deviation: 17.5368 Fuel Consumption: 45.0316\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -329.90019082674553 Explore P: 0.0343 SOC: 0.6030 Cumulative_SOC_deviation: 28.5154 Fuel Consumption: 44.7463\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -227.9461971806044 Explore P: 0.0336 SOC: 0.6088 Cumulative_SOC_deviation: 18.2786 Fuel Consumption: 45.1603\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -241.63339533176287 Explore P: 0.0330 SOC: 0.6053 Cumulative_SOC_deviation: 19.6811 Fuel Consumption: 44.8228\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -304.1375372311577 Explore P: 0.0324 SOC: 0.6048 Cumulative_SOC_deviation: 25.9363 Fuel Consumption: 44.7741\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -376.76896379192857 Explore P: 0.0318 SOC: 0.6044 Cumulative_SOC_deviation: 33.1759 Fuel Consumption: 45.0103\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -254.37496723897806 Explore P: 0.0312 SOC: 0.6070 Cumulative_SOC_deviation: 20.9606 Fuel Consumption: 44.7691\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -310.8141613928859 Explore P: 0.0306 SOC: 0.6062 Cumulative_SOC_deviation: 26.6124 Fuel Consumption: 44.6900\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -310.20166231351874 Explore P: 0.0301 SOC: 0.6048 Cumulative_SOC_deviation: 26.5438 Fuel Consumption: 44.7640\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -257.86439606680096 Explore P: 0.0295 SOC: 0.6074 Cumulative_SOC_deviation: 21.3310 Fuel Consumption: 44.5547\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -263.1929086583985 Explore P: 0.0290 SOC: 0.6097 Cumulative_SOC_deviation: 21.7938 Fuel Consumption: 45.2546\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -291.2378966421581 Explore P: 0.0285 SOC: 0.6054 Cumulative_SOC_deviation: 24.6544 Fuel Consumption: 44.6941\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -312.28621273795704 Explore P: 0.0280 SOC: 0.6056 Cumulative_SOC_deviation: 26.7717 Fuel Consumption: 44.5694\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -297.5672380623195 Explore P: 0.0275 SOC: 0.6068 Cumulative_SOC_deviation: 25.2574 Fuel Consumption: 44.9936\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -394.5483083048074 Explore P: 0.0270 SOC: 0.6066 Cumulative_SOC_deviation: 34.9394 Fuel Consumption: 45.1547\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -259.16684265791014 Explore P: 0.0265 SOC: 0.6049 Cumulative_SOC_deviation: 21.4845 Fuel Consumption: 44.3221\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -287.0946022514823 Explore P: 0.0261 SOC: 0.6070 Cumulative_SOC_deviation: 24.2347 Fuel Consumption: 44.7478\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -250.5684228173229 Explore P: 0.0257 SOC: 0.6077 Cumulative_SOC_deviation: 20.5826 Fuel Consumption: 44.7422\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -271.80669190204503 Explore P: 0.0252 SOC: 0.6065 Cumulative_SOC_deviation: 22.7340 Fuel Consumption: 44.4670\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -304.4592171327287 Explore P: 0.0248 SOC: 0.6074 Cumulative_SOC_deviation: 25.9631 Fuel Consumption: 44.8279\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -334.19345362727023 Explore P: 0.0244 SOC: 0.6056 Cumulative_SOC_deviation: 28.9309 Fuel Consumption: 44.8841\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -254.94848021153044 Explore P: 0.0240 SOC: 0.6041 Cumulative_SOC_deviation: 21.0229 Fuel Consumption: 44.7194\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -237.67737324844217 Explore P: 0.0237 SOC: 0.6083 Cumulative_SOC_deviation: 19.2483 Fuel Consumption: 45.1948\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -252.07427385736008 Explore P: 0.0233 SOC: 0.6052 Cumulative_SOC_deviation: 20.7176 Fuel Consumption: 44.8978\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -272.6463493551901 Explore P: 0.0229 SOC: 0.6061 Cumulative_SOC_deviation: 22.7743 Fuel Consumption: 44.9034\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {i}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "for size, history in results_dict.items(): \n",
    "    plt.plot(history, label=size, linewidth=3.0) \n",
    "\n",
    "\n",
    "plt.grid() \n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel(\"episode number\", fontsize=20) \n",
    "plt.ylabel(\"total rewards\", fontsize=20) \n",
    "plt.xlim([0, 120])\n",
    "\n",
    "\n",
    "plt.savefig(\"replay_memory_size_effect_300.png\")\n",
    "with open(\"replay_memory_size_effect_300.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-(3 > 2) * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

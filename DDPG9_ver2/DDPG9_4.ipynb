{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG9_4 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, stack_size, buffer_capacity=100000, batch_size=64): \n",
    "        self.power_mean = 0 \n",
    "        self.power_std = 0\n",
    "        self.sum = 0 \n",
    "        self.sum_deviation = 0 \n",
    "        self.N = 0 \n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, stack_size, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, stack_size, num_states))\n",
    "#         self.terminal_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        self.N += 1 \n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "        power = obs_tuple[0][-1][0] \n",
    "        \n",
    "        self.sum += power \n",
    "        self.power_mean = self.sum / self.N \n",
    "        self.sum_deviation += (power - self.power_mean) ** 2  \n",
    "        self.power_std = np.sqrt(self.sum_deviation / self.N) \n",
    "            \n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "#         self.terminal_buffer[index] = obs_tuple[4]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        state_batch = self.state_buffer[batch_indices]\n",
    "        power_batch = (state_batch[:, :, 0] - self.power_mean) / self.power_std\n",
    "        state_batch[:, :, 0] = power_batch \n",
    "        \n",
    "        next_state_batch = self.next_state_buffer[batch_indices]\n",
    "        power_batch = (next_state_batch[:, :, 0] - self.power_mean) / self.power_std\n",
    "        next_state_batch[:, :, 0] = power_batch \n",
    "#         print(state_batch)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(state_batch)\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(next_state_batch)\n",
    "#         terminal_batch = tf.convert_to_tensor(self.terminal_buffer[batch_indices])\n",
    "#         terminal_batch = tf.cast(terminal_batch, dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(stack_size): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(stack_size, num_states))\n",
    "    inputs_flatten = layers.Flatten()(inputs) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs_flatten)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(stack_size): \n",
    "    state_input = layers.Input(shape=(stack_size, num_states))\n",
    "    state_input_flatten = layers.Flatten()(state_input)\n",
    "    \n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input_flatten)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_states(stacked_states, state, stack_size, is_new_episode): \n",
    "    if is_new_episode: \n",
    "        stacked_states = deque([[0.0] * num_states for _ in range(stack_size)], \n",
    "                               maxlen=stack_size)\n",
    "        for _ in range(stack_size): \n",
    "            stacked_states.append(state)\n",
    "        stacked_array = np.array(stacked_states)\n",
    "    else: \n",
    "        stacked_states.append(state)\n",
    "        stacked_array = np.array(stacked_states)\n",
    "    return stacked_array, stacked_states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-1][-2].numpy()\n",
    "    j_max = state[0][-1][-1].numpy()\n",
    "    \n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "# actor_model = get_actor() \n",
    "# critic_model = get_critic() \n",
    "\n",
    "# target_actor = get_actor() \n",
    "# target_critic = get_critic() \n",
    "# target_actor.set_weights(actor_model.get_weights())\n",
    "# target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(stack_size): \n",
    "    actor_model = get_actor(stack_size) \n",
    "    critic_model = get_critic(stack_size) \n",
    "\n",
    "    target_actor = get_actor(stack_size) \n",
    "    target_critic = get_critic(stack_size) \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(stack_size, 500000, BATCH_SIZE)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 17.994\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -976.062122452242 SOC: 0.7926 Cumulative_SOC_deviation: 91.5783 Fuel Consumption: 60.2796 Power_mean: 2.1068, Power_std: 5.0094\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 19.246\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -900.3248200865742 SOC: 0.7856 Cumulative_SOC_deviation: 84.0427 Fuel Consumption: 59.8976 Power_mean: 2.1068, Power_std: 5.0131\n",
      "WARNING:tensorflow:Layer flatten_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer flatten_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer flatten_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer flatten is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.864\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -770.937371340311 SOC: 0.7503 Cumulative_SOC_deviation: 71.3927 Fuel Consumption: 57.0108 Power_mean: 2.1068, Power_std: 5.0145\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.586\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -756.56296626893 SOC: 0.7296 Cumulative_SOC_deviation: 70.1151 Fuel Consumption: 55.4117 Power_mean: 2.1068, Power_std: 5.0153\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.479\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -739.4116279164733 SOC: 0.7220 Cumulative_SOC_deviation: 68.4572 Fuel Consumption: 54.8393 Power_mean: 2.1068, Power_std: 5.0157\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.629\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -748.9701092689056 SOC: 0.6854 Cumulative_SOC_deviation: 69.7012 Fuel Consumption: 51.9581 Power_mean: 2.1068, Power_std: 5.0161\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.781\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -822.7154698940925 SOC: 0.6854 Cumulative_SOC_deviation: 77.0636 Fuel Consumption: 52.0790 Power_mean: 2.1068, Power_std: 5.0163\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.333\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -779.4572052972924 SOC: 0.6528 Cumulative_SOC_deviation: 72.9951 Fuel Consumption: 49.5060 Power_mean: 2.1068, Power_std: 5.0165\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.203\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -834.6500561914746 SOC: 0.6647 Cumulative_SOC_deviation: 78.4222 Fuel Consumption: 50.4282 Power_mean: 2.1068, Power_std: 5.0167\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.805\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -866.0216894451462 SOC: 0.6409 Cumulative_SOC_deviation: 81.7425 Fuel Consumption: 48.5966 Power_mean: 2.1068, Power_std: 5.0168\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.564\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -1074.647648560604 SOC: 0.5971 Cumulative_SOC_deviation: 102.9403 Fuel Consumption: 45.2447 Power_mean: 2.1068, Power_std: 5.0169\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.104\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -1108.1237622133415 SOC: 0.6134 Cumulative_SOC_deviation: 106.1570 Fuel Consumption: 46.5542 Power_mean: 2.1068, Power_std: 5.0170\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.312\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -1198.9715668396286 SOC: 0.5908 Cumulative_SOC_deviation: 115.4068 Fuel Consumption: 44.9034 Power_mean: 2.1068, Power_std: 5.0171\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.293\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -1159.010582788663 SOC: 0.6014 Cumulative_SOC_deviation: 111.3495 Fuel Consumption: 45.5153 Power_mean: 2.1068, Power_std: 5.0171\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.700\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -1372.586170498171 SOC: 0.5630 Cumulative_SOC_deviation: 132.9739 Fuel Consumption: 42.8471 Power_mean: 2.1068, Power_std: 5.0172\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.965\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -1555.1394345704182 SOC: 0.5274 Cumulative_SOC_deviation: 151.4999 Fuel Consumption: 40.1403 Power_mean: 2.1068, Power_std: 5.0172\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.490\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -1477.799672773281 SOC: 0.5351 Cumulative_SOC_deviation: 143.7080 Fuel Consumption: 40.7202 Power_mean: 2.1068, Power_std: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.446\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -1670.9592602889325 SOC: 0.5094 Cumulative_SOC_deviation: 163.2192 Fuel Consumption: 38.7676 Power_mean: 2.1068, Power_std: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.702\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -2021.744879615561 SOC: 0.4594 Cumulative_SOC_deviation: 198.6776 Fuel Consumption: 34.9685 Power_mean: 2.1068, Power_std: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.531\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -1887.5405557492347 SOC: 0.4867 Cumulative_SOC_deviation: 185.0437 Fuel Consumption: 37.1040 Power_mean: 2.1068, Power_std: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.402\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -2048.453317674635 SOC: 0.4512 Cumulative_SOC_deviation: 201.4144 Fuel Consumption: 34.3095 Power_mean: 2.1068, Power_std: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.305\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -2001.5817287770697 SOC: 0.4636 Cumulative_SOC_deviation: 196.6291 Fuel Consumption: 35.2906 Power_mean: 2.1068, Power_std: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.983\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -1972.3469102467943 SOC: 0.4679 Cumulative_SOC_deviation: 193.6690 Fuel Consumption: 35.6565 Power_mean: 2.1068, Power_std: 5.0175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.052\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -2270.4407903916417 SOC: 0.4274 Cumulative_SOC_deviation: 223.7847 Fuel Consumption: 32.5934 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.106\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -2273.0325014032446 SOC: 0.4190 Cumulative_SOC_deviation: 224.0915 Fuel Consumption: 32.1173 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.378\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -2312.079759029097 SOC: 0.4104 Cumulative_SOC_deviation: 228.0697 Fuel Consumption: 31.3823 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.857\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -2453.1001612334444 SOC: 0.3797 Cumulative_SOC_deviation: 242.3984 Fuel Consumption: 29.1163 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.851\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -2474.9628361408627 SOC: 0.3997 Cumulative_SOC_deviation: 244.4244 Fuel Consumption: 30.7190 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.594\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -2532.8391343243293 SOC: 0.3767 Cumulative_SOC_deviation: 250.3858 Fuel Consumption: 28.9815 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.954\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -2762.970069359121 SOC: 0.3448 Cumulative_SOC_deviation: 273.6154 Fuel Consumption: 26.8161 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.807\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -2806.329429105901 SOC: 0.3564 Cumulative_SOC_deviation: 277.8656 Fuel Consumption: 27.6731 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.729\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -2942.2569322274953 SOC: 0.3275 Cumulative_SOC_deviation: 291.6697 Fuel Consumption: 25.5601 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.797\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -2795.4855363303623 SOC: 0.3461 Cumulative_SOC_deviation: 276.8696 Fuel Consumption: 26.7894 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.951\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -2652.632705504764 SOC: 0.3676 Cumulative_SOC_deviation: 262.4200 Fuel Consumption: 28.4327 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.142\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -3122.730185405007 SOC: 0.2941 Cumulative_SOC_deviation: 309.9673 Fuel Consumption: 23.0576 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.433\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -2875.0194230785482 SOC: 0.3285 Cumulative_SOC_deviation: 284.9483 Fuel Consumption: 25.5365 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.228\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -2997.3603021914228 SOC: 0.3267 Cumulative_SOC_deviation: 297.1863 Fuel Consumption: 25.4969 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.474\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -2818.6050654637984 SOC: 0.3194 Cumulative_SOC_deviation: 279.3752 Fuel Consumption: 24.8529 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.818\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -3138.932879838187 SOC: 0.2782 Cumulative_SOC_deviation: 311.6915 Fuel Consumption: 22.0177 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.804\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -3407.947674717053 SOC: 0.2693 Cumulative_SOC_deviation: 338.6379 Fuel Consumption: 21.5684 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.785\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -3206.1769425264692 SOC: 0.2847 Cumulative_SOC_deviation: 318.3584 Fuel Consumption: 22.5933 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.273\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -3414.2320151819567 SOC: 0.2698 Cumulative_SOC_deviation: 339.2670 Fuel Consumption: 21.5620 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.691\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -3531.3145311404355 SOC: 0.2502 Cumulative_SOC_deviation: 351.1130 Fuel Consumption: 20.1850 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.667\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -3517.2330452850065 SOC: 0.2441 Cumulative_SOC_deviation: 349.7573 Fuel Consumption: 19.6597 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.782\n",
      "Episode: 45 Exploration P: 0.2977 Total reward: -3400.6174519331403 SOC: 0.2638 Cumulative_SOC_deviation: 337.9427 Fuel Consumption: 21.1907 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.880\n",
      "Episode: 46 Exploration P: 0.2899 Total reward: -3534.2507144942106 SOC: 0.2379 Cumulative_SOC_deviation: 351.4857 Fuel Consumption: 19.3933 Power_mean: 2.1068, Power_std: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.323\n",
      "Episode: 47 Exploration P: 0.2824 Total reward: -3727.092866210469 SOC: 0.2210 Cumulative_SOC_deviation: 370.8775 Fuel Consumption: 18.3181 Power_mean: 2.1068, Power_std: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.742\n",
      "Episode: 48 Exploration P: 0.2750 Total reward: -3689.079104886289 SOC: 0.2145 Cumulative_SOC_deviation: 367.1361 Fuel Consumption: 17.7179 Power_mean: 2.1068, Power_std: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.465\n",
      "Episode: 49 Exploration P: 0.2678 Total reward: -3842.9271991925634 SOC: 0.1964 Cumulative_SOC_deviation: 382.6587 Fuel Consumption: 16.3399 Power_mean: 2.1068, Power_std: 5.0178\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 5 \n",
    "stack_size = 1\n",
    "results_dict = {} \n",
    "for trial in range(num_trials): \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer = initialization(stack_size)\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    stacked_states = deque([[0.0] * num_states for _ in range(stack_size)], \n",
    "                            maxlen=stack_size)\n",
    "    for ep in range(total_episodes): \n",
    "        start = time.time() \n",
    "        episodic_reward = 0 \n",
    "        \n",
    "        state = env.reset() \n",
    "        state, stacked_states = stack_states(stacked_states, state, stack_size, True) \n",
    "#         print(state.shape)     \n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "            \n",
    "            next_state, stacked_states = stack_states(stacked_states, next_state, stack_size, \n",
    "                                                        False)\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "              \"Power_mean: {:.4f}, Power_std: {:.4f}\".format(buffer.power_mean, buffer.power_std)\n",
    "        )\n",
    "\n",
    "    \n",
    "    results_dict[trial+1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG9_1.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG9_2 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, stack_size, buffer_capacity=100000, batch_size=64): \n",
    "        self.power_mean = 0 \n",
    "        self.power_std = 0\n",
    "        self.sum = 0 \n",
    "        self.sum_deviation = 0 \n",
    "        self.N = 0 \n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, stack_size, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, stack_size, num_states))\n",
    "#         self.terminal_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        self.N += 1 \n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "        power = obs_tuple[0][-1][0] \n",
    "        \n",
    "        self.sum += power \n",
    "        self.power_mean = self.sum / self.N \n",
    "        self.sum_deviation += (power - self.power_mean) ** 2  \n",
    "        self.power_std = np.sqrt(self.sum_deviation / self.N) \n",
    "            \n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "#         self.terminal_buffer[index] = obs_tuple[4]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        state_batch = self.state_buffer[batch_indices]\n",
    "        power_batch = (state_batch[:, :, 0] - self.power_mean) / self.power_std\n",
    "        state_batch[:, :, 0] = power_batch \n",
    "        \n",
    "        next_state_batch = self.next_state_buffer[batch_indices]\n",
    "        power_batch = (next_state_batch[:, :, 0] - self.power_mean) / self.power_std\n",
    "        next_state_batch[:, :, 0] = power_batch \n",
    "#         print(state_batch)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(state_batch)\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(next_state_batch)\n",
    "#         terminal_batch = tf.convert_to_tensor(self.terminal_buffer[batch_indices])\n",
    "#         terminal_batch = tf.cast(terminal_batch, dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(stack_size): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(stack_size, num_states))\n",
    "    inputs_flatten = layers.Flatten()(inputs) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs_flatten)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(stack_size): \n",
    "    state_input = layers.Input(shape=(stack_size, num_states))\n",
    "    state_input_flatten = layers.Flatten()(state_input)\n",
    "    \n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input_flatten)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_states(stacked_states, state, stack_size, is_new_episode): \n",
    "    if is_new_episode: \n",
    "        stacked_states = deque([[0.0] * num_states for _ in range(stack_size)], \n",
    "                               maxlen=stack_size)\n",
    "        for _ in range(stack_size): \n",
    "            stacked_states.append(state)\n",
    "        stacked_array = np.array(stacked_states)\n",
    "    else: \n",
    "        stacked_states.append(state)\n",
    "        stacked_array = np.array(stacked_states)\n",
    "    return stacked_array, stacked_states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-1][-2].numpy()\n",
    "    j_max = state[0][-1][-1].numpy()\n",
    "    \n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "# actor_model = get_actor() \n",
    "# critic_model = get_critic() \n",
    "\n",
    "# target_actor = get_actor() \n",
    "# target_critic = get_critic() \n",
    "# target_actor.set_weights(actor_model.get_weights())\n",
    "# target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(stack_size): \n",
    "    actor_model = get_actor(stack_size) \n",
    "    critic_model = get_critic(stack_size) \n",
    "\n",
    "    target_actor = get_actor(stack_size) \n",
    "    target_critic = get_critic(stack_size) \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(stack_size, 500000, BATCH_SIZE)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 17.943\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -984.1876666886461 SOC: 0.8027 Cumulative_SOC_deviation: 92.2903 Fuel Consumption: 61.2842 Power_mean: 2.1068, Power_std: 5.0094\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 17.856\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -1032.5221036468565 SOC: 0.8136 Cumulative_SOC_deviation: 97.0564 Fuel Consumption: 61.9582 Power_mean: 2.1068, Power_std: 5.0131\n",
      "WARNING:tensorflow:Layer flatten_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer flatten_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer flatten_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer flatten is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.005\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -830.7991378150219 SOC: 0.7556 Cumulative_SOC_deviation: 77.3313 Fuel Consumption: 57.4860 Power_mean: 2.1068, Power_std: 5.0145\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.426\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -766.7629283675806 SOC: 0.7413 Cumulative_SOC_deviation: 71.0396 Fuel Consumption: 56.3671 Power_mean: 2.1068, Power_std: 5.0153\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.838\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -742.5888437655387 SOC: 0.7057 Cumulative_SOC_deviation: 68.8945 Fuel Consumption: 53.6442 Power_mean: 2.1068, Power_std: 5.0157\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.512\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -816.6669501697614 SOC: 0.7111 Cumulative_SOC_deviation: 76.2605 Fuel Consumption: 54.0615 Power_mean: 2.1068, Power_std: 5.0161\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.264\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -907.5700942682704 SOC: 0.6710 Cumulative_SOC_deviation: 85.6672 Fuel Consumption: 50.8979 Power_mean: 2.1068, Power_std: 5.0163\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.433\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -811.477789962492 SOC: 0.6745 Cumulative_SOC_deviation: 76.0296 Fuel Consumption: 51.1814 Power_mean: 2.1068, Power_std: 5.0165\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.387\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -868.4935728542389 SOC: 0.6333 Cumulative_SOC_deviation: 82.0445 Fuel Consumption: 48.0488 Power_mean: 2.1068, Power_std: 5.0167\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.245\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -921.7107944256961 SOC: 0.6192 Cumulative_SOC_deviation: 87.4635 Fuel Consumption: 47.0763 Power_mean: 2.1068, Power_std: 5.0168\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.048\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -1185.3529441633407 SOC: 0.5818 Cumulative_SOC_deviation: 114.1333 Fuel Consumption: 44.0197 Power_mean: 2.1068, Power_std: 5.0169\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.403\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -1105.1096424325626 SOC: 0.6057 Cumulative_SOC_deviation: 105.9176 Fuel Consumption: 45.9337 Power_mean: 2.1068, Power_std: 5.0170\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.815\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -1100.8677415197496 SOC: 0.6098 Cumulative_SOC_deviation: 105.4656 Fuel Consumption: 46.2118 Power_mean: 2.1068, Power_std: 5.0171\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.335\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -1276.9364305342538 SOC: 0.5597 Cumulative_SOC_deviation: 123.4524 Fuel Consumption: 42.4127 Power_mean: 2.1068, Power_std: 5.0171\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.199\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -1409.9781862989444 SOC: 0.5574 Cumulative_SOC_deviation: 136.7613 Fuel Consumption: 42.3656 Power_mean: 2.1068, Power_std: 5.0172\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.866\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -1600.252588303093 SOC: 0.5133 Cumulative_SOC_deviation: 156.1216 Fuel Consumption: 39.0362 Power_mean: 2.1068, Power_std: 5.0172\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.099\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -1749.9441948810215 SOC: 0.4994 Cumulative_SOC_deviation: 171.2104 Fuel Consumption: 37.8401 Power_mean: 2.1068, Power_std: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.004\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -1677.3444589896842 SOC: 0.5156 Cumulative_SOC_deviation: 163.8099 Fuel Consumption: 39.2459 Power_mean: 2.1068, Power_std: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.568\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -1815.6204407317744 SOC: 0.5051 Cumulative_SOC_deviation: 177.7112 Fuel Consumption: 38.5087 Power_mean: 2.1068, Power_std: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.348\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -1890.5248084854031 SOC: 0.4768 Cumulative_SOC_deviation: 185.4168 Fuel Consumption: 36.3572 Power_mean: 2.1068, Power_std: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.412\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -1923.6991658716415 SOC: 0.4724 Cumulative_SOC_deviation: 188.7601 Fuel Consumption: 36.0983 Power_mean: 2.1068, Power_std: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.173\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -1898.6427142137063 SOC: 0.4731 Cumulative_SOC_deviation: 186.2599 Fuel Consumption: 36.0438 Power_mean: 2.1068, Power_std: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.573\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -2031.9356604852348 SOC: 0.4536 Cumulative_SOC_deviation: 199.7262 Fuel Consumption: 34.6732 Power_mean: 2.1068, Power_std: 5.0175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.390\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -2183.0399809490423 SOC: 0.4404 Cumulative_SOC_deviation: 214.9339 Fuel Consumption: 33.7007 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.379\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -2452.671852496469 SOC: 0.3993 Cumulative_SOC_deviation: 242.2094 Fuel Consumption: 30.5778 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.402\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -2582.5676152340457 SOC: 0.3794 Cumulative_SOC_deviation: 255.3340 Fuel Consumption: 29.2276 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.547\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -2289.8111455242024 SOC: 0.4162 Cumulative_SOC_deviation: 225.8020 Fuel Consumption: 31.7910 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.831\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -2705.681497130961 SOC: 0.3641 Cumulative_SOC_deviation: 267.7571 Fuel Consumption: 28.1106 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.922\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -2848.9355013052227 SOC: 0.3480 Cumulative_SOC_deviation: 282.2091 Fuel Consumption: 26.8450 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.107\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -2646.7088729938723 SOC: 0.3778 Cumulative_SOC_deviation: 261.7565 Fuel Consumption: 29.1441 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.078\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -2488.2387203997714 SOC: 0.3724 Cumulative_SOC_deviation: 245.9528 Fuel Consumption: 28.7108 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.768\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -2774.7529949949553 SOC: 0.3450 Cumulative_SOC_deviation: 274.8110 Fuel Consumption: 26.6428 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.049\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -2927.024127612256 SOC: 0.3341 Cumulative_SOC_deviation: 290.0906 Fuel Consumption: 26.1185 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.774\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -2732.672203100955 SOC: 0.3519 Cumulative_SOC_deviation: 270.5478 Fuel Consumption: 27.1938 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.376\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -2920.8844710469198 SOC: 0.3443 Cumulative_SOC_deviation: 289.4159 Fuel Consumption: 26.7252 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.973\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -3136.385493832395 SOC: 0.2938 Cumulative_SOC_deviation: 311.3299 Fuel Consumption: 23.0865 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.667\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -3019.199118671648 SOC: 0.3133 Cumulative_SOC_deviation: 299.4574 Fuel Consumption: 24.6250 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.635\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -3121.0236599816103 SOC: 0.3005 Cumulative_SOC_deviation: 309.7405 Fuel Consumption: 23.6183 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.928\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -3195.3123807368515 SOC: 0.2915 Cumulative_SOC_deviation: 317.2212 Fuel Consumption: 23.1004 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.023\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -3274.9894410062366 SOC: 0.2878 Cumulative_SOC_deviation: 325.2105 Fuel Consumption: 22.8843 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.384\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -3062.225067663679 SOC: 0.2980 Cumulative_SOC_deviation: 303.8714 Fuel Consumption: 23.5113 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.495\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -3154.6681525836293 SOC: 0.2812 Cumulative_SOC_deviation: 313.2473 Fuel Consumption: 22.1953 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.165\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -3431.1093305360546 SOC: 0.2693 Cumulative_SOC_deviation: 340.9581 Fuel Consumption: 21.5288 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.523\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -3475.414120519924 SOC: 0.2475 Cumulative_SOC_deviation: 345.5382 Fuel Consumption: 20.0320 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.648\n",
      "Episode: 45 Exploration P: 0.2977 Total reward: -3600.5291338558454 SOC: 0.2299 Cumulative_SOC_deviation: 358.1793 Fuel Consumption: 18.7364 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.276\n",
      "Episode: 46 Exploration P: 0.2899 Total reward: -3631.106338891851 SOC: 0.2241 Cumulative_SOC_deviation: 361.2793 Fuel Consumption: 18.3138 Power_mean: 2.1068, Power_std: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.306\n",
      "Episode: 47 Exploration P: 0.2824 Total reward: -3551.5966129640306 SOC: 0.2338 Cumulative_SOC_deviation: 353.2468 Fuel Consumption: 19.1291 Power_mean: 2.1068, Power_std: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.554\n",
      "Episode: 48 Exploration P: 0.2750 Total reward: -3613.1471578943206 SOC: 0.2213 Cumulative_SOC_deviation: 359.5092 Fuel Consumption: 18.0549 Power_mean: 2.1068, Power_std: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.485\n",
      "Episode: 49 Exploration P: 0.2678 Total reward: -3508.4865150430824 SOC: 0.2361 Cumulative_SOC_deviation: 348.9244 Fuel Consumption: 19.2425 Power_mean: 2.1068, Power_std: 5.0178\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 5 \n",
    "stack_size = 1\n",
    "results_dict = {} \n",
    "for trial in range(num_trials): \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer = initialization(stack_size)\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    stacked_states = deque([[0.0] * num_states for _ in range(stack_size)], \n",
    "                            maxlen=stack_size)\n",
    "    for ep in range(total_episodes): \n",
    "        start = time.time() \n",
    "        episodic_reward = 0 \n",
    "        \n",
    "        state = env.reset() \n",
    "        state, stacked_states = stack_states(stacked_states, state, stack_size, True) \n",
    "#         print(state.shape)     \n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "            \n",
    "            next_state, stacked_states = stack_states(stacked_states, next_state, stack_size, \n",
    "                                                        False)\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "              \"Power_mean: {:.4f}, Power_std: {:.4f}\".format(buffer.power_mean, buffer.power_std)\n",
    "        )\n",
    "\n",
    "    \n",
    "    results_dict[trial+1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG9_1.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG9_1 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, stack_size, buffer_capacity=100000, batch_size=64): \n",
    "        self.power_mean = 0 \n",
    "        self.power_std = 0\n",
    "        self.sum = 0 \n",
    "        self.sum_deviation = 0 \n",
    "        self.N = 0 \n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, stack_size, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, stack_size, num_states))\n",
    "#         self.terminal_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        self.N += 1 \n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "        power = obs_tuple[0][-1][0] \n",
    "        \n",
    "        self.sum += power \n",
    "        self.power_mean = self.sum / self.N \n",
    "        self.sum_deviation += (power - self.power_mean) ** 2  \n",
    "        self.power_std = np.sqrt(self.sum_deviation / self.N) \n",
    "            \n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "#         self.terminal_buffer[index] = obs_tuple[4]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        state_batch = self.state_buffer[batch_indices]\n",
    "        power_batch = (state_batch[:, :, 0] - self.power_mean) / self.power_std\n",
    "        state_batch[:, :, 0] = power_batch \n",
    "        \n",
    "        next_state_batch = self.next_state_buffer[batch_indices]\n",
    "        power_batch = (next_state_batch[:, :, 0] - self.power_mean) / self.power_std\n",
    "        next_state_batch[:, :, 0] = power_batch \n",
    "#         print(state_batch)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(state_batch)\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(next_state_batch)\n",
    "#         terminal_batch = tf.convert_to_tensor(self.terminal_buffer[batch_indices])\n",
    "#         terminal_batch = tf.cast(terminal_batch, dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(stack_size): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(stack_size, num_states))\n",
    "    inputs_flatten = layers.Flatten()(inputs) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs_flatten)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(stack_size): \n",
    "    state_input = layers.Input(shape=(stack_size, num_states))\n",
    "    state_input_flatten = layers.Flatten()(state_input)\n",
    "    \n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input_flatten)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_states(stacked_states, state, stack_size, is_new_episode): \n",
    "    if is_new_episode: \n",
    "        stacked_states = deque([[0.0] * num_states for _ in range(stack_size)], \n",
    "                               maxlen=stack_size)\n",
    "        for _ in range(stack_size): \n",
    "            stacked_states.append(state)\n",
    "        stacked_array = np.array(stacked_states)\n",
    "    else: \n",
    "        stacked_states.append(state)\n",
    "        stacked_array = np.array(stacked_states)\n",
    "    return stacked_array, stacked_states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-1][-2].numpy()\n",
    "    j_max = state[0][-1][-1].numpy()\n",
    "    \n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "# actor_model = get_actor() \n",
    "# critic_model = get_critic() \n",
    "\n",
    "# target_actor = get_actor() \n",
    "# target_critic = get_critic() \n",
    "# target_actor.set_weights(actor_model.get_weights())\n",
    "# target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "gamma = 0.0 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(stack_size): \n",
    "    actor_model = get_actor(stack_size) \n",
    "    critic_model = get_critic(stack_size) \n",
    "\n",
    "    target_actor = get_actor(stack_size) \n",
    "    target_critic = get_critic(stack_size) \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(stack_size, 500000, BATCH_SIZE)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 18.016\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -1002.0178592063007 SOC: 0.8036 Cumulative_SOC_deviation: 94.0756 Fuel Consumption: 61.2617 Power_mean: 2.1068, Power_std: 5.0094\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 17.698\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -1010.086383595006 SOC: 0.8088 Cumulative_SOC_deviation: 94.8367 Fuel Consumption: 61.7196 Power_mean: 2.1068, Power_std: 5.0131\n",
      "WARNING:tensorflow:Layer flatten_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer flatten_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer flatten_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer flatten is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 64.772\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -848.8920172542698 SOC: 0.7643 Cumulative_SOC_deviation: 79.0721 Fuel Consumption: 58.1711 Power_mean: 2.1068, Power_std: 5.0145\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.772\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -856.2123779013352 SOC: 0.7571 Cumulative_SOC_deviation: 79.8450 Fuel Consumption: 57.7625 Power_mean: 2.1068, Power_std: 5.0153\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.453\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -758.1056807765585 SOC: 0.7294 Cumulative_SOC_deviation: 70.2718 Fuel Consumption: 55.3879 Power_mean: 2.1068, Power_std: 5.0157\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.032\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -826.7304259145566 SOC: 0.7199 Cumulative_SOC_deviation: 77.2192 Fuel Consumption: 54.5383 Power_mean: 2.1068, Power_std: 5.0161\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.652\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -786.6893445111389 SOC: 0.7231 Cumulative_SOC_deviation: 73.2057 Fuel Consumption: 54.6328 Power_mean: 2.1068, Power_std: 5.0163\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.989\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -718.2070978964222 SOC: 0.7272 Cumulative_SOC_deviation: 66.3152 Fuel Consumption: 55.0552 Power_mean: 2.1068, Power_std: 5.0165\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.743\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -717.3784914777773 SOC: 0.7355 Cumulative_SOC_deviation: 66.1723 Fuel Consumption: 55.6551 Power_mean: 2.1068, Power_std: 5.0167\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.926\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -728.1984961231165 SOC: 0.7364 Cumulative_SOC_deviation: 67.2296 Fuel Consumption: 55.9022 Power_mean: 2.1068, Power_std: 5.0168\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.859\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -655.8031897538367 SOC: 0.7161 Cumulative_SOC_deviation: 60.1734 Fuel Consumption: 54.0688 Power_mean: 2.1068, Power_std: 5.0169\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.709\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -706.6919026802723 SOC: 0.7214 Cumulative_SOC_deviation: 65.2323 Fuel Consumption: 54.3693 Power_mean: 2.1068, Power_std: 5.0170\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.747\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -690.5961944787159 SOC: 0.7178 Cumulative_SOC_deviation: 63.6138 Fuel Consumption: 54.4584 Power_mean: 2.1068, Power_std: 5.0171\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.852\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -666.3487925576187 SOC: 0.7102 Cumulative_SOC_deviation: 61.2780 Fuel Consumption: 53.5690 Power_mean: 2.1068, Power_std: 5.0171\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.682\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -714.1244164101805 SOC: 0.7143 Cumulative_SOC_deviation: 66.0125 Fuel Consumption: 53.9994 Power_mean: 2.1068, Power_std: 5.0172\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.499\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -575.0727843070022 SOC: 0.6823 Cumulative_SOC_deviation: 52.3616 Fuel Consumption: 51.4564 Power_mean: 2.1068, Power_std: 5.0172\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.873\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -600.7247028700343 SOC: 0.6972 Cumulative_SOC_deviation: 54.8074 Fuel Consumption: 52.6504 Power_mean: 2.1068, Power_std: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.731\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -606.5689961499239 SOC: 0.6953 Cumulative_SOC_deviation: 55.4004 Fuel Consumption: 52.5649 Power_mean: 2.1068, Power_std: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.158\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -569.9390167477587 SOC: 0.6852 Cumulative_SOC_deviation: 51.8105 Fuel Consumption: 51.8341 Power_mean: 2.1068, Power_std: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.038\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -597.0195005862375 SOC: 0.6952 Cumulative_SOC_deviation: 54.4421 Fuel Consumption: 52.5989 Power_mean: 2.1068, Power_std: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.870\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -617.5962686473163 SOC: 0.6927 Cumulative_SOC_deviation: 56.5208 Fuel Consumption: 52.3885 Power_mean: 2.1068, Power_std: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.798\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -570.3338717532564 SOC: 0.6761 Cumulative_SOC_deviation: 51.9229 Fuel Consumption: 51.1051 Power_mean: 2.1068, Power_std: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.352\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -510.2147247699965 SOC: 0.6772 Cumulative_SOC_deviation: 45.9071 Fuel Consumption: 51.1438 Power_mean: 2.1068, Power_std: 5.0175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.452\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -525.2870873746907 SOC: 0.6659 Cumulative_SOC_deviation: 47.5047 Fuel Consumption: 50.2406 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.721\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -575.3260303094206 SOC: 0.6803 Cumulative_SOC_deviation: 52.3897 Fuel Consumption: 51.4286 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.662\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -470.28849366085393 SOC: 0.6627 Cumulative_SOC_deviation: 42.0434 Fuel Consumption: 49.8544 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.321\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -475.0343235422195 SOC: 0.6570 Cumulative_SOC_deviation: 42.5552 Fuel Consumption: 49.4820 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.568\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -443.17355196505815 SOC: 0.6439 Cumulative_SOC_deviation: 39.4792 Fuel Consumption: 48.3815 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.550\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -475.02188681490514 SOC: 0.6637 Cumulative_SOC_deviation: 42.4939 Fuel Consumption: 50.0829 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.329\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -452.66536954123427 SOC: 0.6582 Cumulative_SOC_deviation: 40.3014 Fuel Consumption: 49.6514 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.633\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -450.47418761087033 SOC: 0.6489 Cumulative_SOC_deviation: 40.1599 Fuel Consumption: 48.8750 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.271\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -430.10153873673045 SOC: 0.6541 Cumulative_SOC_deviation: 38.0964 Fuel Consumption: 49.1372 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.572\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -456.8421458280776 SOC: 0.6513 Cumulative_SOC_deviation: 40.7837 Fuel Consumption: 49.0052 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.789\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -421.1890289237576 SOC: 0.6539 Cumulative_SOC_deviation: 37.2067 Fuel Consumption: 49.1217 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.023\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -429.41881974826373 SOC: 0.6481 Cumulative_SOC_deviation: 38.0839 Fuel Consumption: 48.5803 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.218\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -411.3885531457799 SOC: 0.6366 Cumulative_SOC_deviation: 36.3652 Fuel Consumption: 47.7361 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.027\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -405.3572908108013 SOC: 0.6417 Cumulative_SOC_deviation: 35.7203 Fuel Consumption: 48.1547 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.421\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -428.1730519600053 SOC: 0.6497 Cumulative_SOC_deviation: 37.9358 Fuel Consumption: 48.8147 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.512\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -412.8352882473998 SOC: 0.6371 Cumulative_SOC_deviation: 36.4926 Fuel Consumption: 47.9088 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.725\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -423.7829428771375 SOC: 0.6494 Cumulative_SOC_deviation: 37.4930 Fuel Consumption: 48.8533 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.060\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -396.8596325925336 SOC: 0.6354 Cumulative_SOC_deviation: 34.9194 Fuel Consumption: 47.6661 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.689\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -420.7812521822289 SOC: 0.6348 Cumulative_SOC_deviation: 37.2890 Fuel Consumption: 47.8908 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.558\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -405.646575003557 SOC: 0.6339 Cumulative_SOC_deviation: 35.8000 Fuel Consumption: 47.6465 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.073\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -400.648744064013 SOC: 0.6274 Cumulative_SOC_deviation: 35.3310 Fuel Consumption: 47.3387 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.539\n",
      "Episode: 45 Exploration P: 0.2977 Total reward: -356.7542922815839 SOC: 0.6300 Cumulative_SOC_deviation: 30.9320 Fuel Consumption: 47.4341 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.532\n",
      "Episode: 46 Exploration P: 0.2899 Total reward: -371.8717561838077 SOC: 0.6266 Cumulative_SOC_deviation: 32.4630 Fuel Consumption: 47.2422 Power_mean: 2.1068, Power_std: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.625\n",
      "Episode: 47 Exploration P: 0.2824 Total reward: -363.9772277704775 SOC: 0.6261 Cumulative_SOC_deviation: 31.6854 Fuel Consumption: 47.1237 Power_mean: 2.1068, Power_std: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.130\n",
      "Episode: 48 Exploration P: 0.2750 Total reward: -363.3853560743067 SOC: 0.6254 Cumulative_SOC_deviation: 31.6342 Fuel Consumption: 47.0434 Power_mean: 2.1068, Power_std: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.113\n",
      "Episode: 49 Exploration P: 0.2678 Total reward: -341.13234377818117 SOC: 0.6266 Cumulative_SOC_deviation: 29.4129 Fuel Consumption: 47.0029 Power_mean: 2.1068, Power_std: 5.0178\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 5 \n",
    "stack_size = 1 \n",
    "results_dict = {} \n",
    "for trial in range(num_trials): \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer = initialization(stack_size)\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    stacked_states = deque([[0.0] * num_states for _ in range(stack_size)], \n",
    "                            maxlen=stack_size)\n",
    "    for ep in range(total_episodes): \n",
    "        start = time.time() \n",
    "        episodic_reward = 0 \n",
    "        \n",
    "        state = env.reset() \n",
    "        state, stacked_states = stack_states(stacked_states, state, stack_size, True) \n",
    "#         print(state.shape)     \n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "            \n",
    "            next_state, stacked_states = stack_states(stacked_states, next_state, stack_size, \n",
    "                                                        False)\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "              \"Power_mean: {:.4f}, Power_std: {:.4f}\".format(buffer.power_mean, buffer.power_std)\n",
    "        )\n",
    "\n",
    "    \n",
    "    results_dict[trial+1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG9_1.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

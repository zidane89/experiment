{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG9_3 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, stack_size, buffer_capacity=100000, batch_size=64): \n",
    "        self.power_mean = 0 \n",
    "        self.power_std = 0\n",
    "        self.sum = 0 \n",
    "        self.sum_deviation = 0 \n",
    "        self.N = 0 \n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, stack_size, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, stack_size, num_states))\n",
    "#         self.terminal_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        self.N += 1 \n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "        power = obs_tuple[0][-1][0] \n",
    "        \n",
    "        self.sum += power \n",
    "        self.power_mean = self.sum / self.N \n",
    "        self.sum_deviation += (power - self.power_mean) ** 2  \n",
    "        self.power_std = np.sqrt(self.sum_deviation / self.N) \n",
    "            \n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "#         self.terminal_buffer[index] = obs_tuple[4]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        state_batch = self.state_buffer[batch_indices]\n",
    "        power_batch = (state_batch[:, :, 0] - self.power_mean) / self.power_std\n",
    "        state_batch[:, :, 0] = power_batch \n",
    "        \n",
    "        next_state_batch = self.next_state_buffer[batch_indices]\n",
    "        power_batch = (next_state_batch[:, :, 0] - self.power_mean) / self.power_std\n",
    "        next_state_batch[:, :, 0] = power_batch \n",
    "#         print(state_batch)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(state_batch)\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(next_state_batch)\n",
    "#         terminal_batch = tf.convert_to_tensor(self.terminal_buffer[batch_indices])\n",
    "#         terminal_batch = tf.cast(terminal_batch, dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(stack_size): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(stack_size, num_states))\n",
    "    inputs_flatten = layers.Flatten()(inputs) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs_flatten)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(stack_size): \n",
    "    state_input = layers.Input(shape=(stack_size, num_states))\n",
    "    state_input_flatten = layers.Flatten()(state_input)\n",
    "    \n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input_flatten)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_states(stacked_states, state, stack_size, is_new_episode): \n",
    "    if is_new_episode: \n",
    "        stacked_states = deque([[0.0] * num_states for _ in range(stack_size)], \n",
    "                               maxlen=stack_size)\n",
    "        for _ in range(stack_size): \n",
    "            stacked_states.append(state)\n",
    "        stacked_array = np.array(stacked_states)\n",
    "    else: \n",
    "        stacked_states.append(state)\n",
    "        stacked_array = np.array(stacked_states)\n",
    "    return stacked_array, stacked_states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-1][-2].numpy()\n",
    "    j_max = state[0][-1][-1].numpy()\n",
    "    \n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "# actor_model = get_actor() \n",
    "# critic_model = get_critic() \n",
    "\n",
    "# target_actor = get_actor() \n",
    "# target_critic = get_critic() \n",
    "# target_actor.set_weights(actor_model.get_weights())\n",
    "# target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "gamma = 0.6 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(stack_size): \n",
    "    actor_model = get_actor(stack_size) \n",
    "    critic_model = get_critic(stack_size) \n",
    "\n",
    "    target_actor = get_actor(stack_size) \n",
    "    target_critic = get_critic(stack_size) \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(stack_size, 500000, BATCH_SIZE)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 18.000\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -1023.4788371839971 SOC: 0.8228 Cumulative_SOC_deviation: 96.0689 Fuel Consumption: 62.7895 Power_mean: 2.1068, Power_std: 5.0094\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 17.200\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -974.4543614931312 SOC: 0.7969 Cumulative_SOC_deviation: 91.3590 Fuel Consumption: 60.8648 Power_mean: 2.1068, Power_std: 5.0131\n",
      "WARNING:tensorflow:Layer flatten_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer flatten_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer flatten_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer flatten is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.534\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -851.870933233489 SOC: 0.7715 Cumulative_SOC_deviation: 79.3016 Fuel Consumption: 58.8549 Power_mean: 2.1068, Power_std: 5.0145\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.658\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -688.412513706853 SOC: 0.7193 Cumulative_SOC_deviation: 63.3875 Fuel Consumption: 54.5376 Power_mean: 2.1068, Power_std: 5.0153\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.237\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -785.8248608437523 SOC: 0.7083 Cumulative_SOC_deviation: 73.1866 Fuel Consumption: 53.9588 Power_mean: 2.1068, Power_std: 5.0157\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.789\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -811.7695973976419 SOC: 0.6756 Cumulative_SOC_deviation: 76.0681 Fuel Consumption: 51.0883 Power_mean: 2.1068, Power_std: 5.0161\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.802\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -815.4949413685304 SOC: 0.6571 Cumulative_SOC_deviation: 76.5592 Fuel Consumption: 49.9029 Power_mean: 2.1068, Power_std: 5.0163\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.611\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -776.0112839683638 SOC: 0.6675 Cumulative_SOC_deviation: 72.5228 Fuel Consumption: 50.7834 Power_mean: 2.1068, Power_std: 5.0165\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.357\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -797.7212536047537 SOC: 0.6783 Cumulative_SOC_deviation: 74.6099 Fuel Consumption: 51.6222 Power_mean: 2.1068, Power_std: 5.0167\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.939\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -927.481573877535 SOC: 0.6347 Cumulative_SOC_deviation: 87.9315 Fuel Consumption: 48.1665 Power_mean: 2.1068, Power_std: 5.0168\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.537\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -945.0544038490422 SOC: 0.6099 Cumulative_SOC_deviation: 89.8800 Fuel Consumption: 46.2546 Power_mean: 2.1068, Power_std: 5.0169\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.299\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -1296.8891392989422 SOC: 0.5745 Cumulative_SOC_deviation: 125.3266 Fuel Consumption: 43.6227 Power_mean: 2.1068, Power_std: 5.0170\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.421\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -1176.6197885568558 SOC: 0.5904 Cumulative_SOC_deviation: 113.1782 Fuel Consumption: 44.8381 Power_mean: 2.1068, Power_std: 5.0171\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.580\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -1252.3543351224055 SOC: 0.5687 Cumulative_SOC_deviation: 120.9116 Fuel Consumption: 43.2386 Power_mean: 2.1068, Power_std: 5.0171\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.273\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -1318.8985712902165 SOC: 0.5535 Cumulative_SOC_deviation: 127.6845 Fuel Consumption: 42.0532 Power_mean: 2.1068, Power_std: 5.0172\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.288\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -1630.647536797346 SOC: 0.5449 Cumulative_SOC_deviation: 158.9275 Fuel Consumption: 41.3728 Power_mean: 2.1068, Power_std: 5.0172\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.596\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -1497.5895158236387 SOC: 0.5479 Cumulative_SOC_deviation: 145.5833 Fuel Consumption: 41.7569 Power_mean: 2.1068, Power_std: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.346\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -1646.3117840704483 SOC: 0.5124 Cumulative_SOC_deviation: 160.7359 Fuel Consumption: 38.9527 Power_mean: 2.1068, Power_std: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.039\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -1999.4597508888714 SOC: 0.4644 Cumulative_SOC_deviation: 196.4224 Fuel Consumption: 35.2360 Power_mean: 2.1068, Power_std: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.760\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -1905.0105802758414 SOC: 0.4704 Cumulative_SOC_deviation: 186.9232 Fuel Consumption: 35.7784 Power_mean: 2.1068, Power_std: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.407\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -1995.1892447245755 SOC: 0.4771 Cumulative_SOC_deviation: 195.8789 Fuel Consumption: 36.4000 Power_mean: 2.1068, Power_std: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.432\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -1902.9430063961838 SOC: 0.4928 Cumulative_SOC_deviation: 186.5314 Fuel Consumption: 37.6293 Power_mean: 2.1068, Power_std: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.936\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -2189.658743225593 SOC: 0.4302 Cumulative_SOC_deviation: 215.6952 Fuel Consumption: 32.7068 Power_mean: 2.1068, Power_std: 5.0175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.336\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -2206.0863831167712 SOC: 0.4238 Cumulative_SOC_deviation: 217.3767 Fuel Consumption: 32.3195 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.510\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -2239.572625820027 SOC: 0.4297 Cumulative_SOC_deviation: 220.6678 Fuel Consumption: 32.8951 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.397\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -2346.35444189844 SOC: 0.4210 Cumulative_SOC_deviation: 231.4055 Fuel Consumption: 32.2992 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.930\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -2587.1619662638573 SOC: 0.3860 Cumulative_SOC_deviation: 255.7465 Fuel Consumption: 29.6973 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.279\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -2571.0502770357393 SOC: 0.3832 Cumulative_SOC_deviation: 254.1443 Fuel Consumption: 29.6074 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.241\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -2576.387831663386 SOC: 0.3750 Cumulative_SOC_deviation: 254.7452 Fuel Consumption: 28.9355 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.181\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -2772.685784156478 SOC: 0.3609 Cumulative_SOC_deviation: 274.4836 Fuel Consumption: 27.8496 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.381\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -2844.680750329626 SOC: 0.3555 Cumulative_SOC_deviation: 281.6975 Fuel Consumption: 27.7062 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.382\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -2656.071615756113 SOC: 0.3613 Cumulative_SOC_deviation: 262.8182 Fuel Consumption: 27.8892 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.174\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -2932.1712346398967 SOC: 0.3212 Cumulative_SOC_deviation: 290.7179 Fuel Consumption: 24.9920 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.648\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -2938.2731216699945 SOC: 0.3149 Cumulative_SOC_deviation: 291.3753 Fuel Consumption: 24.5202 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.691\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -2929.323001919516 SOC: 0.3205 Cumulative_SOC_deviation: 290.4298 Fuel Consumption: 25.0251 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.893\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -3035.605569270404 SOC: 0.3093 Cumulative_SOC_deviation: 301.1330 Fuel Consumption: 24.2752 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.765\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -3197.6724077950284 SOC: 0.2954 Cumulative_SOC_deviation: 317.4312 Fuel Consumption: 23.3604 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.536\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -3161.35930640554 SOC: 0.2951 Cumulative_SOC_deviation: 313.8216 Fuel Consumption: 23.1432 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.963\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -3150.8782483408413 SOC: 0.3065 Cumulative_SOC_deviation: 312.6766 Fuel Consumption: 24.1125 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.462\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -3314.0662579336463 SOC: 0.2822 Cumulative_SOC_deviation: 329.1804 Fuel Consumption: 22.2627 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.579\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -3382.8858871969246 SOC: 0.2588 Cumulative_SOC_deviation: 336.2181 Fuel Consumption: 20.7050 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.368\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -3196.4272347127994 SOC: 0.2712 Cumulative_SOC_deviation: 317.4887 Fuel Consumption: 21.5406 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.518\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -3366.2247164192268 SOC: 0.2728 Cumulative_SOC_deviation: 334.4403 Fuel Consumption: 21.8219 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.987\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -3531.3560083215084 SOC: 0.2446 Cumulative_SOC_deviation: 351.1503 Fuel Consumption: 19.8534 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.135\n",
      "Episode: 45 Exploration P: 0.2977 Total reward: -3474.9191104770025 SOC: 0.2426 Cumulative_SOC_deviation: 345.5283 Fuel Consumption: 19.6362 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.347\n",
      "Episode: 46 Exploration P: 0.2899 Total reward: -3470.157490202788 SOC: 0.2321 Cumulative_SOC_deviation: 345.1234 Fuel Consumption: 18.9237 Power_mean: 2.1068, Power_std: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.251\n",
      "Episode: 47 Exploration P: 0.2824 Total reward: -3533.4606443202624 SOC: 0.2284 Cumulative_SOC_deviation: 351.4860 Fuel Consumption: 18.6005 Power_mean: 2.1068, Power_std: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.047\n",
      "Episode: 48 Exploration P: 0.2750 Total reward: -3808.0588789492454 SOC: 0.2001 Cumulative_SOC_deviation: 379.1276 Fuel Consumption: 16.7828 Power_mean: 2.1068, Power_std: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.779\n",
      "Episode: 49 Exploration P: 0.2678 Total reward: -3526.633633345827 SOC: 0.2318 Cumulative_SOC_deviation: 350.7939 Fuel Consumption: 18.6947 Power_mean: 2.1068, Power_std: 5.0178\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 5 \n",
    "stack_size = 1\n",
    "results_dict = {} \n",
    "for trial in range(num_trials): \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer = initialization(stack_size)\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    stacked_states = deque([[0.0] * num_states for _ in range(stack_size)], \n",
    "                            maxlen=stack_size)\n",
    "    for ep in range(total_episodes): \n",
    "        start = time.time() \n",
    "        episodic_reward = 0 \n",
    "        \n",
    "        state = env.reset() \n",
    "        state, stacked_states = stack_states(stacked_states, state, stack_size, True) \n",
    "#         print(state.shape)     \n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "            \n",
    "            next_state, stacked_states = stack_states(stacked_states, next_state, stack_size, \n",
    "                                                        False)\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "              \"Power_mean: {:.4f}, Power_std: {:.4f}\".format(buffer.power_mean, buffer.power_std)\n",
    "        )\n",
    "\n",
    "    \n",
    "    results_dict[trial+1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG9_1.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

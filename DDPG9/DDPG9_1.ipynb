{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG9_1 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, stack_size, buffer_capacity=100000, batch_size=64): \n",
    "        self.power_mean = 0 \n",
    "        self.power_std = 0\n",
    "        self.sum = 0 \n",
    "        self.sum_deviation = 0 \n",
    "        self.N = 0 \n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, stack_size, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, stack_size, num_states))\n",
    "        self.terminal_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        self.N += 1 \n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "        power = obs_tuple[0][-1][0] \n",
    "        \n",
    "        self.sum += power \n",
    "        self.power_mean = self.sum / self.N \n",
    "        self.sum_deviation += (power - self.power_mean) ** 2  \n",
    "        self.power_std = np.sqrt(self.sum_deviation / self.N) \n",
    "            \n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.terminal_buffer[index] = obs_tuple[4]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        state_batch = self.state_buffer[batch_indices]\n",
    "        power_batch = (state_batch[:, :, 0] - self.power_mean) / self.power_std\n",
    "        state_batch[:, :, 0] = power_batch \n",
    "        \n",
    "        next_state_batch = self.next_state_buffer[batch_indices]\n",
    "        power_batch = (next_state_batch[:, :, 0] - self.power_mean) / self.power_std\n",
    "        next_state_batch[:, :, 0] = power_batch \n",
    "#         print(state_batch)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(state_batch)\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(next_state_batch)\n",
    "        terminal_batch = tf.convert_to_tensor(self.terminal_buffer[batch_indices])\n",
    "        terminal_batch = tf.cast(terminal_batch, dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = (\n",
    "                reward_batch + gamma * (1 - terminal_batch) * \n",
    "                 target_critic([next_state_batch, target_actions])\n",
    "                )\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(stack_size): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(stack_size, num_states))\n",
    "    inputs_flatten = layers.Flatten()(inputs) \n",
    "    \n",
    "    out = layers.Dense(32, activation=\"relu\")(inputs_flatten)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(32, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(stack_size): \n",
    "    state_input = layers.Input(shape=(stack_size, num_states))\n",
    "    state_input_flatten = layers.Flatten()(state_input)\n",
    "    \n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input_flatten)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_out)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(16, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(16, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(16, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_states(stacked_states, state, stack_size, is_new_episode): \n",
    "    if is_new_episode: \n",
    "        stacked_states = deque([[0.0] * num_states for _ in range(stack_size)], \n",
    "                               maxlen=stack_size)\n",
    "        for _ in range(stack_size): \n",
    "            stacked_states.append(state)\n",
    "        stacked_array = np.array(stacked_states)\n",
    "    else: \n",
    "        stacked_states.append(state)\n",
    "        stacked_array = np.array(stacked_states)\n",
    "    return stacked_array, stacked_states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-1][-2].numpy()\n",
    "    j_max = state[0][-1][-1].numpy()\n",
    "    \n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "# actor_model = get_actor() \n",
    "# critic_model = get_critic() \n",
    "\n",
    "# target_actor = get_actor() \n",
    "# target_critic = get_critic() \n",
    "# target_actor.set_weights(actor_model.get_weights())\n",
    "# target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(stack_size): \n",
    "    actor_model = get_actor(stack_size) \n",
    "    critic_model = get_critic(stack_size) \n",
    "\n",
    "    target_actor = get_actor(stack_size) \n",
    "    target_critic = get_critic(stack_size) \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(stack_size, 500000, BATCH_SIZE)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 16.833\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -1095.7608785021398 SOC: 0.8217 Cumulative_SOC_deviation: 103.2888 Fuel Consumption: 62.8730 Power_mean: 2.1068, Power_std: 5.0094\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 16.962\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -1054.6984289798772 SOC: 0.8184 Cumulative_SOC_deviation: 99.2279 Fuel Consumption: 62.4193 Power_mean: 2.1068, Power_std: 5.0131\n",
      "WARNING:tensorflow:Layer flatten_24 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer flatten_25 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer flatten_23 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer flatten_22 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 45.484\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -830.8425320880091 SOC: 0.7673 Cumulative_SOC_deviation: 77.2502 Fuel Consumption: 58.3403 Power_mean: 2.1068, Power_std: 5.0145\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.709\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -759.4369910483058 SOC: 0.7404 Cumulative_SOC_deviation: 70.3177 Fuel Consumption: 56.2601 Power_mean: 2.1068, Power_std: 5.0153\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.846\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -784.1681197008368 SOC: 0.7403 Cumulative_SOC_deviation: 72.7646 Fuel Consumption: 56.5222 Power_mean: 2.1068, Power_std: 5.0157\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.995\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -789.7285426468965 SOC: 0.6935 Cumulative_SOC_deviation: 73.7026 Fuel Consumption: 52.7027 Power_mean: 2.1068, Power_std: 5.0161\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.716\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -762.4442318916796 SOC: 0.7091 Cumulative_SOC_deviation: 70.8528 Fuel Consumption: 53.9160 Power_mean: 2.1068, Power_std: 5.0163\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.715\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -889.6940323635431 SOC: 0.6635 Cumulative_SOC_deviation: 83.9231 Fuel Consumption: 50.4635 Power_mean: 2.1068, Power_std: 5.0165\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.621\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -873.9119809984312 SOC: 0.6497 Cumulative_SOC_deviation: 82.4631 Fuel Consumption: 49.2814 Power_mean: 2.1068, Power_std: 5.0167\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.039\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -835.6808032771463 SOC: 0.6257 Cumulative_SOC_deviation: 78.8483 Fuel Consumption: 47.1976 Power_mean: 2.1068, Power_std: 5.0168\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.570\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -968.3953861679988 SOC: 0.6198 Cumulative_SOC_deviation: 92.1386 Fuel Consumption: 47.0091 Power_mean: 2.1068, Power_std: 5.0169\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.330\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -932.7758774977078 SOC: 0.6301 Cumulative_SOC_deviation: 88.5091 Fuel Consumption: 47.6851 Power_mean: 2.1068, Power_std: 5.0170\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.595\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -785.4322021827031 SOC: 0.6811 Cumulative_SOC_deviation: 73.4180 Fuel Consumption: 51.2520 Power_mean: 2.1068, Power_std: 5.0171\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.012\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -735.3235797572413 SOC: 0.7048 Cumulative_SOC_deviation: 68.2366 Fuel Consumption: 52.9575 Power_mean: 2.1068, Power_std: 5.0171\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.069\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -712.9409453714152 SOC: 0.6994 Cumulative_SOC_deviation: 66.0499 Fuel Consumption: 52.4420 Power_mean: 2.1068, Power_std: 5.0172\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.735\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -651.4381197478077 SOC: 0.7054 Cumulative_SOC_deviation: 59.8314 Fuel Consumption: 53.1240 Power_mean: 2.1068, Power_std: 5.0172\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.052\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -632.8212572496218 SOC: 0.7030 Cumulative_SOC_deviation: 57.9786 Fuel Consumption: 53.0351 Power_mean: 2.1068, Power_std: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.146\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -594.271613638974 SOC: 0.6910 Cumulative_SOC_deviation: 54.2104 Fuel Consumption: 52.1677 Power_mean: 2.1068, Power_std: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.866\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -529.2986368494287 SOC: 0.6814 Cumulative_SOC_deviation: 47.7978 Fuel Consumption: 51.3206 Power_mean: 2.1068, Power_std: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.678\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -590.036733459873 SOC: 0.6859 Cumulative_SOC_deviation: 53.8119 Fuel Consumption: 51.9176 Power_mean: 2.1068, Power_std: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.009\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -503.58471764343534 SOC: 0.6750 Cumulative_SOC_deviation: 45.2545 Fuel Consumption: 51.0394 Power_mean: 2.1068, Power_std: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.038\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -591.1388585335416 SOC: 0.6815 Cumulative_SOC_deviation: 53.9451 Fuel Consumption: 51.6880 Power_mean: 2.1068, Power_std: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.960\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -465.893492848595 SOC: 0.6624 Cumulative_SOC_deviation: 41.6032 Fuel Consumption: 49.8618 Power_mean: 2.1068, Power_std: 5.0175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.132\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -519.5650788662772 SOC: 0.6661 Cumulative_SOC_deviation: 46.9301 Fuel Consumption: 50.2645 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.877\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -464.7280073701219 SOC: 0.6660 Cumulative_SOC_deviation: 41.4534 Fuel Consumption: 50.1944 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.073\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -550.8459614803609 SOC: 0.6696 Cumulative_SOC_deviation: 50.0116 Fuel Consumption: 50.7304 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.946\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -496.24381427993467 SOC: 0.6609 Cumulative_SOC_deviation: 44.6197 Fuel Consumption: 50.0470 Power_mean: 2.1068, Power_std: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.083\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -477.8526420874003 SOC: 0.6631 Cumulative_SOC_deviation: 42.7752 Fuel Consumption: 50.1009 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.826\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -448.58070557505795 SOC: 0.6520 Cumulative_SOC_deviation: 39.9298 Fuel Consumption: 49.2825 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.787\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -489.1431243378534 SOC: 0.6689 Cumulative_SOC_deviation: 43.8539 Fuel Consumption: 50.6045 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.400\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -412.3996088464563 SOC: 0.6547 Cumulative_SOC_deviation: 36.2987 Fuel Consumption: 49.4125 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.979\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -400.8298439243987 SOC: 0.6425 Cumulative_SOC_deviation: 35.2375 Fuel Consumption: 48.4552 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.191\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -418.3188150087238 SOC: 0.6491 Cumulative_SOC_deviation: 36.9216 Fuel Consumption: 49.1032 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.016\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -448.10343136939713 SOC: 0.6552 Cumulative_SOC_deviation: 39.8568 Fuel Consumption: 49.5350 Power_mean: 2.1068, Power_std: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.234\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -394.68238955191504 SOC: 0.6425 Cumulative_SOC_deviation: 34.6123 Fuel Consumption: 48.5598 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.990\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -364.5618944763794 SOC: 0.6336 Cumulative_SOC_deviation: 31.6772 Fuel Consumption: 47.7898 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.901\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -374.6436700500841 SOC: 0.6313 Cumulative_SOC_deviation: 32.6963 Fuel Consumption: 47.6802 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.889\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -382.39977867100106 SOC: 0.6349 Cumulative_SOC_deviation: 33.4460 Fuel Consumption: 47.9403 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.385\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -350.73520600837185 SOC: 0.6336 Cumulative_SOC_deviation: 30.2981 Fuel Consumption: 47.7547 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.895\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -374.136386246655 SOC: 0.6377 Cumulative_SOC_deviation: 32.5916 Fuel Consumption: 48.2203 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.029\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -321.3151211672562 SOC: 0.6260 Cumulative_SOC_deviation: 27.4234 Fuel Consumption: 47.0811 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.067\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -306.596886912273 SOC: 0.6223 Cumulative_SOC_deviation: 25.9810 Fuel Consumption: 46.7865 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.025\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -344.6985157037666 SOC: 0.6290 Cumulative_SOC_deviation: 29.7266 Fuel Consumption: 47.4328 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.181\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -364.6323418346282 SOC: 0.6325 Cumulative_SOC_deviation: 31.6942 Fuel Consumption: 47.6907 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.495\n",
      "Episode: 45 Exploration P: 0.2977 Total reward: -339.1710519128556 SOC: 0.6209 Cumulative_SOC_deviation: 29.2328 Fuel Consumption: 46.8426 Power_mean: 2.1068, Power_std: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.221\n",
      "Episode: 46 Exploration P: 0.2899 Total reward: -316.2308568956616 SOC: 0.6203 Cumulative_SOC_deviation: 26.9542 Fuel Consumption: 46.6886 Power_mean: 2.1068, Power_std: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.134\n",
      "Episode: 47 Exploration P: 0.2824 Total reward: -305.0420982474439 SOC: 0.6154 Cumulative_SOC_deviation: 25.8851 Fuel Consumption: 46.1907 Power_mean: 2.1068, Power_std: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.943\n",
      "Episode: 48 Exploration P: 0.2750 Total reward: -314.2346524124316 SOC: 0.6271 Cumulative_SOC_deviation: 26.7139 Fuel Consumption: 47.0953 Power_mean: 2.1068, Power_std: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.672\n",
      "Episode: 49 Exploration P: 0.2678 Total reward: -297.08095404615506 SOC: 0.6232 Cumulative_SOC_deviation: 25.0382 Fuel Consumption: 46.6994 Power_mean: 2.1068, Power_std: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.701\n",
      "Episode: 50 Exploration P: 0.2608 Total reward: -328.00920242930397 SOC: 0.6148 Cumulative_SOC_deviation: 28.1822 Fuel Consumption: 46.1877 Power_mean: 2.1068, Power_std: 5.0178\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "stack_sizes = [1, 4, 8, 16, 32] \n",
    "results_dict = {} \n",
    "for stack_size in stack_sizes: \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer = initialization(stack_size)\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    stacked_states = deque([[0.0] * num_states for _ in range(stack_size)], \n",
    "                            maxlen=stack_size)\n",
    "    for ep in range(total_episodes): \n",
    "        start = time.time() \n",
    "        episodic_reward = 0 \n",
    "        \n",
    "        state = env.reset() \n",
    "        state, stacked_states = stack_states(stacked_states, state, stack_size, True) \n",
    "#         print(state.shape)     \n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "            \n",
    "            next_state, stacked_states = stack_states(stacked_states, next_state, stack_size, \n",
    "                                                        False)\n",
    "            buffer.record((state, action, reward, next_state, done))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "              \"Power_mean: {:.4f}, Power_std: {:.4f}\".format(buffer.power_mean, buffer.power_std)\n",
    "        )\n",
    "\n",
    "    results_dict = {} \n",
    "    results_dict[\"rewards\"] = episode_rewards \n",
    "    results_dict[\"SOCs\"] = episode_SOCs\n",
    "    results_dict[\"FCs\"] = episode_FCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG9_1.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

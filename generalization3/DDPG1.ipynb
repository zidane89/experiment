{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import glob\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG1 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):      \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    inputs_batchnorm = layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs_batchnorm)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_input_batchnorm = layers.BatchNormalization()(state_input)\n",
    "    \n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input_batchnorm)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 500\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 10000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(actor_model, critic_model, target_actor, target_critic, root): \n",
    "    actor_model.save_weights(\"./{}/actor_model_checkpoint\".format(root))\n",
    "    critic_model.save_weights(\"./{}/critic_model_checkpoint\".format(root))\n",
    "    target_actor.save_weights(\"./{}/target_actor_checkpoint\".format(root))\n",
    "    target_critic.save_weights(\"./{}/target_critic_checkpoint\".format(root))\n",
    "    print(\"model is saved..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_env(driving_path, reward_factor):\n",
    "    env = Environment(cell_model, driving_path, battery_path, motor_path, reward_factor)\n",
    "    return env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Trial 0\n",
      "\n",
      "training\\03_nedc.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ValueCreatorSong\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\generalization3\\vehicle_model_DDPG1.py:251: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  del_i = (1 / (2 * r_cha)) * (v_cha - (v_cha ** 2 - 4 * r_cha * p_bat) ** (0.5)) * (p_bat < 0) + (1 / (\n",
      "C:\\Users\\ValueCreatorSong\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\generalization3\\vehicle_model_DDPG1.py:277: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  del_i = (1 / (2 * r_cha)) * (v_cha - (v_cha ** 2 - 4 * r_cha * p_bat) ** (0.5)) * (p_bat < 0) + (1 / (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOC is nan...\n",
      "elapsed_time: 14.109\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -3076.0200908294455 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 48.5147\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 14.236\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -3173.4850215001075 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.9365\n",
      "\n",
      "training\\03_nedc.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ValueCreatorSong\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\generalization3\\vehicle_model_DDPG1.py:252: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  2 * r_dis)) * (v_dis - (v_dis ** 2 - 4 * r_dis * p_bat) ** (0.5)) * (p_bat >= 0)\n",
      "C:\\Users\\ValueCreatorSong\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\generalization3\\vehicle_model_DDPG1.py:278: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  2 * r_dis)) * (v_dis - (v_dis ** 2 - 4 * r_dis * p_bat) ** (0.5)) * (p_bat >= 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOC is nan...\n",
      "elapsed_time: 14.005\n",
      "Episode: 3 Exploration P: 1.0000 Total reward: -3222.044061353341 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 50.2340\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 16.707\n",
      "Episode: 4 Exploration P: 1.0000 Total reward: -3149.5395157929743 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.6595\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 13.819\n",
      "Episode: 5 Exploration P: 1.0000 Total reward: -3057.7346621001543 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.1951\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 13.820\n",
      "Episode: 6 Exploration P: 1.0000 Total reward: -3122.0378631306066 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.4498\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 13.764\n",
      "Episode: 7 Exploration P: 1.0000 Total reward: -3038.5932982566155 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 48.4302\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 12.890\n",
      "Episode: 8 Exploration P: 1.0000 Total reward: -3115.5828045114563 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.5321\n",
      "\n",
      "training\\03_nedc.mat\n",
      "WARNING:tensorflow:Layer batch_normalization_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "SOC is nan...\n",
      "elapsed_time: 12.886\n",
      "Episode: 9 Exploration P: 0.8200 Total reward: -3128.8757941804774 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.9102\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 54.560\n",
      "Episode: 10 Exploration P: 0.8022 Total reward: -2811.7517281052606 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 47.1594\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 52.553\n",
      "Episode: 11 Exploration P: 0.7847 Total reward: -2556.1922841862024 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 41.6671\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 52.594\n",
      "Episode: 12 Exploration P: 0.7676 Total reward: -2336.150111593588 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 39.4268\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 54.861\n",
      "Episode: 13 Exploration P: 0.7510 Total reward: -2383.39865122659 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 38.4026\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 57.271\n",
      "Episode: 14 Exploration P: 0.7347 Total reward: -2331.736998004195 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 37.2086\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 56.918\n",
      "Episode: 15 Exploration P: 0.7187 Total reward: -2271.7139575619267 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 36.2514\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 62.555\n",
      "Episode: 16 Exploration P: 0.7031 Total reward: -2078.3534047131243 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 35.6211\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 62.709\n",
      "Episode: 17 Exploration P: 0.6879 Total reward: -2098.353201945203 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 34.2971\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 70.009\n",
      "Episode: 18 Exploration P: 0.6730 Total reward: -2142.4985829487136 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 35.2323\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 73.865\n",
      "Episode: 19 Exploration P: 0.6584 Total reward: -2067.8614030236745 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 33.9462\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 85.474\n",
      "Episode: 20 Exploration P: 0.6442 Total reward: -1948.286669975996 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 31.9154\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 85.323\n",
      "Episode: 21 Exploration P: 0.6303 Total reward: -2077.7917414948797 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 33.4844\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 89.275\n",
      "Episode: 22 Exploration P: 0.6166 Total reward: -1808.5297056476143 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 30.0762\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 84.142\n",
      "Episode: 23 Exploration P: 0.6033 Total reward: -1922.8376270478316 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 31.4486\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 88.834\n",
      "Episode: 24 Exploration P: 0.5903 Total reward: -1773.204742147609 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 29.3915\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 86.420\n",
      "Episode: 25 Exploration P: 0.5776 Total reward: -1722.3300092584454 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 27.8473\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 86.688\n",
      "Episode: 26 Exploration P: 0.5651 Total reward: -1794.2608905302177 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 30.3292\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 85.229\n",
      "Episode: 27 Exploration P: 0.5530 Total reward: -1821.556166760441 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 29.1537\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 84.674\n",
      "Episode: 28 Exploration P: 0.5411 Total reward: -1840.3986327833966 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 28.3975\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 91.033\n",
      "Episode: 29 Exploration P: 0.5294 Total reward: -1892.2002550963643 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 31.0757\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 75.088\n",
      "Episode: 30 Exploration P: 0.5180 Total reward: -1760.5934694178577 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 27.9257\n",
      "\n",
      "training\\03_nedc.mat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOC is nan...\n",
      "elapsed_time: 81.069\n",
      "Episode: 31 Exploration P: 0.5069 Total reward: -1679.7532895122954 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 26.9684\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 72.443\n",
      "Episode: 32 Exploration P: 0.4960 Total reward: -1650.2768290375707 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 25.7193\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 77.591\n",
      "Episode: 33 Exploration P: 0.4854 Total reward: -1739.4283772019862 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 27.3014\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 76.308\n",
      "Episode: 34 Exploration P: 0.4750 Total reward: -1919.6536001997013 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 29.3343\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 68.364\n",
      "Episode: 35 Exploration P: 0.4648 Total reward: -1613.6973663788808 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 25.4226\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 71.310\n",
      "Episode: 36 Exploration P: 0.4549 Total reward: -1620.2531038822153 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 26.3694\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 83.729\n",
      "Episode: 37 Exploration P: 0.4451 Total reward: -1747.818785666276 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 26.6337\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 76.057\n",
      "Episode: 38 Exploration P: 0.4356 Total reward: -1573.488552298133 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 22.9033\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 68.852\n",
      "Episode: 39 Exploration P: 0.4263 Total reward: -1548.5080568564044 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 23.1923\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 78.104\n",
      "Episode: 40 Exploration P: 0.4172 Total reward: -1570.039215334292 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 23.2311\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 81.702\n",
      "Episode: 41 Exploration P: 0.4083 Total reward: -1463.8953190079196 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 22.6501\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 74.360\n",
      "Episode: 42 Exploration P: 0.3996 Total reward: -1492.7491169761163 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 23.1196\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 65.866\n",
      "Episode: 43 Exploration P: 0.3911 Total reward: -1494.0837843047448 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 20.5796\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 67.139\n",
      "Episode: 44 Exploration P: 0.3828 Total reward: -1523.7413545158938 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 20.4100\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 83.466\n",
      "Episode: 45 Exploration P: 0.3747 Total reward: -1468.1000937450076 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 17.8274\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 76.803\n",
      "Episode: 46 Exploration P: 0.3668 Total reward: -1517.1717960387914 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 19.3854\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 69.315\n",
      "Episode: 47 Exploration P: 0.3590 Total reward: -1468.3612746095243 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 19.7972\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 70.821\n",
      "Episode: 48 Exploration P: 0.3514 Total reward: -1471.5419071944232 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 19.8305\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 87.958\n",
      "Episode: 49 Exploration P: 0.3439 Total reward: -1485.3350345601411 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 19.8431\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 74.394\n",
      "Episode: 50 Exploration P: 0.3367 Total reward: -1430.332736901115 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 16.5101\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 68.573\n",
      "Episode: 51 Exploration P: 0.3296 Total reward: -1475.5268559550107 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 17.7129\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 75.594\n",
      "Episode: 52 Exploration P: 0.3226 Total reward: -1424.4548052481678 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 16.8321\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 66.608\n",
      "Episode: 53 Exploration P: 0.3158 Total reward: -1442.8493093488294 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 16.7879\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 91.989\n",
      "Episode: 54 Exploration P: 0.3092 Total reward: -1445.834402308983 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 16.6481\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 87.404\n",
      "Episode: 55 Exploration P: 0.3027 Total reward: -1455.5062399136273 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 15.1374\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 77.002\n",
      "Episode: 56 Exploration P: 0.2963 Total reward: -1449.3496882345978 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 15.6627\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 75.800\n",
      "Episode: 57 Exploration P: 0.2901 Total reward: -1492.7764946511581 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 14.1371\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 76.462\n",
      "Episode: 58 Exploration P: 0.2840 Total reward: -1468.2998183183472 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 14.9448\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 77.327\n",
      "Episode: 59 Exploration P: 0.2781 Total reward: -1504.2123273185007 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 13.6064\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 75.402\n",
      "Episode: 60 Exploration P: 0.2723 Total reward: -1495.9553404953087 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 14.1745\n",
      "\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "elapsed_time: 79.586\n",
      "Episode: 61 Exploration P: 0.2666 Total reward: -1456.5174970809887 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 15.1128\n",
      "\n",
      "training\\03_nedc.mat\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 3\n",
    "results_dict = {} \n",
    "driving_cycle_paths = glob.glob(\"training/*.mat\")[1:2]\n",
    "\n",
    "for trial in range(num_trials): \n",
    "    print(\"\")\n",
    "    print(\"Trial {}\".format(trial))\n",
    "    print(\"\")\n",
    "    \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer = initialization()\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    for ep in range(total_episodes): \n",
    "        driving_cycle_path = np.random.choice(driving_cycle_paths)\n",
    "        print(driving_cycle_path)\n",
    "        env = initialization_env(driving_cycle_path, 10)\n",
    "        \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * (steps\n",
    "                                                                        -DELAY_TRAINING))\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "        )\n",
    "        print(\"\")\n",
    "    \n",
    "    root = \"DDPG1_trial{}\".format(trial+1)\n",
    "    save_weights(actor_model, critic_model, target_actor, target_critic, root)\n",
    "    \n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG1.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

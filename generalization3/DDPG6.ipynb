{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import glob\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG6 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):      \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    inputs_batchnorm = layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs_batchnorm)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_input_batchnorm = layers.BatchNormalization()(state_input)\n",
    "    \n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input_batchnorm)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 500\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 10000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(actor_model, critic_model, target_actor, target_critic, root): \n",
    "    actor_model.save_weights(\"./{}/actor_model_checkpoint\".format(root))\n",
    "    critic_model.save_weights(\"./{}/critic_model_checkpoint\".format(root))\n",
    "    target_actor.save_weights(\"./{}/target_actor_checkpoint\".format(root))\n",
    "    target_critic.save_weights(\"./{}/target_critic_checkpoint\".format(root))\n",
    "    print(\"model is saved..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_env(driving_path, reward_factor):\n",
    "    env = Environment(cell_model, driving_path, battery_path, motor_path, reward_factor)\n",
    "    return env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(actor_model, reward_factor, test_path_start):\n",
    "    test_cycles = glob.glob(\"../data/driving_cycles/all/*.mat\")[test_path_start:]\n",
    "    test_cycle = np.random.choice(test_cycles)\n",
    "    env = initialization_env(test_cycle, reward_factor)\n",
    "    \n",
    "    total_reward = 0\n",
    "    state = env.reset() \n",
    "    while True: \n",
    "        tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "        action = policy_epsilon_greedy(tf_state, -1)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        state = next_state \n",
    "        total_reward += reward \n",
    "        \n",
    "        if done: \n",
    "            break \n",
    "        \n",
    "    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "    \n",
    "    print(\"******************* Test is start *****************\")\n",
    "    print(test_cycle)\n",
    "    print('Total reward: {}'.format(total_reward), \n",
    "          \"SOC: {:.4f}\".format(env.SOC), \n",
    "          \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "          \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption))\n",
    "    print(\"******************* Test is done *****************\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Trial 0\n",
      "\n",
      "../data/driving_cycles/all\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 15.498\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -4628.309848960044 SOC: 1.0000 Cumulative_SOC_deviation: 447.3346 Fuel Consumption: 154.9641\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_Antwerp1_May19c.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 12.613\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -3623.8848190494937 SOC: 1.0000 Cumulative_SOC_deviation: 350.5143 Fuel Consumption: 118.7423\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_TMB_Line24N_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 37.178\n",
      "Episode: 3 Exploration P: 1.0000 Total reward: -11545.101255008169 SOC: 1.0000 Cumulative_SOC_deviation: 1122.2120 Fuel Consumption: 322.9813\n",
      "\n",
      "../data/driving_cycles/all\\VITO_MOLCity.mat\n",
      "Available condition is not avail... SOC: 0.9992414085827286\n",
      "elapsed_time: 11.545\n",
      "Episode: 4 Exploration P: 1.0000 Total reward: -3116.6240078285564 SOC: 0.9992 Cumulative_SOC_deviation: 301.5776 Fuel Consumption: 100.8478\n",
      "\n",
      "../data/driving_cycles/all\\07_manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 13.424\n",
      "Episode: 5 Exploration P: 1.0000 Total reward: -3864.776377138435 SOC: 1.0000 Cumulative_SOC_deviation: 374.6453 Fuel Consumption: 118.3234\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_VH_Brussels_Empty_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 29.986\n",
      "Episode: 6 Exploration P: 1.0000 Total reward: -8071.043046236733 SOC: 1.0000 Cumulative_SOC_deviation: 783.6374 Fuel Consumption: 234.6691\n",
      "\n",
      "../data/driving_cycles/all\\07_manhattan.mat\n",
      "WARNING:tensorflow:Layer batch_normalization_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 49.793\n",
      "Episode: 7 Exploration P: 0.9889 Total reward: -3901.4636232170687 SOC: 1.0000 Cumulative_SOC_deviation: 377.8923 Fuel Consumption: 122.5409\n",
      "\n",
      "../data/driving_cycles/all\\nuremberg_r36.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.125\n",
      "Episode: 8 Exploration P: 0.9679 Total reward: -3845.9668633539673 SOC: 1.0000 Cumulative_SOC_deviation: 372.9889 Fuel Consumption: 116.0780\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_VH_Brussels_Empty_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 173.989\n",
      "Episode: 9 Exploration P: 0.9280 Total reward: -8001.742483127233 SOC: 1.0000 Cumulative_SOC_deviation: 778.6610 Fuel Consumption: 215.1328\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_Antwerp1_May19c.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.769\n",
      "Episode: 10 Exploration P: 0.9094 Total reward: -3552.983634787467 SOC: 1.0000 Cumulative_SOC_deviation: 344.7227 Fuel Consumption: 105.7564\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/all\\VITO_RW_Jumper_Brussels_101_1.mat\n",
      "Total reward: -2935.5432896418984 SOC: 0.3953 Cumulative_SOC_deviation: 292.9359 Fuel Consumption: 6.1845\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/all\\ny_city_traffic.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 45.589\n",
      "Episode: 11 Exploration P: 0.8987 Total reward: -1773.1163006326526 SOC: 1.0000 Cumulative_SOC_deviation: 171.2881 Fuel Consumption: 60.2353\n",
      "\n",
      "../data/driving_cycles/all\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 107.990\n",
      "Episode: 12 Exploration P: 0.8660 Total reward: -6399.9071629952505 SOC: 1.0000 Cumulative_SOC_deviation: 621.1700 Fuel Consumption: 188.2067\n",
      "\n",
      "../data/driving_cycles/all\\cudec_freeway.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 31.797\n",
      "Episode: 13 Exploration P: 0.8568 Total reward: -630.8088842591319 SOC: 0.5006 Cumulative_SOC_deviation: 57.6275 Fuel Consumption: 54.5334\n",
      "\n",
      "../data/driving_cycles/all\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.774\n",
      "Episode: 14 Exploration P: 0.8338 Total reward: -4131.644106310677 SOC: 1.0000 Cumulative_SOC_deviation: 400.3540 Fuel Consumption: 128.1046\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_VH_Brussels_Empty_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 125.967\n",
      "Episode: 15 Exploration P: 0.7995 Total reward: -7815.0954862373 SOC: 1.0000 Cumulative_SOC_deviation: 761.8152 Fuel Consumption: 196.9439\n",
      "\n",
      "../data/driving_cycles/all\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 62.848\n",
      "Episode: 16 Exploration P: 0.7829 Total reward: -3342.899861435534 SOC: 1.0000 Cumulative_SOC_deviation: 324.4451 Fuel Consumption: 98.4489\n",
      "\n",
      "../data/driving_cycles/all\\ny_city_composite_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.292\n",
      "Episode: 17 Exploration P: 0.7672 Total reward: -3516.8212218919775 SOC: 1.0000 Cumulative_SOC_deviation: 342.8458 Fuel Consumption: 88.3636\n",
      "\n",
      "../data/driving_cycles/all\\ny_city_traffic.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 35.301\n",
      "Episode: 18 Exploration P: 0.7582 Total reward: -1646.2369931387516 SOC: 1.0000 Cumulative_SOC_deviation: 159.1748 Fuel Consumption: 54.4893\n",
      "\n",
      "../data/driving_cycles/all\\03_nedc.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 69.994\n",
      "Episode: 19 Exploration P: 0.7407 Total reward: -3802.1836516617905 SOC: 0.7988 Cumulative_SOC_deviation: 369.6489 Fuel Consumption: 105.6950\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_Antwerp1_May19c.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.369\n",
      "Episode: 20 Exploration P: 0.7259 Total reward: -3405.135538340008 SOC: 1.0000 Cumulative_SOC_deviation: 332.1179 Fuel Consumption: 83.9562\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_VH_Brussels_Full_1.mat\n",
      "Total reward: -5632.910359476618 SOC: 0.2770 Cumulative_SOC_deviation: 562.6754 Fuel Consumption: 6.1563\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/all\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 102.370\n",
      "Episode: 21 Exploration P: 0.7065 Total reward: -3477.998874626488 SOC: 1.0000 Cumulative_SOC_deviation: 336.4641 Fuel Consumption: 113.3576\n",
      "\n",
      "../data/driving_cycles/all\\nuremberg_r36.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.383\n",
      "Episode: 22 Exploration P: 0.6916 Total reward: -3589.688237151609 SOC: 1.0000 Cumulative_SOC_deviation: 350.2308 Fuel Consumption: 87.3798\n",
      "\n",
      "../data/driving_cycles/all\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 102.305\n",
      "Episode: 23 Exploration P: 0.6731 Total reward: -3793.2948471038026 SOC: 1.0000 Cumulative_SOC_deviation: 368.5287 Fuel Consumption: 108.0081\n",
      "\n",
      "../data/driving_cycles/all\\03_nedc.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ValueCreatorSong\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\generalization3\\vehicle_model_DDPG6.py:251: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  del_i = (1 / (2 * r_cha)) * (v_cha - (v_cha ** 2 - 4 * r_cha * p_bat) ** (0.5)) * (p_bat < 0) + (1 / (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.814\n",
      "Episode: 24 Exploration P: 0.6576 Total reward: -3585.051934864912 SOC: 0.8005 Cumulative_SOC_deviation: 349.7391 Fuel Consumption: 87.6613\n",
      "\n",
      "../data/driving_cycles/all\\VITO_MOLCity.mat\n",
      "Available condition is not avail... SOC: 0.9902306675316731\n",
      "elapsed_time: 68.518\n",
      "Episode: 25 Exploration P: 0.6459 Total reward: -2544.9693211357353 SOC: 0.9902 Cumulative_SOC_deviation: 248.2637 Fuel Consumption: 62.3326\n",
      "\n",
      "../data/driving_cycles/all\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.880\n",
      "Episode: 26 Exploration P: 0.6321 Total reward: -3552.6065099698517 SOC: 1.0000 Cumulative_SOC_deviation: 347.2620 Fuel Consumption: 79.9863\n",
      "\n",
      "../data/driving_cycles/all\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 133.506\n",
      "Episode: 27 Exploration P: 0.6092 Total reward: -5155.976354618653 SOC: 0.9856 Cumulative_SOC_deviation: 502.4488 Fuel Consumption: 131.4882\n",
      "\n",
      "../data/driving_cycles/all\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 63.449\n",
      "Episode: 28 Exploration P: 0.5967 Total reward: -2498.0500640192386 SOC: 0.9893 Cumulative_SOC_deviation: 242.1108 Fuel Consumption: 76.9417\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_Antwerp1_May19c.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.454\n",
      "Episode: 29 Exploration P: 0.5848 Total reward: -3233.82488776455 SOC: 1.0000 Cumulative_SOC_deviation: 316.2731 Fuel Consumption: 71.0939\n",
      "\n",
      "../data/driving_cycles/all\\03_nedc.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ValueCreatorSong\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\generalization3\\vehicle_model_DDPG6.py:277: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  del_i = (1 / (2 * r_cha)) * (v_cha - (v_cha ** 2 - 4 * r_cha * p_bat) ** (0.5)) * (p_bat < 0) + (1 / (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOC is nan...\n",
      "elapsed_time: 66.806\n",
      "Episode: 30 Exploration P: 0.5721 Total reward: -13228.355254739337 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 75.3960\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/all\\VITO_RW_Jumper_Brussels_101_1.mat\n",
      "Total reward: -2935.5432896418984 SOC: 0.3953 Cumulative_SOC_deviation: 292.9359 Fuel Consumption: 6.1845\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/all\\cudec_freeway.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 43.779\n",
      "Episode: 31 Exploration P: 0.5660 Total reward: -1450.872351567504 SOC: 0.1963 Cumulative_SOC_deviation: 141.6470 Fuel Consumption: 34.4024\n",
      "\n",
      "../data/driving_cycles/all\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 91.174\n",
      "Episode: 32 Exploration P: 0.5509 Total reward: -2696.801414359936 SOC: 1.0000 Cumulative_SOC_deviation: 260.7264 Fuel Consumption: 89.5376\n",
      "\n",
      "../data/driving_cycles/all\\cudec_freeway.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 32.510\n",
      "Episode: 33 Exploration P: 0.5451 Total reward: -1412.344465363713 SOC: 0.1974 Cumulative_SOC_deviation: 137.8014 Fuel Consumption: 34.3306\n",
      "\n",
      "../data/driving_cycles/all\\cudec_freeway.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 32.502\n",
      "Episode: 34 Exploration P: 0.5393 Total reward: -1495.676642372468 SOC: 0.1723 Cumulative_SOC_deviation: 146.2564 Fuel Consumption: 33.1125\n",
      "\n",
      "../data/driving_cycles/all\\nuremberg_r36.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.370\n",
      "Episode: 35 Exploration P: 0.5280 Total reward: -3378.9095371487088 SOC: 1.0000 Cumulative_SOC_deviation: 330.8521 Fuel Consumption: 70.3889\n",
      "\n",
      "../data/driving_cycles/all\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 88.832\n",
      "Episode: 36 Exploration P: 0.5168 Total reward: -3271.260773535311 SOC: 1.0000 Cumulative_SOC_deviation: 320.4680 Fuel Consumption: 66.5809\n",
      "\n",
      "../data/driving_cycles/all\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 123.022\n",
      "Episode: 37 Exploration P: 0.4982 Total reward: -3928.385373246903 SOC: 0.9157 Cumulative_SOC_deviation: 381.7994 Fuel Consumption: 110.3919\n",
      "\n",
      "../data/driving_cycles/all\\03_nedc.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.194\n",
      "Episode: 38 Exploration P: 0.4868 Total reward: -3105.385642893892 SOC: 0.7237 Cumulative_SOC_deviation: 302.9090 Fuel Consumption: 76.2958\n",
      "\n",
      "../data/driving_cycles/all\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.352\n",
      "Episode: 39 Exploration P: 0.4765 Total reward: -3004.814089104128 SOC: 1.0000 Cumulative_SOC_deviation: 294.6967 Fuel Consumption: 57.8470\n",
      "\n",
      "../data/driving_cycles/all\\ny_city_composite_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.569\n",
      "Episode: 40 Exploration P: 0.4670 Total reward: -2940.0286483014766 SOC: 1.0000 Cumulative_SOC_deviation: 288.4828 Fuel Consumption: 55.2008\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/all\\VITO_RW_Jumper_Brussels_101_1.mat\n",
      "Total reward: -2935.5432896418984 SOC: 0.3953 Cumulative_SOC_deviation: 292.9359 Fuel Consumption: 6.1845\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/all\\ny_city_composite_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 66.378\n",
      "Episode: 41 Exploration P: 0.4577 Total reward: -2946.7320500188985 SOC: 1.0000 Cumulative_SOC_deviation: 289.0253 Fuel Consumption: 56.4792\n",
      "\n",
      "../data/driving_cycles/all\\VITO_MOLCity.mat\n",
      "Available condition is not avail... SOC: 0.9874290055112808\n",
      "elapsed_time: 58.723\n",
      "Episode: 42 Exploration P: 0.4495 Total reward: -2058.518920665561 SOC: 0.9874 Cumulative_SOC_deviation: 200.7325 Fuel Consumption: 51.1940\n",
      "\n",
      "../data/driving_cycles/all\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.734\n",
      "Episode: 43 Exploration P: 0.4334 Total reward: -3419.782640524423 SOC: 0.8974 Cumulative_SOC_deviation: 331.6846 Fuel Consumption: 102.9365\n",
      "\n",
      "../data/driving_cycles/all\\07_manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.632\n",
      "Episode: 44 Exploration P: 0.4242 Total reward: -3028.0362978627704 SOC: 1.0000 Cumulative_SOC_deviation: 297.3189 Fuel Consumption: 54.8476\n",
      "\n",
      "../data/driving_cycles/all\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 71.375\n",
      "Episode: 45 Exploration P: 0.4153 Total reward: -2733.590823721542 SOC: 1.0000 Cumulative_SOC_deviation: 268.1286 Fuel Consumption: 52.3051\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_VH_Brussels_Empty_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 132.181\n",
      "Episode: 46 Exploration P: 0.3984 Total reward: -6510.072318025821 SOC: 1.0000 Cumulative_SOC_deviation: 640.7345 Fuel Consumption: 102.7273\n",
      "\n",
      "../data/driving_cycles/all\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 69.347\n",
      "Episode: 47 Exploration P: 0.3900 Total reward: -2963.23957200551 SOC: 1.0000 Cumulative_SOC_deviation: 291.0975 Fuel Consumption: 52.2648\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_TMB_Line24N_1.mat\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 3\n",
    "results_dict = {} \n",
    "driving_cycle_paths = glob.glob(\"../data/driving_cycles/all/*.mat\")\n",
    "# driving_cycle_paths.pop(1)\n",
    "driving_cycle_paths = driving_cycle_paths[:15]\n",
    "\n",
    "for trial in range(num_trials): \n",
    "    print(\"\")\n",
    "    print(\"Trial {}\".format(trial))\n",
    "    print(\"\")\n",
    "    \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer = initialization()\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    for ep in range(total_episodes): \n",
    "        driving_cycle_path = np.random.choice(driving_cycle_paths)\n",
    "        print(driving_cycle_path)\n",
    "        env = initialization_env(driving_cycle_path, 10)\n",
    "        \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * (steps\n",
    "                                                                        -DELAY_TRAINING))\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "        )\n",
    "        print(\"\")\n",
    "        \n",
    "        if (ep + 1) % 10 == 0: \n",
    "            test_agent(actor_model, 10, 15)\n",
    "    \n",
    "    root = \"DDPG6_trial{}\".format(trial+1)\n",
    "    save_weights(actor_model, critic_model, target_actor, target_critic, root)\n",
    "    \n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG6.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import glob\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG1 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):      \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    inputs_batchnorm = layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs_batchnorm)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_input_batchnorm = layers.BatchNormalization()(state_input)\n",
    "    \n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input_batchnorm)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 300\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 10000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(actor_model, critic_model, target_actor, target_critic, root): \n",
    "    actor_model.save_weights(\"./{}/actor_model_checkpoint\".format(root))\n",
    "    critic_model.save_weights(\"./{}/critic_model_checkpoint\".format(root))\n",
    "    target_actor.save_weights(\"./{}/target_actor_checkpoint\".format(root))\n",
    "    target_critic.save_weights(\"./{}/target_critic_checkpoint\".format(root))\n",
    "    print(\"model is saved..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_env(driving_path, reward_factor):\n",
    "    env = Environment(cell_model, driving_path, battery_path, motor_path, reward_factor)\n",
    "    return env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(actor_model, reward_factor):\n",
    "    test_cycle = \"../data/driving_cycles/all/00_nedc.mat\"\n",
    "    env = initialization_env(test_cycle, reward_factor)\n",
    "    \n",
    "    total_reward = 0\n",
    "    state = env.reset() \n",
    "    while True: \n",
    "        tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "        action = policy_epsilon_greedy(tf_state, -1)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        state = next_state \n",
    "        total_reward += reward \n",
    "        \n",
    "        if done: \n",
    "            break \n",
    "        \n",
    "    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "    \n",
    "    print(\"******************* Test is start *****************\")\n",
    "    print(test_cycle)\n",
    "    print('Total reward: {}'.format(total_reward), \n",
    "          \"SOC: {:.4f}\".format(env.SOC), \n",
    "          \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "          \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption))\n",
    "    print(\"******************* Test is done *****************\")\n",
    "    print(\"\")\n",
    "    return env.history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Trial 0\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_Decade_Octavia_BCN_City1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 19.921\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -5959.981988464335 SOC: 1.0000 Cumulative_SOC_deviation: 578.2462 Fuel Consumption: 177.5199\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_BUS_TMB_Line24N_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 36.356\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -11487.199170049034 SOC: 1.0000 Cumulative_SOC_deviation: 1116.5252 Fuel Consumption: 321.9470\n",
      "\n",
      "../data/driving_cycles/city\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 16.862\n",
      "Episode: 3 Exploration P: 1.0000 Total reward: -4709.719491787197 SOC: 1.0000 Cumulative_SOC_deviation: 455.4804 Fuel Consumption: 154.9153\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_Decade_Octavia_MOL_City1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 18.360\n",
      "Episode: 4 Exploration P: 1.0000 Total reward: -5459.114701824723 SOC: 1.0000 Cumulative_SOC_deviation: 529.5665 Fuel Consumption: 163.4494\n",
      "\n",
      "../data/driving_cycles/city\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 16.959\n",
      "Episode: 5 Exploration P: 1.0000 Total reward: -4576.792099912173 SOC: 1.0000 Cumulative_SOC_deviation: 442.5606 Fuel Consumption: 151.1862\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_Decade_Jumper_BCN_City1.mat\n",
      "WARNING:tensorflow:Layer batch_normalization_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 51.903\n",
      "Episode: 6 Exploration P: 0.9889 Total reward: -6653.195567806192 SOC: 1.0000 Cumulative_SOC_deviation: 646.1527 Fuel Consumption: 191.6686\n",
      "\n",
      "../data/driving_cycles/city\\ny_city_composite_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.529\n",
      "Episode: 7 Exploration P: 0.9689 Total reward: -3669.529480265905 SOC: 1.0000 Cumulative_SOC_deviation: 355.3927 Fuel Consumption: 115.6024\n",
      "\n",
      "../data/driving_cycles/city\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 109.022\n",
      "Episode: 8 Exploration P: 0.9336 Total reward: -6495.0095317970645 SOC: 1.0000 Cumulative_SOC_deviation: 629.7161 Fuel Consumption: 197.8484\n",
      "\n",
      "../data/driving_cycles/city\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.982\n",
      "Episode: 9 Exploration P: 0.9142 Total reward: -3614.5765234110136 SOC: 1.0000 Cumulative_SOC_deviation: 350.3303 Fuel Consumption: 111.2739\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_Decade_Jumper_BCN_City1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 105.311\n",
      "Episode: 10 Exploration P: 0.8829 Total reward: -6584.086397348744 SOC: 1.0000 Cumulative_SOC_deviation: 641.2310 Fuel Consumption: 171.7761\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/city\\wvucity.mat\n",
      "Total reward: -1042.2947551629077 SOC: 0.4632 Cumulative_SOC_deviation: 103.8983 Fuel Consumption: 3.3120\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_Antwerp1_May19c.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.453\n",
      "Episode: 11 Exploration P: 0.8652 Total reward: -3496.3009791184945 SOC: 1.0000 Cumulative_SOC_deviation: 339.6279 Fuel Consumption: 100.0219\n",
      "\n",
      "../data/driving_cycles/city\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 63.707\n",
      "Episode: 12 Exploration P: 0.8472 Total reward: -3528.6982330611927 SOC: 1.0000 Cumulative_SOC_deviation: 342.5259 Fuel Consumption: 103.4392\n",
      "\n",
      "../data/driving_cycles/city\\07_manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.426\n",
      "Episode: 13 Exploration P: 0.8292 Total reward: -3743.464984062867 SOC: 1.0000 Cumulative_SOC_deviation: 364.3293 Fuel Consumption: 100.1719\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_Decade_Polo_BCN_City1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 88.348\n",
      "Episode: 14 Exploration P: 0.8054 Total reward: -5206.789203257916 SOC: 1.0000 Cumulative_SOC_deviation: 507.9258 Fuel Consumption: 127.5313\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_BUS_VH_Brussels_Medium_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 137.656\n",
      "Episode: 15 Exploration P: 0.7701 Total reward: -8373.82827751374 SOC: 1.0000 Cumulative_SOC_deviation: 817.5436 Fuel Consumption: 198.3919\n",
      "\n",
      "../data/driving_cycles/city\\VITO_DUBDC.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 52.900\n",
      "Episode: 16 Exploration P: 0.7565 Total reward: -2670.9627327436315 SOC: 1.0000 Cumulative_SOC_deviation: 259.5699 Fuel Consumption: 75.2633\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_Decade_Jumper_MOL_City1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 123.888\n",
      "Episode: 17 Exploration P: 0.7272 Total reward: -7332.549494984242 SOC: 1.0000 Cumulative_SOC_deviation: 716.6983 Fuel Consumption: 165.5668\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_Decade_Octavia_BCN_City1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 99.044\n",
      "Episode: 18 Exploration P: 0.7044 Total reward: -5705.8360544951065 SOC: 1.0000 Cumulative_SOC_deviation: 557.4017 Fuel Consumption: 131.8194\n",
      "\n",
      "../data/driving_cycles/city\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.835\n",
      "Episode: 19 Exploration P: 0.6898 Total reward: -3198.8121075743475 SOC: 1.0000 Cumulative_SOC_deviation: 311.2660 Fuel Consumption: 86.1525\n",
      "\n",
      "../data/driving_cycles/city\\ny_city_traffic.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 36.077\n",
      "Episode: 20 Exploration P: 0.6817 Total reward: -1573.4113244926525 SOC: 1.0000 Cumulative_SOC_deviation: 152.5763 Fuel Consumption: 47.6487\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/city\\wvucity.mat\n",
      "Total reward: -1042.2947551629077 SOC: 0.4632 Cumulative_SOC_deviation: 103.8983 Fuel Consumption: 3.3120\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_Decade_Octavia_MOL_City1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.793\n",
      "Episode: 21 Exploration P: 0.6619 Total reward: -4973.248215672752 SOC: 1.0000 Cumulative_SOC_deviation: 486.1501 Fuel Consumption: 111.7471\n",
      "\n",
      "../data/driving_cycles/city\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.704\n",
      "Episode: 22 Exploration P: 0.6479 Total reward: -3531.1215945399304 SOC: 1.0000 Cumulative_SOC_deviation: 345.0075 Fuel Consumption: 81.0464\n",
      "\n",
      "../data/driving_cycles/city\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.621\n",
      "Episode: 23 Exploration P: 0.6306 Total reward: -3432.215870840746 SOC: 1.0000 Cumulative_SOC_deviation: 332.6610 Fuel Consumption: 105.6062\n",
      "\n",
      "../data/driving_cycles/city\\VITO_DUBDC.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 54.275\n",
      "Episode: 24 Exploration P: 0.6196 Total reward: -2299.261410370185 SOC: 1.0000 Cumulative_SOC_deviation: 223.7541 Fuel Consumption: 61.7201\n",
      "\n",
      "../data/driving_cycles/city\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.662\n",
      "Episode: 25 Exploration P: 0.6064 Total reward: -3452.121568137693 SOC: 1.0000 Cumulative_SOC_deviation: 337.3402 Fuel Consumption: 78.7193\n",
      "\n",
      "../data/driving_cycles/city\\07_manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 64.074\n",
      "Episode: 26 Exploration P: 0.5936 Total reward: -3522.337487971456 SOC: 1.0000 Cumulative_SOC_deviation: 344.4796 Fuel Consumption: 77.5414\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_BUS_TMB_Line24N_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 178.034\n",
      "Episode: 27 Exploration P: 0.5602 Total reward: -11025.157752349009 SOC: 1.0000 Cumulative_SOC_deviation: 1083.6707 Fuel Consumption: 188.4509\n",
      "\n",
      "../data/driving_cycles/city\\ny_city_composite_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.363\n",
      "Episode: 28 Exploration P: 0.5490 Total reward: -3186.336677001823 SOC: 1.0000 Cumulative_SOC_deviation: 312.0305 Fuel Consumption: 66.0319\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_Decade_Jumper_MOL_City1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.472\n",
      "Episode: 29 Exploration P: 0.5278 Total reward: -6831.684102262838 SOC: 1.0000 Cumulative_SOC_deviation: 670.4725 Fuel Consumption: 126.9596\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_Decade_Jumper_BCN_City1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.805\n",
      "Episode: 30 Exploration P: 0.5099 Total reward: -5947.28873080631 SOC: 1.0000 Cumulative_SOC_deviation: 584.4030 Fuel Consumption: 103.2582\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/city\\VITO_RW_Kangoo_DePost_Brussels_101_1.mat\n",
      "Total reward: -3834.1646647404073 SOC: 0.3629 Cumulative_SOC_deviation: 382.7844 Fuel Consumption: 6.3208\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_Decade_Polo_BCN_City1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.600\n",
      "Episode: 31 Exploration P: 0.4953 Total reward: -4415.359154409889 SOC: 1.0000 Cumulative_SOC_deviation: 432.9285 Fuel Consumption: 86.0741\n",
      "\n",
      "../data/driving_cycles/city\\ny_city_traffic.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 36.042\n",
      "Episode: 32 Exploration P: 0.4896 Total reward: -1212.436608924883 SOC: 0.9832 Cumulative_SOC_deviation: 117.5903 Fuel Consumption: 36.5331\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_BUS_VH_Brussels_Empty_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 126.848\n",
      "Episode: 33 Exploration P: 0.4696 Total reward: -6764.442472504828 SOC: 1.0000 Cumulative_SOC_deviation: 664.5922 Fuel Consumption: 118.5201\n",
      "\n",
      "../data/driving_cycles/city\\VITO_MOLCity.mat\n",
      "Available condition is not avail... SOC: 0.9801546124038731\n",
      "elapsed_time: 55.778\n",
      "Episode: 34 Exploration P: 0.4612 Total reward: -1924.3233197584752 SOC: 0.9802 Cumulative_SOC_deviation: 187.8373 Fuel Consumption: 45.9508\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_Decade_Octavia_BCN_City1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 98.039\n",
      "Episode: 35 Exploration P: 0.4469 Total reward: -4919.634075643142 SOC: 1.0000 Cumulative_SOC_deviation: 483.3963 Fuel Consumption: 85.6706\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_BUS_VH_Brussels_Full_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 156.982\n",
      "Episode: 36 Exploration P: 0.4246 Total reward: -8532.861141493722 SOC: 1.0000 Cumulative_SOC_deviation: 840.0285 Fuel Consumption: 132.5763\n",
      "\n",
      "../data/driving_cycles/city\\VITO_RW_Antwerp1_May19c.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 63.472\n",
      "Episode: 37 Exploration P: 0.4162 Total reward: -2730.558294360859 SOC: 1.0000 Cumulative_SOC_deviation: 267.7728 Fuel Consumption: 52.8303\n",
      "\n",
      "../data/driving_cycles/city\\nuremberg_r36.mat\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 1\n",
    "results_dict = {} \n",
    "driving_cycle_paths = glob.glob(\"../data/driving_cycles/all/*.mat\")[1:][:7]\n",
    "\n",
    "for trial in range(num_trials): \n",
    "    print(\"\")\n",
    "    print(\"Trial {}\".format(trial))\n",
    "    print(\"\")\n",
    "    \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer = initialization()\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    episode_test_history = [] \n",
    "    episode_num_test = [] \n",
    "    for ep in range(total_episodes): \n",
    "        driving_cycle_path = np.random.choice(driving_cycle_paths)\n",
    "        print(driving_cycle_path)\n",
    "        env = initialization_env(driving_cycle_path, 10)\n",
    "        \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * (steps\n",
    "                                                                        -DELAY_TRAINING))\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "        )\n",
    "        print(\"\")\n",
    "        \n",
    "        if (ep + 1) % 10 == 0: \n",
    "            history = test_agent(actor_model, 10)\n",
    "            episode_test_history.append(history) \n",
    "            episode_num_test.append(ep + 1)\n",
    "    \n",
    "    root = \"DDPG4_trial{}\".format(trial+1)\n",
    "    save_weights(actor_model, critic_model, target_actor, target_critic, root)\n",
    "    \n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs, \n",
    "        \"test_history\": episode_test_history, \n",
    "        \"test_episode_num\": episode_num_test, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG4.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

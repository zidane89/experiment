{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import glob\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG7 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):      \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    inputs_batchnorm = layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs_batchnorm)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_input_batchnorm = layers.BatchNormalization()(state_input)\n",
    "    \n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input_batchnorm)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 500\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 10000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(actor_model, critic_model, target_actor, target_critic, root): \n",
    "    actor_model.save_weights(\"./{}/actor_model_checkpoint\".format(root))\n",
    "    critic_model.save_weights(\"./{}/critic_model_checkpoint\".format(root))\n",
    "    target_actor.save_weights(\"./{}/target_actor_checkpoint\".format(root))\n",
    "    target_critic.save_weights(\"./{}/target_critic_checkpoint\".format(root))\n",
    "    print(\"model is saved..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_env(driving_path, reward_factor):\n",
    "    env = Environment(cell_model, driving_path, battery_path, motor_path, reward_factor)\n",
    "    return env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(actor_model, reward_factor, test_path_start):\n",
    "    test_cycles = glob.glob(\"../data/driving_cycles/all/*.mat\")[test_path_start:]\n",
    "    test_cycle = np.random.choice(test_cycles)\n",
    "    env = initialization_env(test_cycle, reward_factor)\n",
    "    \n",
    "    total_reward = 0\n",
    "    state = env.reset() \n",
    "    while True: \n",
    "        tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "        action = policy_epsilon_greedy(tf_state, -1)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        state = next_state \n",
    "        total_reward += reward \n",
    "        \n",
    "        if done: \n",
    "            break \n",
    "        \n",
    "    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "    \n",
    "    print(\"******************* Test is start *****************\")\n",
    "    print(test_cycle)\n",
    "    print('Total reward: {}'.format(total_reward), \n",
    "          \"SOC: {:.4f}\".format(env.SOC), \n",
    "          \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "          \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption))\n",
    "    print(\"******************* Test is done *****************\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Trial 0\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_VH_Brussels_Medium_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 26.856\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -8614.509224590083 SOC: 1.0000 Cumulative_SOC_deviation: 836.9696 Fuel Consumption: 244.8136\n",
      "\n",
      "../data/driving_cycles/all\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 23.476\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -6601.191892633903 SOC: 1.0000 Cumulative_SOC_deviation: 639.4460 Fuel Consumption: 206.7317\n",
      "\n",
      "../data/driving_cycles/all\\ny_city_composite_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 12.863\n",
      "Episode: 3 Exploration P: 1.0000 Total reward: -3661.670773154882 SOC: 1.0000 Cumulative_SOC_deviation: 354.9424 Fuel Consumption: 112.2468\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_VH_Brussels_Full_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 33.373\n",
      "Episode: 4 Exploration P: 1.0000 Total reward: -10127.494108588035 SOC: 1.0000 Cumulative_SOC_deviation: 983.7725 Fuel Consumption: 289.7688\n",
      "\n",
      "../data/driving_cycles/all\\03_nedc.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 16.698\n",
      "Episode: 5 Exploration P: 1.0000 Total reward: -4164.800851855351 SOC: 0.8976 Cumulative_SOC_deviation: 402.2668 Fuel Consumption: 142.1331\n",
      "\n",
      "../data/driving_cycles/all\\VITO_DUBDC.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 12.238\n",
      "Episode: 6 Exploration P: 1.0000 Total reward: -3034.230389174519 SOC: 1.0000 Cumulative_SOC_deviation: 293.4446 Fuel Consumption: 99.7843\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_VH_Brussels_Empty_1.mat\n",
      "WARNING:tensorflow:Layer batch_normalization_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 146.741\n",
      "Episode: 7 Exploration P: 0.9612 Total reward: -8061.698902593283 SOC: 1.0000 Cumulative_SOC_deviation: 783.5192 Fuel Consumption: 226.5070\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_VH_Brussels_Full_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 211.661\n",
      "Episode: 8 Exploration P: 0.9127 Total reward: -10014.604014352968 SOC: 1.0000 Cumulative_SOC_deviation: 974.2880 Fuel Consumption: 271.7236\n",
      "\n",
      "../data/driving_cycles/all\\07_manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.112\n",
      "Episode: 9 Exploration P: 0.8932 Total reward: -3825.4288402922934 SOC: 1.0000 Cumulative_SOC_deviation: 371.8230 Fuel Consumption: 107.1992\n",
      "\n",
      "../data/driving_cycles/all\\ny_city_traffic.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 44.904\n",
      "Episode: 10 Exploration P: 0.8827 Total reward: -1723.966957068954 SOC: 1.0000 Cumulative_SOC_deviation: 166.3505 Fuel Consumption: 60.4622\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/all\\VITO_RW_Decade_Polo_BCN_City1.mat\n",
      "Total reward: -2145.9144184531337 SOC: 0.3293 Cumulative_SOC_deviation: 214.2447 Fuel Consumption: 3.4672\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_Decade_Octavia_BCN_City1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.434\n",
      "Episode: 11 Exploration P: 0.8550 Total reward: -5846.380078979266 SOC: 1.0000 Cumulative_SOC_deviation: 569.7794 Fuel Consumption: 148.5861\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_Decade_Jumper_MOL_City1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 117.287\n",
      "Episode: 12 Exploration P: 0.8218 Total reward: -7405.401632478222 SOC: 1.0000 Cumulative_SOC_deviation: 721.7848 Fuel Consumption: 187.5537\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_VH_Brussels_Empty_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 124.019\n",
      "Episode: 13 Exploration P: 0.7880 Total reward: -7714.047967786133 SOC: 1.0000 Cumulative_SOC_deviation: 752.7562 Fuel Consumption: 186.4863\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_TMB_Line24N_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 170.058\n",
      "Episode: 14 Exploration P: 0.7435 Total reward: -11262.04850250928 SOC: 1.0000 Cumulative_SOC_deviation: 1101.1544 Fuel Consumption: 250.5045\n",
      "\n",
      "../data/driving_cycles/all\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.254\n",
      "Episode: 15 Exploration P: 0.7281 Total reward: -3097.1392514860117 SOC: 1.0000 Cumulative_SOC_deviation: 300.7781 Fuel Consumption: 89.3587\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_Decade_Jumper_BCN_City1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 124.929\n",
      "Episode: 16 Exploration P: 0.7032 Total reward: -6276.697100814236 SOC: 1.0000 Cumulative_SOC_deviation: 613.7716 Fuel Consumption: 138.9809\n",
      "\n",
      "../data/driving_cycles/all\\nuremberg_r36.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.595\n",
      "Episode: 17 Exploration P: 0.6884 Total reward: -3553.1683763131214 SOC: 1.0000 Cumulative_SOC_deviation: 346.7837 Fuel Consumption: 85.3313\n",
      "\n",
      "../data/driving_cycles/all\\cudec_freeway.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 39.638\n",
      "Episode: 18 Exploration P: 0.6811 Total reward: -945.2199369428869 SOC: 0.3618 Cumulative_SOC_deviation: 89.9823 Fuel Consumption: 45.3973\n",
      "\n",
      "../data/driving_cycles/all\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.280\n",
      "Episode: 19 Exploration P: 0.6666 Total reward: -3589.8365809200377 SOC: 1.0000 Cumulative_SOC_deviation: 350.4650 Fuel Consumption: 85.1864\n",
      "\n",
      "../data/driving_cycles/all\\VITO_DUBDC.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.824\n",
      "Episode: 20 Exploration P: 0.6549 Total reward: -2493.071768939674 SOC: 1.0000 Cumulative_SOC_deviation: 242.6827 Fuel Consumption: 66.2450\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/all\\wvucity.mat\n",
      "Total reward: -1042.2947551629077 SOC: 0.4632 Cumulative_SOC_deviation: 103.8983 Fuel Consumption: 3.3120\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/all\\ny_city_traffic.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 43.792\n",
      "Episode: 21 Exploration P: 0.6472 Total reward: -1325.1948594825262 SOC: 1.0000 Cumulative_SOC_deviation: 128.3042 Fuel Consumption: 42.1527\n",
      "\n",
      "../data/driving_cycles/all\\07_manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.978\n",
      "Episode: 22 Exploration P: 0.6335 Total reward: -3497.45890344257 SOC: 1.0000 Cumulative_SOC_deviation: 341.6096 Fuel Consumption: 81.3624\n",
      "\n",
      "../data/driving_cycles/all\\03_nedc.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ValueCreatorSong\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\generalization3\\vehicle_model_DDPG7.py:251: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  del_i = (1 / (2 * r_cha)) * (v_cha - (v_cha ** 2 - 4 * r_cha * p_bat) ** (0.5)) * (p_bat < 0) + (1 / (\n",
      "C:\\Users\\ValueCreatorSong\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\generalization3\\vehicle_model_DDPG7.py:277: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  del_i = (1 / (2 * r_cha)) * (v_cha - (v_cha ** 2 - 4 * r_cha * p_bat) ** (0.5)) * (p_bat < 0) + (1 / (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOC is nan...\n",
      "elapsed_time: 84.394\n",
      "Episode: 23 Exploration P: 0.6197 Total reward: -13406.942759581212 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 84.2075\n",
      "\n",
      "../data/driving_cycles/all\\nuremberg_r36.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.684\n",
      "Episode: 24 Exploration P: 0.6067 Total reward: -3442.130141253733 SOC: 1.0000 Cumulative_SOC_deviation: 336.4599 Fuel Consumption: 77.5312\n",
      "\n",
      "../data/driving_cycles/all\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.160\n",
      "Episode: 25 Exploration P: 0.5905 Total reward: -2878.8164809060327 SOC: 1.0000 Cumulative_SOC_deviation: 278.5811 Fuel Consumption: 93.0053\n",
      "\n",
      "../data/driving_cycles/all\\ny_city_traffic.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 35.485\n",
      "Episode: 26 Exploration P: 0.5836 Total reward: -1321.9520378530135 SOC: 1.0000 Cumulative_SOC_deviation: 128.2345 Fuel Consumption: 39.6072\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_Antwerp1_May19c.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.699\n",
      "Episode: 27 Exploration P: 0.5720 Total reward: -3006.6598568090453 SOC: 1.0000 Cumulative_SOC_deviation: 293.8373 Fuel Consumption: 68.2869\n",
      "\n",
      "../data/driving_cycles/all\\cudec_freeway.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 32.225\n",
      "Episode: 28 Exploration P: 0.5659 Total reward: -1458.6156049143535 SOC: 0.2147 Cumulative_SOC_deviation: 142.2880 Fuel Consumption: 35.7355\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_VH_Brussels_Full_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 169.294\n",
      "Episode: 29 Exploration P: 0.5375 Total reward: -9223.232514826756 SOC: 1.0000 Cumulative_SOC_deviation: 905.9516 Fuel Consumption: 163.7166\n",
      "\n",
      "../data/driving_cycles/all\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.369\n",
      "Episode: 30 Exploration P: 0.5232 Total reward: -2200.1085981888928 SOC: 1.0000 Cumulative_SOC_deviation: 211.1536 Fuel Consumption: 88.5730\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/all\\VITO_RW_Kangoo_DePost_Brussels_101_1.mat\n",
      "Total reward: -10686.983191707897 SOC: 1.0000 Cumulative_SOC_deviation: 1028.0793 Fuel Consumption: 406.1904\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/all\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 113.659\n",
      "Episode: 31 Exploration P: 0.5093 Total reward: -4617.591960415932 SOC: 1.0000 Cumulative_SOC_deviation: 449.2512 Fuel Consumption: 125.0795\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_VH_Brussels_Empty_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 138.238\n",
      "Episode: 32 Exploration P: 0.4885 Total reward: -7149.827565083932 SOC: 1.0000 Cumulative_SOC_deviation: 696.8139 Fuel Consumption: 181.6888\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_VH_Brussels_Empty_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 136.275\n",
      "Episode: 33 Exploration P: 0.4686 Total reward: -8395.381401008703 SOC: 1.0000 Cumulative_SOC_deviation: 804.5069 Fuel Consumption: 350.3124\n",
      "\n",
      "../data/driving_cycles/all\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 87.044\n",
      "Episode: 34 Exploration P: 0.4562 Total reward: -5178.932984109168 SOC: 1.0000 Cumulative_SOC_deviation: 503.4881 Fuel Consumption: 144.0522\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_VH_Brussels_Full_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 163.268\n",
      "Episode: 35 Exploration P: 0.4334 Total reward: -10226.39398268569 SOC: 1.0000 Cumulative_SOC_deviation: 996.4221 Fuel Consumption: 262.1727\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_Decade_Octavia_BCN_City1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 100.489\n",
      "Episode: 36 Exploration P: 0.4200 Total reward: -5133.1121073132435 SOC: 1.0000 Cumulative_SOC_deviation: 495.8560 Fuel Consumption: 174.5522\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_Decade_Octavia_BCN_City1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 115.822\n",
      "Episode: 37 Exploration P: 0.4069 Total reward: -5832.950645683424 SOC: 1.0000 Cumulative_SOC_deviation: 562.0701 Fuel Consumption: 212.2495\n",
      "\n",
      "../data/driving_cycles/all\\VITO_DUBDC.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 55.237\n",
      "Episode: 38 Exploration P: 0.3999 Total reward: -1512.8005591581477 SOC: 0.9551 Cumulative_SOC_deviation: 146.6276 Fuel Consumption: 46.5251\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_TMB_Line24N_1.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 182.254\n",
      "Episode: 39 Exploration P: 0.3776 Total reward: -10629.463655878313 SOC: 1.0000 Cumulative_SOC_deviation: 1035.2341 Fuel Consumption: 277.1227\n",
      "\n",
      "../data/driving_cycles/all\\VITO_MOLCity.mat\n",
      "Available condition is not avail... SOC: 0.9908123872703952\n",
      "elapsed_time: 86.680\n",
      "Episode: 40 Exploration P: 0.3673 Total reward: -3765.695062068803 SOC: 0.9908 Cumulative_SOC_deviation: 369.6345 Fuel Consumption: 69.3498\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/all\\VITO_RW_Kangoo_DePost_Brussels_101_1.mat\n",
      "Total reward: -334.98425020054606 SOC: 0.5802 Cumulative_SOC_deviation: 31.2656 Fuel Consumption: 22.3286\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/all\\VITO_RW_BUS_TMB_Line24N_1.mat\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 3\n",
    "results_dict = {} \n",
    "driving_cycle_paths = glob.glob(\"../data/driving_cycles/all/*.mat\")\n",
    "# driving_cycle_paths.pop(1)\n",
    "driving_cycle_paths = driving_cycle_paths[:20]\n",
    "\n",
    "for trial in range(num_trials): \n",
    "    print(\"\")\n",
    "    print(\"Trial {}\".format(trial))\n",
    "    print(\"\")\n",
    "    \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer = initialization()\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    for ep in range(total_episodes): \n",
    "        driving_cycle_path = np.random.choice(driving_cycle_paths)\n",
    "        print(driving_cycle_path)\n",
    "        env = initialization_env(driving_cycle_path, 10)\n",
    "        \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * (steps\n",
    "                                                                        -DELAY_TRAINING))\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "        )\n",
    "        print(\"\")\n",
    "        \n",
    "        if (ep + 1) % 10 == 0: \n",
    "            test_agent(actor_model, 10, 20)\n",
    "    \n",
    "    root = \"DDPG7_trial{}\".format(trial+1)\n",
    "    save_weights(actor_model, critic_model, target_actor, target_critic, root)\n",
    "    \n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG7.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import glob\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG5 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):      \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    inputs_batchnorm = layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs_batchnorm)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_input_batchnorm = layers.BatchNormalization()(state_input)\n",
    "    \n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input_batchnorm)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 500\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 10000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(actor_model, critic_model, target_actor, target_critic, root): \n",
    "    actor_model.save_weights(\"./{}/actor_model_checkpoint\".format(root))\n",
    "    critic_model.save_weights(\"./{}/critic_model_checkpoint\".format(root))\n",
    "    target_actor.save_weights(\"./{}/target_actor_checkpoint\".format(root))\n",
    "    target_critic.save_weights(\"./{}/target_critic_checkpoint\".format(root))\n",
    "    print(\"model is saved..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_env(driving_path, reward_factor):\n",
    "    env = Environment(cell_model, driving_path, battery_path, motor_path, reward_factor)\n",
    "    return env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(actor_model, reward_factor, test_path_start):\n",
    "    test_cycles = glob.glob(\"../data/driving_cycles/all/*.mat\")[test_path_start:]\n",
    "    test_cycle = np.random.choice(test_cycles)\n",
    "    env = initialization_env(test_cycle, reward_factor)\n",
    "    \n",
    "    total_reward = 0\n",
    "    state = env.reset() \n",
    "    while True: \n",
    "        tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "        action = policy_epsilon_greedy(tf_state, -1)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        state = next_state \n",
    "        total_reward += reward \n",
    "        \n",
    "        if done: \n",
    "            break \n",
    "        \n",
    "    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "    \n",
    "    print(\"******************* Test is start *****************\")\n",
    "    print(test_cycle)\n",
    "    print('Total reward: {}'.format(total_reward), \n",
    "          \"SOC: {:.4f}\".format(env.SOC), \n",
    "          \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "          \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption))\n",
    "    print(\"******************* Test is done *****************\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Trial 0\n",
      "\n",
      "../data/driving_cycles/all\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 12.007\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -3619.2437438706634 SOC: 1.0000 Cumulative_SOC_deviation: 349.9006 Fuel Consumption: 120.2376\n",
      "\n",
      "../data/driving_cycles/all\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 12.445\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -3652.6010647017374 SOC: 1.0000 Cumulative_SOC_deviation: 353.4929 Fuel Consumption: 117.6720\n",
      "\n",
      "../data/driving_cycles/all\\nuremberg_r36.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 13.558\n",
      "Episode: 3 Exploration P: 1.0000 Total reward: -3816.9834511463605 SOC: 1.0000 Cumulative_SOC_deviation: 370.0865 Fuel Consumption: 116.1182\n",
      "\n",
      "../data/driving_cycles/all\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 13.783\n",
      "Episode: 4 Exploration P: 1.0000 Total reward: -3873.980515913234 SOC: 1.0000 Cumulative_SOC_deviation: 375.6801 Fuel Consumption: 117.1799\n",
      "\n",
      "../data/driving_cycles/all\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 13.776\n",
      "Episode: 5 Exploration P: 1.0000 Total reward: -3886.885409514541 SOC: 1.0000 Cumulative_SOC_deviation: 376.5522 Fuel Consumption: 121.3630\n",
      "\n",
      "../data/driving_cycles/all\\03_nedc.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 14.630\n",
      "Episode: 6 Exploration P: 1.0000 Total reward: -4063.587424491529 SOC: 0.8621 Cumulative_SOC_deviation: 393.1421 Fuel Consumption: 132.1667\n",
      "\n",
      "../data/driving_cycles/all\\nuremberg_r36.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 13.590\n",
      "Episode: 7 Exploration P: 1.0000 Total reward: -3853.5256102933777 SOC: 1.0000 Cumulative_SOC_deviation: 373.3500 Fuel Consumption: 120.0255\n",
      "\n",
      "../data/driving_cycles/all\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 19.545\n",
      "Episode: 8 Exploration P: 1.0000 Total reward: -4701.19992874958 SOC: 1.0000 Cumulative_SOC_deviation: 454.7896 Fuel Consumption: 153.3035\n",
      "\n",
      "../data/driving_cycles/all\\manhattan.mat\n",
      "WARNING:tensorflow:Layer batch_normalization_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 20.584\n",
      "Episode: 9 Exploration P: 0.9978 Total reward: -3861.849170370821 SOC: 1.0000 Cumulative_SOC_deviation: 374.2259 Fuel Consumption: 119.5904\n",
      "\n",
      "../data/driving_cycles/all\\03_nedc.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.361\n",
      "Episode: 10 Exploration P: 0.9748 Total reward: -4098.04044199879 SOC: 0.8885 Cumulative_SOC_deviation: 396.3416 Fuel Consumption: 134.6245\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/all\\VITO_RW_Decade_Polo_BCN_City1.mat\n",
      "Total reward: -2145.9144184531337 SOC: 0.3293 Cumulative_SOC_deviation: 214.2447 Fuel Consumption: 3.4672\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/all\\ny_city_composite_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.495\n",
      "Episode: 11 Exploration P: 0.9551 Total reward: -3638.2912086983147 SOC: 1.0000 Cumulative_SOC_deviation: 352.6240 Fuel Consumption: 112.0514\n",
      "\n",
      "../data/driving_cycles/all\\nuremberg_r36.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 93.945\n",
      "Episode: 12 Exploration P: 0.9349 Total reward: -3864.5382967285823 SOC: 1.0000 Cumulative_SOC_deviation: 374.8845 Fuel Consumption: 115.6930\n",
      "\n",
      "../data/driving_cycles/all\\07_manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.755\n",
      "Episode: 13 Exploration P: 0.9149 Total reward: -3867.538317055164 SOC: 1.0000 Cumulative_SOC_deviation: 375.5975 Fuel Consumption: 111.5632\n",
      "\n",
      "../data/driving_cycles/all\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.303\n",
      "Episode: 14 Exploration P: 0.8954 Total reward: -3869.520608583914 SOC: 1.0000 Cumulative_SOC_deviation: 376.0305 Fuel Consumption: 109.2160\n",
      "\n",
      "../data/driving_cycles/all\\03_nedc.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 72.739\n",
      "Episode: 15 Exploration P: 0.8748 Total reward: -3978.0886842723253 SOC: 0.8548 Cumulative_SOC_deviation: 385.9669 Fuel Consumption: 118.4195\n",
      "\n",
      "../data/driving_cycles/all\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 63.149\n",
      "Episode: 16 Exploration P: 0.8561 Total reward: -3808.2511143032843 SOC: 1.0000 Cumulative_SOC_deviation: 370.5043 Fuel Consumption: 103.2086\n",
      "\n",
      "../data/driving_cycles/all\\03_nedc.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 69.752\n",
      "Episode: 17 Exploration P: 0.8364 Total reward: -3982.9616734862766 SOC: 0.8614 Cumulative_SOC_deviation: 386.3850 Fuel Consumption: 119.1119\n",
      "\n",
      "../data/driving_cycles/all\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 110.982\n",
      "Episode: 18 Exploration P: 0.8059 Total reward: -6144.826654488832 SOC: 1.0000 Cumulative_SOC_deviation: 597.2795 Fuel Consumption: 172.0318\n",
      "\n",
      "../data/driving_cycles/all\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 109.925\n",
      "Episode: 19 Exploration P: 0.7766 Total reward: -6105.426925320082 SOC: 1.0000 Cumulative_SOC_deviation: 593.4292 Fuel Consumption: 171.1354\n",
      "\n",
      "../data/driving_cycles/all\\ny_city_traffic.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 35.310\n",
      "Episode: 20 Exploration P: 0.7675 Total reward: -1680.573208409089 SOC: 1.0000 Cumulative_SOC_deviation: 162.8147 Fuel Consumption: 52.4265\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/all\\VITO_RW_Decade_Octavia_BCN_City1.mat\n",
      "Total reward: -1602.5559801988218 SOC: 0.4121 Cumulative_SOC_deviation: 159.8760 Fuel Consumption: 3.7963\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/all\\01_FTP72_fuds.mat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.146\n",
      "Episode: 21 Exploration P: 0.7470 Total reward: -3961.104532307303 SOC: 1.0000 Cumulative_SOC_deviation: 384.0754 Fuel Consumption: 120.3504\n",
      "\n",
      "../data/driving_cycles/all\\cudec_freeway.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 31.888\n",
      "Episode: 22 Exploration P: 0.7391 Total reward: -802.3658310414455 SOC: 0.4122 Cumulative_SOC_deviation: 75.4196 Fuel Consumption: 48.1698\n",
      "\n",
      "../data/driving_cycles/all\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.614\n",
      "Episode: 23 Exploration P: 0.7233 Total reward: -3627.827230185816 SOC: 1.0000 Cumulative_SOC_deviation: 353.9179 Fuel Consumption: 88.6483\n",
      "\n",
      "../data/driving_cycles/all\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.308\n",
      "Episode: 24 Exploration P: 0.7079 Total reward: -3662.396887202895 SOC: 1.0000 Cumulative_SOC_deviation: 357.1430 Fuel Consumption: 90.9668\n",
      "\n",
      "../data/driving_cycles/all\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 101.617\n",
      "Episode: 25 Exploration P: 0.6890 Total reward: -3661.901128904188 SOC: 1.0000 Cumulative_SOC_deviation: 355.1284 Fuel Consumption: 110.6167\n",
      "\n",
      "../data/driving_cycles/all\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 139.067\n",
      "Episode: 26 Exploration P: 0.6641 Total reward: -5327.863350761514 SOC: 1.0000 Cumulative_SOC_deviation: 518.6458 Fuel Consumption: 141.4057\n",
      "\n",
      "../data/driving_cycles/all\\nuremberg_r36.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.765\n",
      "Episode: 27 Exploration P: 0.6500 Total reward: -3562.344676049195 SOC: 1.0000 Cumulative_SOC_deviation: 347.9932 Fuel Consumption: 82.4124\n",
      "\n",
      "../data/driving_cycles/all\\ny_city_traffic.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 44.693\n",
      "Episode: 28 Exploration P: 0.6424 Total reward: -1445.098002722632 SOC: 1.0000 Cumulative_SOC_deviation: 140.1877 Fuel Consumption: 43.2215\n",
      "\n",
      "../data/driving_cycles/all\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 102.160\n",
      "Episode: 29 Exploration P: 0.6253 Total reward: -3290.695102631065 SOC: 1.0000 Cumulative_SOC_deviation: 319.0878 Fuel Consumption: 99.8172\n",
      "\n",
      "../data/driving_cycles/all\\ny_city_composite_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.237\n",
      "Episode: 30 Exploration P: 0.6128 Total reward: -3342.8610647154364 SOC: 1.0000 Cumulative_SOC_deviation: 326.6294 Fuel Consumption: 76.5672\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/all\\VITO_RW_Decade_Jumper_MOL_City1.mat\n",
      "Total reward: -3068.5014844299676 SOC: 0.3091 Cumulative_SOC_deviation: 306.3793 Fuel Consumption: 4.7083\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/all\\ny_city_traffic.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 35.946\n",
      "Episode: 31 Exploration P: 0.6056 Total reward: -1509.4448871903342 SOC: 1.0000 Cumulative_SOC_deviation: 146.4324 Fuel Consumption: 45.1205\n",
      "\n",
      "../data/driving_cycles/all\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.554\n",
      "Episode: 32 Exploration P: 0.5894 Total reward: -3054.523652041105 SOC: 1.0000 Cumulative_SOC_deviation: 296.0214 Fuel Consumption: 94.3097\n",
      "\n",
      "../data/driving_cycles/all\\ny_city_composite_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.568\n",
      "Episode: 33 Exploration P: 0.5776 Total reward: -3240.7185827509556 SOC: 1.0000 Cumulative_SOC_deviation: 317.0960 Fuel Consumption: 69.7582\n",
      "\n",
      "../data/driving_cycles/all\\ny_city_composite_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.863\n",
      "Episode: 34 Exploration P: 0.5661 Total reward: -3165.965628087262 SOC: 1.0000 Cumulative_SOC_deviation: 310.1135 Fuel Consumption: 64.8310\n",
      "\n",
      "../data/driving_cycles/all\\03_nedc.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ValueCreatorSong\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\generalization3\\vehicle_model_DDPG5.py:251: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  del_i = (1 / (2 * r_cha)) * (v_cha - (v_cha ** 2 - 4 * r_cha * p_bat) ** (0.5)) * (p_bat < 0) + (1 / (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.467\n",
      "Episode: 35 Exploration P: 0.5531 Total reward: -3226.0854524339843 SOC: 0.7186 Cumulative_SOC_deviation: 315.0324 Fuel Consumption: 75.7614\n",
      "\n",
      "../data/driving_cycles/all\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.796\n",
      "Episode: 36 Exploration P: 0.5331 Total reward: -4738.226947371234 SOC: 0.9722 Cumulative_SOC_deviation: 461.0477 Fuel Consumption: 127.7504\n",
      "\n",
      "../data/driving_cycles/all\\cudec_freeway.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 30.998\n",
      "Episode: 37 Exploration P: 0.5274 Total reward: -1316.9772167451436 SOC: 0.2389 Cumulative_SOC_deviation: 128.0506 Fuel Consumption: 36.4710\n",
      "\n",
      "../data/driving_cycles/all\\ny_city_composite_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.655\n",
      "Episode: 38 Exploration P: 0.5169 Total reward: -3068.2994572608613 SOC: 1.0000 Cumulative_SOC_deviation: 300.6701 Fuel Consumption: 61.5989\n",
      "\n",
      "../data/driving_cycles/all\\nuremberg_r36.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 86.048\n",
      "Episode: 39 Exploration P: 0.5060 Total reward: -3252.5086121052673 SOC: 1.0000 Cumulative_SOC_deviation: 318.5691 Fuel Consumption: 66.8178\n",
      "\n",
      "../data/driving_cycles/all\\03_nedc.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.468\n",
      "Episode: 40 Exploration P: 0.4945 Total reward: -2930.14824050949 SOC: 0.6801 Cumulative_SOC_deviation: 286.1362 Fuel Consumption: 68.7858\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/all\\VITO_RW_Decade_Octavia_MOL_City1.mat\n",
      "Total reward: -2906.186628233317 SOC: 0.1951 Cumulative_SOC_deviation: 290.2642 Fuel Consumption: 3.5447\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/all\\07_manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.965\n",
      "Episode: 41 Exploration P: 0.4840 Total reward: -3155.7898454711276 SOC: 1.0000 Cumulative_SOC_deviation: 309.4240 Fuel Consumption: 61.5503\n",
      "\n",
      "../data/driving_cycles/all\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 70.376\n",
      "Episode: 42 Exploration P: 0.4741 Total reward: -1606.6233862129611 SOC: 0.8124 Cumulative_SOC_deviation: 154.3891 Fuel Consumption: 62.7319\n",
      "\n",
      "../data/driving_cycles/all\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 69.109\n",
      "Episode: 43 Exploration P: 0.4641 Total reward: -3004.8071664489044 SOC: 1.0000 Cumulative_SOC_deviation: 294.8917 Fuel Consumption: 55.8905\n",
      "\n",
      "../data/driving_cycles/all\\ny_city_composite_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 62.712\n",
      "Episode: 44 Exploration P: 0.4548 Total reward: -2933.080410350145 SOC: 1.0000 Cumulative_SOC_deviation: 287.7633 Fuel Consumption: 55.4478\n",
      "\n",
      "../data/driving_cycles/all\\07_manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 69.855\n",
      "Episode: 45 Exploration P: 0.4452 Total reward: -2844.3755988182047 SOC: 1.0000 Cumulative_SOC_deviation: 278.9298 Fuel Consumption: 55.0775\n",
      "\n",
      "../data/driving_cycles/all\\07_manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 69.858\n",
      "Episode: 46 Exploration P: 0.4358 Total reward: -2857.1506101068367 SOC: 1.0000 Cumulative_SOC_deviation: 280.1651 Fuel Consumption: 55.4998\n",
      "\n",
      "../data/driving_cycles/all\\07_manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.250\n",
      "Episode: 47 Exploration P: 0.4266 Total reward: -2864.959748077073 SOC: 1.0000 Cumulative_SOC_deviation: 281.0701 Fuel Consumption: 54.2587\n",
      "\n",
      "../data/driving_cycles/all\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.630\n",
      "Episode: 48 Exploration P: 0.4154 Total reward: -1236.7256698100455 SOC: 0.8683 Cumulative_SOC_deviation: 116.8103 Fuel Consumption: 68.6223\n",
      "\n",
      "../data/driving_cycles/all\\nuremberg_r36.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.626\n",
      "Episode: 49 Exploration P: 0.4067 Total reward: -2705.9902342699866 SOC: 1.0000 Cumulative_SOC_deviation: 265.5784 Fuel Consumption: 50.2062\n",
      "\n",
      "../data/driving_cycles/all\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 116.520\n",
      "Episode: 50 Exploration P: 0.3921 Total reward: -1772.2444925935652 SOC: 0.7424 Cumulative_SOC_deviation: 168.3957 Fuel Consumption: 88.2871\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/all\\wvucity.mat\n",
      "Total reward: -1042.2947551629077 SOC: 0.4632 Cumulative_SOC_deviation: 103.8983 Fuel Consumption: 3.3120\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/all\\07_manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 68.702\n",
      "Episode: 51 Exploration P: 0.3838 Total reward: -2680.7015925172495 SOC: 1.0000 Cumulative_SOC_deviation: 263.1738 Fuel Consumption: 48.9638\n",
      "\n",
      "../data/driving_cycles/all\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.382\n",
      "Episode: 52 Exploration P: 0.3758 Total reward: -2926.5044224903004 SOC: 1.0000 Cumulative_SOC_deviation: 287.2952 Fuel Consumption: 53.5519\n",
      "\n",
      "../data/driving_cycles/all\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 139.007\n",
      "Episode: 53 Exploration P: 0.3623 Total reward: -1836.311173438156 SOC: 0.7108 Cumulative_SOC_deviation: 175.0276 Fuel Consumption: 86.0347\n",
      "\n",
      "../data/driving_cycles/all\\FTP_75_cycle.mat\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 3\n",
    "results_dict = {} \n",
    "driving_cycle_paths = glob.glob(\"../data/driving_cycles/all/*.mat\")\n",
    "driving_cycle_paths = driving_cycle_paths[:10]\n",
    "\n",
    "for trial in range(num_trials): \n",
    "    print(\"\")\n",
    "    print(\"Trial {}\".format(trial))\n",
    "    print(\"\")\n",
    "    \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer = initialization()\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    for ep in range(total_episodes): \n",
    "        driving_cycle_path = np.random.choice(driving_cycle_paths)\n",
    "        print(driving_cycle_path)\n",
    "        env = initialization_env(driving_cycle_path, 10)\n",
    "        \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * (steps\n",
    "                                                                        -DELAY_TRAINING))\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "        )\n",
    "        print(\"\")\n",
    "        \n",
    "        if (ep + 1) % 10 == 0: \n",
    "            test_agent(actor_model, 10, 10)\n",
    "    \n",
    "    root = \"DDPG5_trial{}\".format(trial+1)\n",
    "    save_weights(actor_model, critic_model, target_actor, target_critic, root)\n",
    "    \n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG5.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import glob\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG2 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):      \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    inputs_batchnorm = layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs_batchnorm)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_input_batchnorm = layers.BatchNormalization()(state_input)\n",
    "    \n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input_batchnorm)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 500\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 10000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(actor_model, critic_model, target_actor, target_critic, root): \n",
    "    actor_model.save_weights(\"./{}/actor_model_checkpoint\".format(root))\n",
    "    critic_model.save_weights(\"./{}/critic_model_checkpoint\".format(root))\n",
    "    target_actor.save_weights(\"./{}/target_actor_checkpoint\".format(root))\n",
    "    target_critic.save_weights(\"./{}/target_critic_checkpoint\".format(root))\n",
    "    print(\"model is saved..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_env(driving_path, reward_factor):\n",
    "    env = Environment(cell_model, driving_path, battery_path, motor_path, reward_factor)\n",
    "    return env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Trial 0\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 13.317\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -1022.2352612725438 SOC: 0.6450 Cumulative_SOC_deviation: 97.5508 Fuel Consumption: 46.7276\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 12.945\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -1001.4999390900024 SOC: 0.6425 Cumulative_SOC_deviation: 95.4898 Fuel Consumption: 46.6024\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 16.110\n",
      "Episode: 3 Exploration P: 1.0000 Total reward: -970.2508685079604 SOC: 0.8026 Cumulative_SOC_deviation: 91.0384 Fuel Consumption: 59.8666\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 16.244\n",
      "Episode: 4 Exploration P: 1.0000 Total reward: -1057.17246064962 SOC: 0.8180 Cumulative_SOC_deviation: 99.6131 Fuel Consumption: 61.0413\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 15.750\n",
      "Episode: 5 Exploration P: 1.0000 Total reward: -1152.0332097525652 SOC: 0.8294 Cumulative_SOC_deviation: 109.0164 Fuel Consumption: 61.8694\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 12.740\n",
      "Episode: 6 Exploration P: 1.0000 Total reward: -1023.6040393021952 SOC: 0.6515 Cumulative_SOC_deviation: 97.6434 Fuel Consumption: 47.1705\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 17.106\n",
      "Episode: 7 Exploration P: 1.0000 Total reward: -1147.0205438137468 SOC: 0.8444 Cumulative_SOC_deviation: 108.4073 Fuel Consumption: 62.9478\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "WARNING:tensorflow:Layer batch_normalization_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 19.042\n",
      "Episode: 8 Exploration P: 0.8198 Total reward: -946.3459373489645 SOC: 0.7938 Cumulative_SOC_deviation: 88.7222 Fuel Consumption: 59.1243\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.919\n",
      "Episode: 9 Exploration P: 0.8028 Total reward: -1099.2220151433576 SOC: 0.5384 Cumulative_SOC_deviation: 106.0048 Fuel Consumption: 39.1738\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.545\n",
      "Episode: 10 Exploration P: 0.7813 Total reward: -1002.5047749837886 SOC: 0.6318 Cumulative_SOC_deviation: 95.5011 Fuel Consumption: 47.4935\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.574\n",
      "Episode: 11 Exploration P: 0.7651 Total reward: -1106.3762605727743 SOC: 0.5083 Cumulative_SOC_deviation: 106.9349 Fuel Consumption: 37.0269\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 93.991\n",
      "Episode: 12 Exploration P: 0.7447 Total reward: -1005.7883222288274 SOC: 0.6292 Cumulative_SOC_deviation: 95.8723 Fuel Consumption: 47.0655\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.172\n",
      "Episode: 13 Exploration P: 0.7292 Total reward: -1183.9178952775994 SOC: 0.4760 Cumulative_SOC_deviation: 114.9115 Fuel Consumption: 34.8033\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 105.921\n",
      "Episode: 14 Exploration P: 0.7098 Total reward: -1314.153809480555 SOC: 0.5894 Cumulative_SOC_deviation: 126.9652 Fuel Consumption: 44.5018\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.290\n",
      "Episode: 15 Exploration P: 0.6951 Total reward: -1224.508858546 SOC: 0.4429 Cumulative_SOC_deviation: 119.2196 Fuel Consumption: 32.3133\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.880\n",
      "Episode: 16 Exploration P: 0.6807 Total reward: -1198.3784330902918 SOC: 0.4417 Cumulative_SOC_deviation: 116.5943 Fuel Consumption: 32.4356\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 84.922\n",
      "Episode: 17 Exploration P: 0.6666 Total reward: -1271.181170103765 SOC: 0.4196 Cumulative_SOC_deviation: 124.0234 Fuel Consumption: 30.9469\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.643\n",
      "Episode: 18 Exploration P: 0.6528 Total reward: -1221.9353093572674 SOC: 0.4350 Cumulative_SOC_deviation: 118.9934 Fuel Consumption: 32.0010\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 105.968\n",
      "Episode: 19 Exploration P: 0.6354 Total reward: -1770.7355981873768 SOC: 0.5212 Cumulative_SOC_deviation: 173.1315 Fuel Consumption: 39.4203\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.733\n",
      "Episode: 20 Exploration P: 0.6185 Total reward: -1917.9914938017098 SOC: 0.4947 Cumulative_SOC_deviation: 188.0519 Fuel Consumption: 37.4724\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 111.107\n",
      "Episode: 21 Exploration P: 0.6020 Total reward: -1917.5698272784364 SOC: 0.5024 Cumulative_SOC_deviation: 187.9465 Fuel Consumption: 38.1053\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 69.286\n",
      "Episode: 22 Exploration P: 0.5895 Total reward: -1254.1559535287938 SOC: 0.4090 Cumulative_SOC_deviation: 122.4089 Fuel Consumption: 30.0674\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 97.239\n",
      "Episode: 23 Exploration P: 0.5738 Total reward: -2156.5922174308666 SOC: 0.4659 Cumulative_SOC_deviation: 212.1167 Fuel Consumption: 35.4252\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 93.622\n",
      "Episode: 24 Exploration P: 0.5586 Total reward: -1966.8556344717158 SOC: 0.4751 Cumulative_SOC_deviation: 193.0715 Fuel Consumption: 36.1404\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.067\n",
      "Episode: 25 Exploration P: 0.5437 Total reward: -2065.527875695558 SOC: 0.4674 Cumulative_SOC_deviation: 202.9915 Fuel Consumption: 35.6125\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 68.904\n",
      "Episode: 26 Exploration P: 0.5325 Total reward: -1360.018404903133 SOC: 0.3461 Cumulative_SOC_deviation: 133.4219 Fuel Consumption: 25.7997\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.347\n",
      "Episode: 27 Exploration P: 0.5215 Total reward: -1318.9294510525724 SOC: 0.3673 Cumulative_SOC_deviation: 129.1506 Fuel Consumption: 27.4237\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 101.638\n",
      "Episode: 28 Exploration P: 0.5077 Total reward: -2102.7666135795353 SOC: 0.4560 Cumulative_SOC_deviation: 206.8152 Fuel Consumption: 34.6144\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.234\n",
      "Episode: 29 Exploration P: 0.4942 Total reward: -1454.088563689806 SOC: 0.6220 Cumulative_SOC_deviation: 140.6542 Fuel Consumption: 47.5468\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 64.873\n",
      "Episode: 30 Exploration P: 0.4840 Total reward: -747.5941772443917 SOC: 0.6003 Cumulative_SOC_deviation: 70.3653 Fuel Consumption: 43.9407\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.430\n",
      "Episode: 31 Exploration P: 0.4741 Total reward: -739.1731383897611 SOC: 0.5974 Cumulative_SOC_deviation: 69.5508 Fuel Consumption: 43.6651\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 98.793\n",
      "Episode: 32 Exploration P: 0.4615 Total reward: -380.87168729697754 SOC: 0.6327 Cumulative_SOC_deviation: 33.3700 Fuel Consumption: 47.1713\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 88.459\n",
      "Episode: 33 Exploration P: 0.4493 Total reward: -396.3760384419848 SOC: 0.6270 Cumulative_SOC_deviation: 34.9609 Fuel Consumption: 46.7674\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.401\n",
      "Episode: 34 Exploration P: 0.4374 Total reward: -336.7740901651644 SOC: 0.6271 Cumulative_SOC_deviation: 29.0137 Fuel Consumption: 46.6375\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.423\n",
      "Episode: 35 Exploration P: 0.4284 Total reward: -716.5986521594533 SOC: 0.6006 Cumulative_SOC_deviation: 67.2751 Fuel Consumption: 43.8473\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 97.740\n",
      "Episode: 36 Exploration P: 0.4171 Total reward: -350.3050512606544 SOC: 0.6452 Cumulative_SOC_deviation: 30.2078 Fuel Consumption: 48.2275\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.880\n",
      "Episode: 37 Exploration P: 0.4085 Total reward: -670.1100889674908 SOC: 0.6064 Cumulative_SOC_deviation: 62.5808 Fuel Consumption: 44.3025\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.862\n",
      "Episode: 38 Exploration P: 0.3977 Total reward: -345.36724606623113 SOC: 0.6333 Cumulative_SOC_deviation: 29.8024 Fuel Consumption: 47.3432\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.455\n",
      "Episode: 39 Exploration P: 0.3896 Total reward: -659.7077228931951 SOC: 0.6017 Cumulative_SOC_deviation: 61.5867 Fuel Consumption: 43.8412\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.722\n",
      "Episode: 40 Exploration P: 0.3793 Total reward: -317.5849754738554 SOC: 0.6272 Cumulative_SOC_deviation: 27.0625 Fuel Consumption: 46.9605\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.813\n",
      "Episode: 41 Exploration P: 0.3716 Total reward: -661.9556791083163 SOC: 0.6018 Cumulative_SOC_deviation: 61.7946 Fuel Consumption: 44.0097\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.295\n",
      "Episode: 42 Exploration P: 0.3618 Total reward: -327.3689837227867 SOC: 0.6174 Cumulative_SOC_deviation: 28.1129 Fuel Consumption: 46.2402\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 101.405\n",
      "Episode: 43 Exploration P: 0.3522 Total reward: -311.29501616209194 SOC: 0.6164 Cumulative_SOC_deviation: 26.5185 Fuel Consumption: 46.1099\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 109.481\n",
      "Episode: 44 Exploration P: 0.3430 Total reward: -293.56788375035535 SOC: 0.6108 Cumulative_SOC_deviation: 24.8000 Fuel Consumption: 45.5677\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 95.007\n",
      "Episode: 45 Exploration P: 0.3339 Total reward: -277.42522076266897 SOC: 0.6141 Cumulative_SOC_deviation: 23.1708 Fuel Consumption: 45.7177\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 95.469\n",
      "Episode: 46 Exploration P: 0.3252 Total reward: -297.0258404050282 SOC: 0.6071 Cumulative_SOC_deviation: 25.1702 Fuel Consumption: 45.3234\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.904\n",
      "Episode: 47 Exploration P: 0.3186 Total reward: -580.3940851801513 SOC: 0.5969 Cumulative_SOC_deviation: 53.7085 Fuel Consumption: 43.3089\n",
      "\n",
      "training\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 72.695\n",
      "Episode: 48 Exploration P: 0.3121 Total reward: -606.6710885869566 SOC: 0.6040 Cumulative_SOC_deviation: 56.2726 Fuel Consumption: 43.9447\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 94.313\n",
      "Episode: 49 Exploration P: 0.3039 Total reward: -294.24035089800816 SOC: 0.6152 Cumulative_SOC_deviation: 24.8416 Fuel Consumption: 45.8246\n",
      "\n",
      "training\\01_FTP72_fuds.mat\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 3\n",
    "results_dict = {} \n",
    "driving_cycle_paths = glob.glob(\"training/*.mat\")\n",
    "driving_cycle_paths.pop(1)\n",
    "driving_cycle_paths = driving_cycle_paths[:2]\n",
    "\n",
    "for trial in range(num_trials): \n",
    "    print(\"\")\n",
    "    print(\"Trial {}\".format(trial))\n",
    "    print(\"\")\n",
    "    \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer = initialization()\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    for ep in range(total_episodes): \n",
    "        driving_cycle_path = np.random.choice(driving_cycle_paths)\n",
    "        print(driving_cycle_path)\n",
    "        env = initialization_env(driving_cycle_path, 10)\n",
    "        \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * (steps\n",
    "                                                                        -DELAY_TRAINING))\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "        )\n",
    "        print(\"\")\n",
    "    \n",
    "    root = \"DDPG2_trial{}\".format(trial+1)\n",
    "    save_weights(actor_model, critic_model, target_actor, target_critic, root)\n",
    "    \n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG2.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

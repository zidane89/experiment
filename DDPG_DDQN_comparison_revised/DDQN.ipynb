{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "        self.power_mean = 0 \n",
    "        self.power_std = 0 \n",
    "        self.sum = 0 \n",
    "        self.sum_deviation = 0 \n",
    "        self.N = 0 \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self.N += 1 \n",
    "        power = sample[0][0]\n",
    "        self.sum += power \n",
    "        self.power_mean = self.sum / self.N \n",
    "        self.sum_deviation += (power - self.power_mean) ** 2\n",
    "        self.power_std = np.sqrt(self.sum_deviation / self.N)\n",
    "        \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    states[:, 0] = (states[:, 0] - memory.power_mean) / memory.power_std \n",
    "    next_states[:, 0] = (next_states[:, 0] - memory.power_mean) / memory.power_std\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -923.6689841056224 Explore P: 0.9217 SOC: 0.7945 Cumulative_SOC_deviation: 86.3274 Fuel Consumption: 60.3952 Mean: 2.1068, STD: 5.0145\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -875.6953171571973 Explore P: 0.8970 SOC: 0.7666 Cumulative_SOC_deviation: 81.7734 Fuel Consumption: 57.9611 Mean: 2.1068, STD: 5.0153\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -866.0237938165045 Explore P: 0.8730 SOC: 0.7688 Cumulative_SOC_deviation: 80.7921 Fuel Consumption: 58.1025 Mean: 2.1068, STD: 5.0157\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -975.3668736785567 Explore P: 0.8496 SOC: 0.7979 Cumulative_SOC_deviation: 91.4867 Fuel Consumption: 60.4996 Mean: 2.1068, STD: 5.0161\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -851.8335094367072 Explore P: 0.8269 SOC: 0.7632 Cumulative_SOC_deviation: 79.4266 Fuel Consumption: 57.5678 Mean: 2.1068, STD: 5.0163\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -806.8022786145368 Explore P: 0.8048 SOC: 0.7585 Cumulative_SOC_deviation: 74.9485 Fuel Consumption: 57.3175 Mean: 2.1068, STD: 5.0165\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -905.735569946517 Explore P: 0.7832 SOC: 0.7666 Cumulative_SOC_deviation: 84.7645 Fuel Consumption: 58.0908 Mean: 2.1068, STD: 5.0167\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -891.7392344034062 Explore P: 0.7623 SOC: 0.7759 Cumulative_SOC_deviation: 83.3189 Fuel Consumption: 58.5505 Mean: 2.1068, STD: 5.0168\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -906.7970612164722 Explore P: 0.7419 SOC: 0.7767 Cumulative_SOC_deviation: 84.8312 Fuel Consumption: 58.4851 Mean: 2.1068, STD: 5.0169\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -737.1471013285457 Explore P: 0.7221 SOC: 0.7166 Cumulative_SOC_deviation: 68.3186 Fuel Consumption: 53.9611 Mean: 2.1068, STD: 5.0170\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -763.78687489009 Explore P: 0.7028 SOC: 0.7239 Cumulative_SOC_deviation: 70.9418 Fuel Consumption: 54.3685 Mean: 2.1068, STD: 5.0171\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -1003.3350930581526 Explore P: 0.6840 SOC: 0.8072 Cumulative_SOC_deviation: 94.2287 Fuel Consumption: 61.0479 Mean: 2.1068, STD: 5.0171\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -831.4043012425293 Explore P: 0.6658 SOC: 0.7610 Cumulative_SOC_deviation: 77.3834 Fuel Consumption: 57.5699 Mean: 2.1068, STD: 5.0172\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -838.510300971168 Explore P: 0.6480 SOC: 0.7584 Cumulative_SOC_deviation: 78.1494 Fuel Consumption: 57.0160 Mean: 2.1068, STD: 5.0172\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -1012.9588915152135 Explore P: 0.6307 SOC: 0.8038 Cumulative_SOC_deviation: 95.2180 Fuel Consumption: 60.7788 Mean: 2.1068, STD: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -940.4361731594128 Explore P: 0.6139 SOC: 0.7797 Cumulative_SOC_deviation: 88.1534 Fuel Consumption: 58.9022 Mean: 2.1068, STD: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -892.6885191114953 Explore P: 0.5976 SOC: 0.7688 Cumulative_SOC_deviation: 83.4477 Fuel Consumption: 58.2120 Mean: 2.1068, STD: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -914.5944396056656 Explore P: 0.5816 SOC: 0.7884 Cumulative_SOC_deviation: 85.5414 Fuel Consumption: 59.1804 Mean: 2.1068, STD: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -895.9545793980822 Explore P: 0.5662 SOC: 0.7600 Cumulative_SOC_deviation: 83.8683 Fuel Consumption: 57.2714 Mean: 2.1068, STD: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -718.2049417732558 Explore P: 0.5511 SOC: 0.7068 Cumulative_SOC_deviation: 66.5184 Fuel Consumption: 53.0210 Mean: 2.1068, STD: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -726.5018136447492 Explore P: 0.5364 SOC: 0.7101 Cumulative_SOC_deviation: 67.3088 Fuel Consumption: 53.4143 Mean: 2.1068, STD: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -836.2717434829032 Explore P: 0.5222 SOC: 0.6864 Cumulative_SOC_deviation: 78.4513 Fuel Consumption: 51.7586 Mean: 2.1068, STD: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -727.1371826017519 Explore P: 0.5083 SOC: 0.6762 Cumulative_SOC_deviation: 67.6231 Fuel Consumption: 50.9057 Mean: 2.1068, STD: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -744.0586819859517 Explore P: 0.4948 SOC: 0.7269 Cumulative_SOC_deviation: 68.9375 Fuel Consumption: 54.6832 Mean: 2.1068, STD: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -770.2047099576264 Explore P: 0.4817 SOC: 0.6945 Cumulative_SOC_deviation: 71.7716 Fuel Consumption: 52.4889 Mean: 2.1068, STD: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -768.7809174217077 Explore P: 0.4689 SOC: 0.7228 Cumulative_SOC_deviation: 71.4073 Fuel Consumption: 54.7081 Mean: 2.1068, STD: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -771.8609462867793 Explore P: 0.4565 SOC: 0.6958 Cumulative_SOC_deviation: 71.8805 Fuel Consumption: 53.0560 Mean: 2.1068, STD: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -904.6809725137282 Explore P: 0.4444 SOC: 0.6545 Cumulative_SOC_deviation: 85.4890 Fuel Consumption: 49.7908 Mean: 2.1068, STD: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -987.762414905718 Explore P: 0.4326 SOC: 0.6177 Cumulative_SOC_deviation: 94.0688 Fuel Consumption: 47.0744 Mean: 2.1068, STD: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -977.2260106130143 Explore P: 0.4212 SOC: 0.6321 Cumulative_SOC_deviation: 92.9280 Fuel Consumption: 47.9456 Mean: 2.1068, STD: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -913.9771371204334 Explore P: 0.4100 SOC: 0.6751 Cumulative_SOC_deviation: 86.2623 Fuel Consumption: 51.3542 Mean: 2.1068, STD: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -793.0913440482434 Explore P: 0.3992 SOC: 0.7202 Cumulative_SOC_deviation: 73.8667 Fuel Consumption: 54.4243 Mean: 2.1068, STD: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -599.1200719889481 Explore P: 0.3887 SOC: 0.6872 Cumulative_SOC_deviation: 54.7848 Fuel Consumption: 51.2721 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -741.2252894364215 Explore P: 0.3784 SOC: 0.7155 Cumulative_SOC_deviation: 68.7504 Fuel Consumption: 53.7214 Mean: 2.1068, STD: 5.0177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -869.0773354564245 Explore P: 0.3684 SOC: 0.7558 Cumulative_SOC_deviation: 81.2185 Fuel Consumption: 56.8928 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -794.0058563131655 Explore P: 0.3587 SOC: 0.7164 Cumulative_SOC_deviation: 74.0261 Fuel Consumption: 53.7447 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -803.2158641481669 Explore P: 0.3493 SOC: 0.7351 Cumulative_SOC_deviation: 74.7938 Fuel Consumption: 55.2777 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -919.9576960126468 Explore P: 0.3401 SOC: 0.7496 Cumulative_SOC_deviation: 86.3676 Fuel Consumption: 56.2821 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -492.9527690549571 Explore P: 0.3311 SOC: 0.6695 Cumulative_SOC_deviation: 44.2549 Fuel Consumption: 50.4035 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -617.6087360842763 Explore P: 0.3224 SOC: 0.6535 Cumulative_SOC_deviation: 56.8394 Fuel Consumption: 49.2151 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -735.5573311492202 Explore P: 0.3140 SOC: 0.6165 Cumulative_SOC_deviation: 68.9080 Fuel Consumption: 46.4774 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -756.6195503745982 Explore P: 0.3057 SOC: 0.6492 Cumulative_SOC_deviation: 70.7785 Fuel Consumption: 48.8345 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -729.9482344512533 Explore P: 0.2977 SOC: 0.6378 Cumulative_SOC_deviation: 68.1941 Fuel Consumption: 48.0074 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -653.0225529367334 Explore P: 0.2899 SOC: 0.6166 Cumulative_SOC_deviation: 60.7045 Fuel Consumption: 45.9777 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -910.5547585221158 Explore P: 0.2824 SOC: 0.6445 Cumulative_SOC_deviation: 86.2114 Fuel Consumption: 48.4407 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -882.8424084734039 Explore P: 0.2750 SOC: 0.6153 Cumulative_SOC_deviation: 83.6832 Fuel Consumption: 46.0107 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -954.8890403698571 Explore P: 0.2678 SOC: 0.6402 Cumulative_SOC_deviation: 90.7400 Fuel Consumption: 47.4890 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -661.2454891184541 Explore P: 0.2608 SOC: 0.6902 Cumulative_SOC_deviation: 61.0035 Fuel Consumption: 51.2103 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -717.3867879940583 Explore P: 0.2540 SOC: 0.7120 Cumulative_SOC_deviation: 66.4475 Fuel Consumption: 52.9115 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -871.425740799593 Explore P: 0.2474 SOC: 0.7467 Cumulative_SOC_deviation: 81.5695 Fuel Consumption: 55.7303 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -787.3325556579986 Explore P: 0.2410 SOC: 0.7599 Cumulative_SOC_deviation: 73.0562 Fuel Consumption: 56.7702 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -778.5941445568619 Explore P: 0.2347 SOC: 0.7327 Cumulative_SOC_deviation: 72.4049 Fuel Consumption: 54.5454 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -867.4506431051274 Explore P: 0.2286 SOC: 0.7500 Cumulative_SOC_deviation: 81.1605 Fuel Consumption: 55.8453 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -867.9809218531051 Explore P: 0.2227 SOC: 0.7614 Cumulative_SOC_deviation: 81.1088 Fuel Consumption: 56.8928 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -883.8049970644921 Explore P: 0.2170 SOC: 0.7602 Cumulative_SOC_deviation: 82.6982 Fuel Consumption: 56.8229 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -874.9522692281924 Explore P: 0.2114 SOC: 0.7058 Cumulative_SOC_deviation: 82.1587 Fuel Consumption: 53.3651 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -796.4497706995134 Explore P: 0.2059 SOC: 0.7460 Cumulative_SOC_deviation: 74.0395 Fuel Consumption: 56.0551 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -981.3434822854985 Explore P: 0.2006 SOC: 0.7670 Cumulative_SOC_deviation: 92.3717 Fuel Consumption: 57.6261 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -948.6111992218856 Explore P: 0.1954 SOC: 0.7635 Cumulative_SOC_deviation: 89.1076 Fuel Consumption: 57.5354 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -904.3261737994667 Explore P: 0.1904 SOC: 0.7577 Cumulative_SOC_deviation: 84.7613 Fuel Consumption: 56.7134 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -642.5110015874999 Explore P: 0.1855 SOC: 0.7047 Cumulative_SOC_deviation: 58.9707 Fuel Consumption: 52.8036 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -697.2150933858218 Explore P: 0.1808 SOC: 0.7019 Cumulative_SOC_deviation: 64.4571 Fuel Consumption: 52.6445 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -667.674990459005 Explore P: 0.1761 SOC: 0.7063 Cumulative_SOC_deviation: 61.5090 Fuel Consumption: 52.5847 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -640.4510255420084 Explore P: 0.1716 SOC: 0.6641 Cumulative_SOC_deviation: 59.0398 Fuel Consumption: 50.0533 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -807.1553888550752 Explore P: 0.1673 SOC: 0.6235 Cumulative_SOC_deviation: 76.0100 Fuel Consumption: 47.0557 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -806.2555987901534 Explore P: 0.1630 SOC: 0.6341 Cumulative_SOC_deviation: 75.8269 Fuel Consumption: 47.9861 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -1139.9262138440095 Explore P: 0.1589 SOC: 0.6145 Cumulative_SOC_deviation: 109.2781 Fuel Consumption: 47.1454 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -930.1735769549481 Explore P: 0.1548 SOC: 0.6361 Cumulative_SOC_deviation: 88.1909 Fuel Consumption: 48.2649 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -977.9803401833414 Explore P: 0.1509 SOC: 0.6273 Cumulative_SOC_deviation: 93.0370 Fuel Consumption: 47.6101 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -958.87762916425 Explore P: 0.1471 SOC: 0.6344 Cumulative_SOC_deviation: 91.0565 Fuel Consumption: 48.3130 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -1167.948143021232 Explore P: 0.1434 SOC: 0.5827 Cumulative_SOC_deviation: 112.3894 Fuel Consumption: 44.0545 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -973.2768306633159 Explore P: 0.1398 SOC: 0.6257 Cumulative_SOC_deviation: 92.5467 Fuel Consumption: 47.8098 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -912.5179834902312 Explore P: 0.1362 SOC: 0.6496 Cumulative_SOC_deviation: 86.3014 Fuel Consumption: 49.5044 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -1081.1189073630583 Explore P: 0.1328 SOC: 0.6115 Cumulative_SOC_deviation: 103.4800 Fuel Consumption: 46.3188 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -868.5956345693405 Explore P: 0.1295 SOC: 0.6690 Cumulative_SOC_deviation: 81.7476 Fuel Consumption: 51.1201 Mean: 2.1068, STD: 5.0179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -1022.6403877672004 Explore P: 0.1263 SOC: 0.6298 Cumulative_SOC_deviation: 97.4677 Fuel Consumption: 47.9633 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -992.0525658209082 Explore P: 0.1231 SOC: 0.6277 Cumulative_SOC_deviation: 94.3843 Fuel Consumption: 48.2091 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -1022.0893925224107 Explore P: 0.1200 SOC: 0.6375 Cumulative_SOC_deviation: 97.3041 Fuel Consumption: 49.0483 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -1121.348478247442 Explore P: 0.1171 SOC: 0.6664 Cumulative_SOC_deviation: 106.9870 Fuel Consumption: 51.4784 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -1102.5555639945733 Explore P: 0.1142 SOC: 0.6597 Cumulative_SOC_deviation: 105.1644 Fuel Consumption: 50.9113 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -840.8249750385634 Explore P: 0.1113 SOC: 0.6689 Cumulative_SOC_deviation: 79.0067 Fuel Consumption: 50.7577 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -838.7072414263322 Explore P: 0.1086 SOC: 0.6968 Cumulative_SOC_deviation: 78.6488 Fuel Consumption: 52.2193 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -849.8421370933577 Explore P: 0.1059 SOC: 0.6908 Cumulative_SOC_deviation: 79.7938 Fuel Consumption: 51.9041 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -842.9123267720969 Explore P: 0.1033 SOC: 0.7380 Cumulative_SOC_deviation: 78.7436 Fuel Consumption: 55.4759 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -822.6002806842221 Explore P: 0.1008 SOC: 0.7077 Cumulative_SOC_deviation: 76.8868 Fuel Consumption: 53.7320 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -851.845957213686 Explore P: 0.0983 SOC: 0.7143 Cumulative_SOC_deviation: 79.7741 Fuel Consumption: 54.1045 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -812.1352171505723 Explore P: 0.0960 SOC: 0.6796 Cumulative_SOC_deviation: 76.0023 Fuel Consumption: 52.1118 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -837.9371421071486 Explore P: 0.0936 SOC: 0.6730 Cumulative_SOC_deviation: 78.6064 Fuel Consumption: 51.8731 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -853.1612420381101 Explore P: 0.0914 SOC: 0.6759 Cumulative_SOC_deviation: 80.0930 Fuel Consumption: 52.2309 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -856.5058140482764 Explore P: 0.0892 SOC: 0.6648 Cumulative_SOC_deviation: 80.5286 Fuel Consumption: 51.2199 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -815.1456118714943 Explore P: 0.0870 SOC: 0.6604 Cumulative_SOC_deviation: 76.4688 Fuel Consumption: 50.4572 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -734.761758019493 Explore P: 0.0849 SOC: 0.6354 Cumulative_SOC_deviation: 68.6475 Fuel Consumption: 48.2866 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -843.68110228901 Explore P: 0.0829 SOC: 0.6310 Cumulative_SOC_deviation: 79.5736 Fuel Consumption: 47.9451 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -922.0842963427515 Explore P: 0.0809 SOC: 0.6554 Cumulative_SOC_deviation: 87.2114 Fuel Consumption: 49.9702 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -784.6902772718271 Explore P: 0.0790 SOC: 0.6508 Cumulative_SOC_deviation: 73.5624 Fuel Consumption: 49.0666 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -605.9710757875487 Explore P: 0.0771 SOC: 0.6429 Cumulative_SOC_deviation: 55.7587 Fuel Consumption: 48.3845 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -1035.7173462568396 Explore P: 0.0753 SOC: 0.6887 Cumulative_SOC_deviation: 98.2538 Fuel Consumption: 53.1791 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -816.6372238529304 Explore P: 0.0735 SOC: 0.6439 Cumulative_SOC_deviation: 76.7076 Fuel Consumption: 49.5617 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -770.3776397675025 Explore P: 0.0718 SOC: 0.6407 Cumulative_SOC_deviation: 72.0987 Fuel Consumption: 49.3904 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -564.680607321999 Explore P: 0.0701 SOC: 0.6242 Cumulative_SOC_deviation: 51.7000 Fuel Consumption: 47.6805 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -821.2056257564171 Explore P: 0.0685 SOC: 0.6242 Cumulative_SOC_deviation: 77.3194 Fuel Consumption: 48.0120 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -915.6744498143812 Explore P: 0.0669 SOC: 0.6378 Cumulative_SOC_deviation: 86.6563 Fuel Consumption: 49.1117 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -998.3042507748266 Explore P: 0.0654 SOC: 0.7114 Cumulative_SOC_deviation: 94.3467 Fuel Consumption: 54.8373 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -929.6370507174018 Explore P: 0.0639 SOC: 0.7113 Cumulative_SOC_deviation: 87.4866 Fuel Consumption: 54.7709 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -1043.4048129022278 Explore P: 0.0624 SOC: 0.7383 Cumulative_SOC_deviation: 98.6593 Fuel Consumption: 56.8122 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -916.7943818625881 Explore P: 0.0610 SOC: 0.6827 Cumulative_SOC_deviation: 86.4301 Fuel Consumption: 52.4934 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -933.5334763789634 Explore P: 0.0596 SOC: 0.7040 Cumulative_SOC_deviation: 87.9486 Fuel Consumption: 54.0477 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -947.660434987846 Explore P: 0.0583 SOC: 0.6421 Cumulative_SOC_deviation: 89.8146 Fuel Consumption: 49.5146 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -925.2438125618655 Explore P: 0.0570 SOC: 0.6751 Cumulative_SOC_deviation: 87.3479 Fuel Consumption: 51.7647 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -911.6886153832356 Explore P: 0.0557 SOC: 0.6734 Cumulative_SOC_deviation: 86.0073 Fuel Consumption: 51.6152 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -913.0146570973001 Explore P: 0.0545 SOC: 0.6793 Cumulative_SOC_deviation: 86.1285 Fuel Consumption: 51.7292 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -839.3379460086937 Explore P: 0.0533 SOC: 0.6789 Cumulative_SOC_deviation: 78.7885 Fuel Consumption: 51.4530 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -805.9904120705049 Explore P: 0.0521 SOC: 0.7496 Cumulative_SOC_deviation: 74.9805 Fuel Consumption: 56.1859 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -728.1650105903047 Explore P: 0.0510 SOC: 0.7243 Cumulative_SOC_deviation: 67.4386 Fuel Consumption: 53.7791 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -541.6210861299036 Explore P: 0.0498 SOC: 0.6693 Cumulative_SOC_deviation: 49.2180 Fuel Consumption: 49.4406 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -543.9830819075104 Explore P: 0.0488 SOC: 0.6616 Cumulative_SOC_deviation: 49.5078 Fuel Consumption: 48.9054 Mean: 2.1068, STD: 5.0180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -624.2866314267153 Explore P: 0.0477 SOC: 0.6763 Cumulative_SOC_deviation: 57.4049 Fuel Consumption: 50.2372 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -949.5928040095562 Explore P: 0.0467 SOC: 0.7716 Cumulative_SOC_deviation: 89.1428 Fuel Consumption: 58.1648 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -832.589482909017 Explore P: 0.0457 SOC: 0.7445 Cumulative_SOC_deviation: 77.7149 Fuel Consumption: 55.4404 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -416.15059178562893 Explore P: 0.0447 SOC: 0.6554 Cumulative_SOC_deviation: 36.7273 Fuel Consumption: 48.8776 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -574.4621790594772 Explore P: 0.0438 SOC: 0.6168 Cumulative_SOC_deviation: 52.9008 Fuel Consumption: 45.4542 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -562.6956422329256 Explore P: 0.0429 SOC: 0.6040 Cumulative_SOC_deviation: 51.7974 Fuel Consumption: 44.7214 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -613.9153788234629 Explore P: 0.0420 SOC: 0.6129 Cumulative_SOC_deviation: 56.8411 Fuel Consumption: 45.5044 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -630.0352613959172 Explore P: 0.0411 SOC: 0.6102 Cumulative_SOC_deviation: 58.4919 Fuel Consumption: 45.1167 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -615.4351372870725 Explore P: 0.0403 SOC: 0.6100 Cumulative_SOC_deviation: 57.0279 Fuel Consumption: 45.1563 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -599.3660043545051 Explore P: 0.0395 SOC: 0.6064 Cumulative_SOC_deviation: 55.4637 Fuel Consumption: 44.7290 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -623.0756269597301 Explore P: 0.0387 SOC: 0.6157 Cumulative_SOC_deviation: 57.7631 Fuel Consumption: 45.4451 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -556.6599582076896 Explore P: 0.0379 SOC: 0.6370 Cumulative_SOC_deviation: 50.9202 Fuel Consumption: 47.4575 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -576.7324079777927 Explore P: 0.0371 SOC: 0.6385 Cumulative_SOC_deviation: 52.9085 Fuel Consumption: 47.6476 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -531.7104320777653 Explore P: 0.0364 SOC: 0.6284 Cumulative_SOC_deviation: 48.4795 Fuel Consumption: 46.9153 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -802.107579279822 Explore P: 0.0357 SOC: 0.6023 Cumulative_SOC_deviation: 75.7146 Fuel Consumption: 44.9611 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -860.5295824236025 Explore P: 0.0350 SOC: 0.5955 Cumulative_SOC_deviation: 81.5963 Fuel Consumption: 44.5669 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -887.4694324424826 Explore P: 0.0343 SOC: 0.5954 Cumulative_SOC_deviation: 84.2866 Fuel Consumption: 44.6039 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -836.65071685011 Explore P: 0.0336 SOC: 0.6170 Cumulative_SOC_deviation: 79.0192 Fuel Consumption: 46.4587 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -935.3303488400375 Explore P: 0.0330 SOC: 0.6273 Cumulative_SOC_deviation: 88.7086 Fuel Consumption: 48.2446 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -1092.089081688641 Explore P: 0.0324 SOC: 0.6418 Cumulative_SOC_deviation: 104.2147 Fuel Consumption: 49.9418 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -1440.5059388346424 Explore P: 0.0318 SOC: 0.5807 Cumulative_SOC_deviation: 139.5579 Fuel Consumption: 44.9272 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -1632.0186775941427 Explore P: 0.0312 SOC: 0.5791 Cumulative_SOC_deviation: 158.7229 Fuel Consumption: 44.7899 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -1253.9336287466067 Explore P: 0.0306 SOC: 0.6155 Cumulative_SOC_deviation: 120.6211 Fuel Consumption: 47.7231 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -956.1837992331164 Explore P: 0.0301 SOC: 0.6079 Cumulative_SOC_deviation: 90.9247 Fuel Consumption: 46.9371 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -989.3297493392761 Explore P: 0.0295 SOC: 0.6196 Cumulative_SOC_deviation: 94.1803 Fuel Consumption: 47.5265 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -734.1171429090108 Explore P: 0.0290 SOC: 0.6304 Cumulative_SOC_deviation: 68.5931 Fuel Consumption: 48.1858 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -798.3487244864534 Explore P: 0.0285 SOC: 0.6769 Cumulative_SOC_deviation: 74.6940 Fuel Consumption: 51.4084 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -812.6069337676328 Explore P: 0.0280 SOC: 0.6779 Cumulative_SOC_deviation: 76.1185 Fuel Consumption: 51.4221 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -962.8842141888217 Explore P: 0.0275 SOC: 0.6421 Cumulative_SOC_deviation: 91.4006 Fuel Consumption: 48.8781 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -1175.8798027656267 Explore P: 0.0270 SOC: 0.6354 Cumulative_SOC_deviation: 112.6946 Fuel Consumption: 48.9338 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -1260.3415001257897 Explore P: 0.0265 SOC: 0.6195 Cumulative_SOC_deviation: 121.2337 Fuel Consumption: 48.0049 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -1147.666245983683 Explore P: 0.0261 SOC: 0.6179 Cumulative_SOC_deviation: 109.9886 Fuel Consumption: 47.7799 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -1493.521616463637 Explore P: 0.0257 SOC: 0.6217 Cumulative_SOC_deviation: 144.5431 Fuel Consumption: 48.0910 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -1409.5015522293752 Explore P: 0.0252 SOC: 0.5963 Cumulative_SOC_deviation: 136.3404 Fuel Consumption: 46.0979 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -1786.6198997377783 Explore P: 0.0248 SOC: 0.5653 Cumulative_SOC_deviation: 174.3002 Fuel Consumption: 43.6177 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -1837.5377257634955 Explore P: 0.0244 SOC: 0.5842 Cumulative_SOC_deviation: 179.2182 Fuel Consumption: 45.3554 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -1447.0369717095798 Explore P: 0.0240 SOC: 0.6369 Cumulative_SOC_deviation: 139.7241 Fuel Consumption: 49.7958 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -1292.366649583025 Explore P: 0.0237 SOC: 0.6263 Cumulative_SOC_deviation: 124.4094 Fuel Consumption: 48.2725 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -1050.2569277162202 Explore P: 0.0233 SOC: 0.6393 Cumulative_SOC_deviation: 100.0885 Fuel Consumption: 49.3717 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -1338.5712984721129 Explore P: 0.0229 SOC: 0.6243 Cumulative_SOC_deviation: 129.0440 Fuel Consumption: 48.1311 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -1196.984570497577 Explore P: 0.0226 SOC: 0.6433 Cumulative_SOC_deviation: 114.7189 Fuel Consumption: 49.7958 Mean: 2.1068, STD: 5.0180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -957.7303726778144 Explore P: 0.0222 SOC: 0.7079 Cumulative_SOC_deviation: 90.2697 Fuel Consumption: 55.0334 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -1123.7333995846693 Explore P: 0.0219 SOC: 0.7073 Cumulative_SOC_deviation: 106.8768 Fuel Consumption: 54.9650 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -1212.749605267056 Explore P: 0.0216 SOC: 0.6925 Cumulative_SOC_deviation: 115.9115 Fuel Consumption: 53.6342 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -1229.3808050333498 Explore P: 0.0213 SOC: 0.6370 Cumulative_SOC_deviation: 118.0155 Fuel Consumption: 49.2257 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -1158.0874519461272 Explore P: 0.0210 SOC: 0.6765 Cumulative_SOC_deviation: 110.6039 Fuel Consumption: 52.0480 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -1052.1407212217769 Explore P: 0.0207 SOC: 0.6807 Cumulative_SOC_deviation: 100.0039 Fuel Consumption: 52.1017 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -1025.7149915649154 Explore P: 0.0204 SOC: 0.7029 Cumulative_SOC_deviation: 97.1864 Fuel Consumption: 53.8506 Mean: 2.1068, STD: 5.0180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -958.0910692567376 Explore P: 0.0201 SOC: 0.6741 Cumulative_SOC_deviation: 90.6436 Fuel Consumption: 51.6547 Mean: 2.1068, STD: 5.0180\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-85948e94b661>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mDELAY_TRAINING\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimary_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m                 \u001b[0mupdate_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimary_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMIN_EPSILON\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mMAX_EPSILON\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mMIN_EPSILON\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mDECAY_RATE\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-9cce513debe1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(primary_network, target_network, memory)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mtarget_q\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_idxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprimary_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_q\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[0;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[0;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[0;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[0;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[0;32m    312\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m           \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[0;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1584\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1585\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1586\u001b[1;33m     \u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1587\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1588\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   6110\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6111\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_a\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6112\u001b[1;33m         transpose_a, \"transpose_b\", transpose_b)\n\u001b[0m\u001b[0;32m   6113\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6114\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "num_trials = 3\n",
    "\n",
    "for trial in range(num_trials): \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(10)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                        \"Mean: {:.4f}, STD: {:.4f}\".format(memory.power_mean, memory.power_std)\n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "import glob \n",
    "\n",
    "from vehicle_model_DDQN1 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 500\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 100000\n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization():\n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", input_shape=[STATE_SIZE], \n",
    "                           kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", input_shape=[STATE_SIZE], \n",
    "                           kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_env(driving_path, reward_factor):\n",
    "    env = Environment(cell_model, driving_path, battery_path, motor_path, reward_factor)\n",
    "    return env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(primary_net, target_net, root): \n",
    "    primary_net.save_weights(\"./{}/primary_net_checkpoint\".format(root))\n",
    "    target_net.save_weights(\"./{}/target_net_checkpoint\".format(root))\n",
    "    print(\"model is saved..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 1\n",
      "training\\03_nedc.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ValueCreatorSong\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\generalization4\\vehicle_model_DDQN1.py:245: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  del_i = (1 / (2 * r_cha)) * (v_cha - (v_cha ** 2 - 4 * r_cha * p_bat) ** (0.5)) * (p_bat < 0) + (1 / (\n",
      "C:\\Users\\ValueCreatorSong\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\generalization4\\vehicle_model_DDQN1.py:271: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  del_i = (1 / (2 * r_cha)) * (v_cha - (v_cha ** 2 - 4 * r_cha * p_bat) ** (0.5)) * (p_bat < 0) + (1 / (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOC is nan...\n",
      "Episode: 0 Total reward: -3175.009711214633 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 48.9321\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 1 Total reward: -3082.824982958693 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.0349\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 2 Total reward: -3113.246790807574 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.1778\n",
      "training\\03_nedc.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ValueCreatorSong\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\generalization4\\vehicle_model_DDQN1.py:246: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  2 * r_dis)) * (v_dis - (v_dis ** 2 - 4 * r_dis * p_bat) ** (0.5)) * (p_bat >= 0)\n",
      "C:\\Users\\ValueCreatorSong\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\generalization4\\vehicle_model_DDQN1.py:272: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  2 * r_dis)) * (v_dis - (v_dis ** 2 - 4 * r_dis * p_bat) ** (0.5)) * (p_bat >= 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOC is nan...\n",
      "Episode: 3 Total reward: -3281.035189424208 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 51.0996\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 4 Total reward: -3271.7246489568383 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.9634\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 5 Total reward: -3179.1942033684727 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.5174\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 6 Total reward: -3204.9892153445194 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.4915\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 7 Total reward: -3231.7823169096937 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 50.3429\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 8 Total reward: -3252.878798540626 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.2797\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 9 Total reward: -3171.7836241100053 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.5929\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 10 Total reward: -3221.0436687283436 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 50.2847\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 11 Total reward: -3143.875234436932 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.1459\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 12 Total reward: -3166.311745748836 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.8306\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 13 Total reward: -3146.543653828378 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.4145\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 14 Total reward: -3118.31037196533 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 48.2712\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 15 Total reward: -3148.5224605714834 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 48.7556\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 16 Total reward: -3241.2154543525153 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 50.2265\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 17 Total reward: -3283.7144235332394 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 50.7049\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 18 Total reward: -3167.011710450306 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.6096\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 19 Total reward: -3190.460266690923 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.4053\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 20 Total reward: -3204.103682816207 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 50.4170\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 21 Total reward: -3206.4449137350175 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.5290\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 22 Total reward: -3132.384113480459 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 50.1332\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 23 Total reward: -3217.7999059338154 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.8204\n",
      "training\\03_nedc.mat\n",
      "SOC is nan...\n",
      "Episode: 24 Total reward: -3247.5002862801894 Explore P: 1.0000 SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: 49.9096\n",
      "training\\03_nedc.mat\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-1b1309243999>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprimary_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\generalization4\\vehicle_model_DDQN1.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[0mj_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_curdensity_region\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_mot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m             \u001b[0mj_fc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculation_comp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'action_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[0mcell_voltage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_voltage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj_fc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\generalization4\\vehicle_model_DDQN1.py\u001b[0m in \u001b[0;36mget_curdensity_region\u001b[1;34m(self, p_mot)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[0mP_battery_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp_mot\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mP_stack_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mcondition_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcondition_check_battery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_bat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp_bat\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mP_battery_set\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition_set\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\generalization4\\vehicle_model_DDQN1.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[0mP_battery_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp_mot\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mP_stack_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mcondition_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcondition_check_battery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_bat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp_bat\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mP_battery_set\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition_set\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\generalization4\\vehicle_model_DDQN1.py\u001b[0m in \u001b[0;36mcondition_check_battery\u001b[1;34m(self, p_bat)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcondition_check_battery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_bat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0mv_dis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_cha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_dis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_cha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi_lim_dis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi_lim_cha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_battery_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m         del_i = (1 / (2 * r_cha)) * (v_cha - (v_cha ** 2 - 4 * r_cha * p_bat) ** (0.5)) * (p_bat < 0) + (1 / (\n\u001b[0;32m    246\u001b[0m                 2 * r_dis)) * (v_dis - (v_dis ** 2 - 4 * r_dis * p_bat) ** (0.5)) * (p_bat >= 0)\n",
      "\u001b[1;32m~\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\generalization4\\vehicle_model_DDQN1.py\u001b[0m in \u001b[0;36mget_battery_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    263\u001b[0m                                          fill_value='extrapolate')(self.SOC)\n\u001b[0;32m    264\u001b[0m         i_lim_cha = interpolate.interp1d(self.battery['SOC_ind'], self.battery['Cur_lim_cha'], assume_sorted=False,\n\u001b[1;32m--> 265\u001b[1;33m                                          fill_value='extrapolate')(self.SOC)\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv_dis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_cha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_dis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_cha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi_lim_dis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi_lim_cha\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\scipy\\interpolate\\interpolate.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, kind, axis, copy, bounds_error, fill_value, assume_sorted)\u001b[0m\n\u001b[0;32m    445\u001b[0m                                       \"routines for other types.\" % kind)\n\u001b[0;32m    446\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0massume_sorted\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10] * 3\n",
    "results_dict = {} \n",
    "driving_cycle_paths = glob.glob(\"training/*.mat\")[1:2]\n",
    "\n",
    "for trial, reward_factor in enumerate(reward_factors): \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    memory, primary_network, target_network = initialization()\n",
    "#     for episode in range(TOTAL_EPISODES):\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        driving_cycle_path = np.random.choice(driving_cycle_paths)\n",
    "        print(driving_cycle_path)\n",
    "        env = initialization_env(driving_cycle_path, 10)\n",
    "        state = env.reset() \n",
    "\n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * (steps - \n",
    "                                                                        DELAY_TRAINING))\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                avg_loss /= cnt \n",
    "                print('Episode: {}'.format(episode),\n",
    "                      'Total reward: {}'.format(total_reward), \n",
    "                      'Explore P: {:.4f}'.format(eps), \n",
    "                      \"SOC: {:.4f}\".format(env.SOC), \n",
    "                     \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                     \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                     )\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    root = \"DDQN1_trial{}\".format(trial+2)\n",
    "    save_weights(primary_network, target_network, root)\n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN1.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

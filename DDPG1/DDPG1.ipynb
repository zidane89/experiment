{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from vehicle_model_DDPG import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64): \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple): \n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "        \n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_input)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    \n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "actor_model = get_actor() \n",
    "critic_model = get_critic() \n",
    "\n",
    "target_actor = get_actor() \n",
    "target_critic = get_critic() \n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "critic_lr = 0.002 \n",
    "actor_lr = 0.001 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 200 \n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002 \n",
    "BATCH_SIZE = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 1 Total reward: -935.9290733229453 SOC: 0.7970 Cumulative_SOC_deviation: 87.5138 Fuel Consumption: 60.7910\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 2 Total reward: -843.7325456267621 SOC: 0.7584 Cumulative_SOC_deviation: 78.5961 Fuel Consumption: 57.7718\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -883.4164374922801 SOC: 0.7645 Cumulative_SOC_deviation: 82.5274 Fuel Consumption: 58.1420\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -772.5179636428113 SOC: 0.7399 Cumulative_SOC_deviation: 71.6292 Fuel Consumption: 56.2258\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -768.9001875878322 SOC: 0.7191 Cumulative_SOC_deviation: 71.4262 Fuel Consumption: 54.6381\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -719.6701662205257 SOC: 0.7053 Cumulative_SOC_deviation: 66.6112 Fuel Consumption: 53.5586\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -758.0917224648211 SOC: 0.7014 Cumulative_SOC_deviation: 70.4765 Fuel Consumption: 53.3265\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -768.3358065335492 SOC: 0.6561 Cumulative_SOC_deviation: 71.8403 Fuel Consumption: 49.9328\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -813.3769839205237 SOC: 0.6427 Cumulative_SOC_deviation: 76.4665 Fuel Consumption: 48.7121\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -878.5921134230986 SOC: 0.6395 Cumulative_SOC_deviation: 83.0047 Fuel Consumption: 48.5452\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -1069.1908943799092 SOC: 0.6057 Cumulative_SOC_deviation: 102.3097 Fuel Consumption: 46.0941\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -1191.2219367457271 SOC: 0.5874 Cumulative_SOC_deviation: 114.6626 Fuel Consumption: 44.5963\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -997.6936749054888 SOC: 0.6061 Cumulative_SOC_deviation: 95.1622 Fuel Consumption: 46.0717\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -1263.864510335387 SOC: 0.5676 Cumulative_SOC_deviation: 122.0811 Fuel Consumption: 43.0536\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -1326.9185931597553 SOC: 0.5710 Cumulative_SOC_deviation: 128.3471 Fuel Consumption: 43.4473\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -1780.2334126200603 SOC: 0.5058 Cumulative_SOC_deviation: 174.1713 Fuel Consumption: 38.5205\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -1446.045520665385 SOC: 0.5454 Cumulative_SOC_deviation: 140.4454 Fuel Consumption: 41.5910\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -1614.5873500670925 SOC: 0.5135 Cumulative_SOC_deviation: 157.5478 Fuel Consumption: 39.1089\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -1997.4428399321516 SOC: 0.4682 Cumulative_SOC_deviation: 196.1937 Fuel Consumption: 35.5056\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -1854.5053964348422 SOC: 0.4870 Cumulative_SOC_deviation: 181.7646 Fuel Consumption: 36.8590\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -2064.755249007807 SOC: 0.4552 Cumulative_SOC_deviation: 203.0092 Fuel Consumption: 34.6636\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -1931.9399573556623 SOC: 0.4645 Cumulative_SOC_deviation: 189.6582 Fuel Consumption: 35.3580\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -2010.3573406669905 SOC: 0.4544 Cumulative_SOC_deviation: 197.5577 Fuel Consumption: 34.7802\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -2368.880728545702 SOC: 0.4223 Cumulative_SOC_deviation: 233.6564 Fuel Consumption: 32.3163\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -2051.814514812075 SOC: 0.4432 Cumulative_SOC_deviation: 201.8185 Fuel Consumption: 33.6291\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -2505.248505015963 SOC: 0.3942 Cumulative_SOC_deviation: 247.4957 Fuel Consumption: 30.2910\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -2475.3832180378213 SOC: 0.3934 Cumulative_SOC_deviation: 244.5229 Fuel Consumption: 30.1541\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -2568.5138237032616 SOC: 0.3876 Cumulative_SOC_deviation: 253.8681 Fuel Consumption: 29.8331\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -2484.6108395470465 SOC: 0.3854 Cumulative_SOC_deviation: 245.4950 Fuel Consumption: 29.6609\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -2644.877262757544 SOC: 0.3621 Cumulative_SOC_deviation: 261.7047 Fuel Consumption: 27.8303\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -2635.3453358238153 SOC: 0.3659 Cumulative_SOC_deviation: 260.7102 Fuel Consumption: 28.2433\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -2901.500854909231 SOC: 0.3338 Cumulative_SOC_deviation: 287.5687 Fuel Consumption: 25.8136\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -2960.889113725799 SOC: 0.3208 Cumulative_SOC_deviation: 293.5883 Fuel Consumption: 25.0059\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 34 Total reward: -2031.846555521623 SOC: 1.0000 Cumulative_SOC_deviation: 194.8322 Fuel Consumption: 83.5242\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 35 Total reward: -2745.439430075206 SOC: 1.0000 Cumulative_SOC_deviation: 265.3772 Fuel Consumption: 91.6678\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 36 Total reward: -2666.1868868619867 SOC: 1.0000 Cumulative_SOC_deviation: 257.5741 Fuel Consumption: 90.4460\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 37 Total reward: -2824.2370934090336 SOC: 1.0000 Cumulative_SOC_deviation: 273.2243 Fuel Consumption: 91.9941\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 38 Total reward: -2907.06535436649 SOC: 1.0000 Cumulative_SOC_deviation: 281.4266 Fuel Consumption: 92.7997\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 39 Total reward: -2898.3246548458715 SOC: 1.0000 Cumulative_SOC_deviation: 280.5859 Fuel Consumption: 92.4659\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 40 Total reward: -2948.1939389979375 SOC: 1.0000 Cumulative_SOC_deviation: 285.3795 Fuel Consumption: 94.3992\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 41 Total reward: -2798.705164876023 SOC: 1.0000 Cumulative_SOC_deviation: 270.6129 Fuel Consumption: 92.5761\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 42 Total reward: -2965.4653158695905 SOC: 1.0000 Cumulative_SOC_deviation: 287.0533 Fuel Consumption: 94.9320\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 43 Total reward: -2910.9764969963417 SOC: 1.0000 Cumulative_SOC_deviation: 281.6552 Fuel Consumption: 94.4249\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 44 Total reward: -2859.1461716721597 SOC: 1.0000 Cumulative_SOC_deviation: 276.5229 Fuel Consumption: 93.9167\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 45 Total reward: -2959.1032358086486 SOC: 1.0000 Cumulative_SOC_deviation: 286.3910 Fuel Consumption: 95.1930\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 46 Total reward: -3035.404009941642 SOC: 1.0000 Cumulative_SOC_deviation: 293.9551 Fuel Consumption: 95.8531\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 47 Total reward: -3034.084146508366 SOC: 1.0000 Cumulative_SOC_deviation: 293.7880 Fuel Consumption: 96.2041\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 48 Total reward: -3093.600191599407 SOC: 1.0000 Cumulative_SOC_deviation: 299.6196 Fuel Consumption: 97.4045\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 49 Total reward: -3014.835818316686 SOC: 1.0000 Cumulative_SOC_deviation: 291.8111 Fuel Consumption: 96.7251\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 50 Total reward: -3105.3116817271925 SOC: 1.0000 Cumulative_SOC_deviation: 300.6751 Fuel Consumption: 98.5610\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 51 Total reward: -3009.280641554932 SOC: 1.0000 Cumulative_SOC_deviation: 291.2041 Fuel Consumption: 97.2397\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 52 Total reward: -3136.9278974637114 SOC: 1.0000 Cumulative_SOC_deviation: 303.6913 Fuel Consumption: 100.0149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available condition is not avail... SOC: 1\n",
      "Episode: 53 Total reward: -3150.3252689877845 SOC: 1.0000 Cumulative_SOC_deviation: 305.1149 Fuel Consumption: 99.1762\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 54 Total reward: -3140.0588983073317 SOC: 1.0000 Cumulative_SOC_deviation: 304.1104 Fuel Consumption: 98.9547\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 55 Total reward: -3095.0140961242246 SOC: 1.0000 Cumulative_SOC_deviation: 299.6206 Fuel Consumption: 98.8081\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 56 Total reward: -3107.425984472269 SOC: 1.0000 Cumulative_SOC_deviation: 300.8737 Fuel Consumption: 98.6894\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 57 Total reward: -3117.2843939553 SOC: 1.0000 Cumulative_SOC_deviation: 301.7903 Fuel Consumption: 99.3816\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 58 Total reward: -3149.891740207598 SOC: 1.0000 Cumulative_SOC_deviation: 304.9941 Fuel Consumption: 99.9508\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 59 Total reward: -3105.0065940390245 SOC: 1.0000 Cumulative_SOC_deviation: 300.5236 Fuel Consumption: 99.7710\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 60 Total reward: -3216.903808522168 SOC: 1.0000 Cumulative_SOC_deviation: 311.6405 Fuel Consumption: 100.4985\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 61 Total reward: -3226.7446686978697 SOC: 1.0000 Cumulative_SOC_deviation: 312.5461 Fuel Consumption: 101.2838\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 62 Total reward: -3217.645159924266 SOC: 1.0000 Cumulative_SOC_deviation: 311.5853 Fuel Consumption: 101.7920\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 63 Total reward: -3244.8365794688834 SOC: 1.0000 Cumulative_SOC_deviation: 314.3479 Fuel Consumption: 101.3576\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 64 Total reward: -3214.575367814115 SOC: 1.0000 Cumulative_SOC_deviation: 311.2811 Fuel Consumption: 101.7642\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 65 Total reward: -3172.823034518016 SOC: 1.0000 Cumulative_SOC_deviation: 307.1490 Fuel Consumption: 101.3330\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 66 Total reward: -3178.846270682377 SOC: 1.0000 Cumulative_SOC_deviation: 307.7665 Fuel Consumption: 101.1811\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 67 Total reward: -3266.177631411273 SOC: 1.0000 Cumulative_SOC_deviation: 316.3154 Fuel Consumption: 103.0234\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 68 Total reward: -3199.6364762116978 SOC: 1.0000 Cumulative_SOC_deviation: 309.8064 Fuel Consumption: 101.5727\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 69 Total reward: -3328.412514284899 SOC: 1.0000 Cumulative_SOC_deviation: 322.4883 Fuel Consumption: 103.5295\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 70 Total reward: -3301.5250354641976 SOC: 1.0000 Cumulative_SOC_deviation: 319.7469 Fuel Consumption: 104.0559\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 71 Total reward: -3306.9397629350287 SOC: 1.0000 Cumulative_SOC_deviation: 320.3895 Fuel Consumption: 103.0448\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 72 Total reward: -3269.534031230893 SOC: 1.0000 Cumulative_SOC_deviation: 316.6230 Fuel Consumption: 103.3037\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 73 Total reward: -3326.776094397743 SOC: 1.0000 Cumulative_SOC_deviation: 322.3123 Fuel Consumption: 103.6536\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 74 Total reward: -3299.006740883301 SOC: 1.0000 Cumulative_SOC_deviation: 319.5556 Fuel Consumption: 103.4503\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "buffer = Buffer(50000, BATCH_SIZE)\n",
    "eps = MAX_EPSILON \n",
    "steps = 0 \n",
    "\n",
    "ep_reward_list = [] \n",
    "avg_reward_list = [] \n",
    "\n",
    "for ep in range(total_episodes): \n",
    "    state = env.reset() \n",
    "    episodic_reward = 0 \n",
    "    \n",
    "    while True: \n",
    "        tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "        action = policy_epsilon_greedy(tf_state, eps)\n",
    "#         print(action)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        if done: \n",
    "            next_state = [0] * num_states \n",
    "        \n",
    "        buffer.record((state, action, reward, next_state))\n",
    "        episodic_reward += reward \n",
    "        \n",
    "        buffer.learn() \n",
    "        update_target(tau)\n",
    "        steps += 1 \n",
    "        \n",
    "        eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "        \n",
    "        if done: \n",
    "            break \n",
    "        \n",
    "        state = next_state \n",
    "    \n",
    "    ep_reward_list.append(episodic_reward) \n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    avg_reward_list.append(avg_reward)\n",
    "    \n",
    "#     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "    print(\n",
    "          'Episode: {}'.format(ep + 1),\n",
    "          'Total reward: {}'.format(episodic_reward), \n",
    "          \"SOC: {:.4f}\".format(env.SOC), \n",
    "          \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "          \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN3_ref import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: ref\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer dense_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -942.8415020467703 Explore P: 0.9217 SOC: 0.7925 Cumulative_SOC_deviation: 88.2697 Fuel Consumption: 60.1448\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -844.5746155604597 Explore P: 0.8970 SOC: 0.7592 Cumulative_SOC_deviation: 78.7060 Fuel Consumption: 57.5141\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -867.0388577768017 Explore P: 0.8730 SOC: 0.7788 Cumulative_SOC_deviation: 80.8025 Fuel Consumption: 59.0142\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -799.8842032591598 Explore P: 0.8496 SOC: 0.7226 Cumulative_SOC_deviation: 74.5397 Fuel Consumption: 54.4876\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -774.3126648347378 Explore P: 0.8269 SOC: 0.7406 Cumulative_SOC_deviation: 71.8241 Fuel Consumption: 56.0713\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -787.2258922969016 Explore P: 0.8048 SOC: 0.7454 Cumulative_SOC_deviation: 73.0764 Fuel Consumption: 56.4620\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -740.490407768808 Explore P: 0.7832 SOC: 0.7068 Cumulative_SOC_deviation: 68.7259 Fuel Consumption: 53.2318\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -729.2665923590746 Explore P: 0.7623 SOC: 0.7202 Cumulative_SOC_deviation: 67.5052 Fuel Consumption: 54.2145\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -774.088335006436 Explore P: 0.7419 SOC: 0.7305 Cumulative_SOC_deviation: 71.8960 Fuel Consumption: 55.1287\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -765.9614170039556 Explore P: 0.7221 SOC: 0.7095 Cumulative_SOC_deviation: 71.2464 Fuel Consumption: 53.4974\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -768.7790570880178 Explore P: 0.7028 SOC: 0.7326 Cumulative_SOC_deviation: 71.3709 Fuel Consumption: 55.0704\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -753.1979932942958 Explore P: 0.6840 SOC: 0.6985 Cumulative_SOC_deviation: 70.0640 Fuel Consumption: 52.5578\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -782.9334780457218 Explore P: 0.6658 SOC: 0.7202 Cumulative_SOC_deviation: 72.8830 Fuel Consumption: 54.1035\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -763.6071807299444 Explore P: 0.6480 SOC: 0.7347 Cumulative_SOC_deviation: 70.8231 Fuel Consumption: 55.3765\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -725.3536849790084 Explore P: 0.6307 SOC: 0.7023 Cumulative_SOC_deviation: 67.2598 Fuel Consumption: 52.7560\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -746.296298402489 Explore P: 0.6139 SOC: 0.6799 Cumulative_SOC_deviation: 69.5309 Fuel Consumption: 50.9878\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -727.3304642394961 Explore P: 0.5976 SOC: 0.6784 Cumulative_SOC_deviation: 67.6522 Fuel Consumption: 50.8089\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -655.0127514661357 Explore P: 0.5816 SOC: 0.7004 Cumulative_SOC_deviation: 60.2507 Fuel Consumption: 52.5056\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -643.1653758642065 Explore P: 0.5662 SOC: 0.7003 Cumulative_SOC_deviation: 59.0761 Fuel Consumption: 52.4048\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -665.174942295103 Explore P: 0.5511 SOC: 0.6827 Cumulative_SOC_deviation: 61.4296 Fuel Consumption: 50.8788\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -742.3269979864762 Explore P: 0.5364 SOC: 0.6622 Cumulative_SOC_deviation: 69.2683 Fuel Consumption: 49.6438\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -788.2073277203973 Explore P: 0.5222 SOC: 0.6572 Cumulative_SOC_deviation: 73.9015 Fuel Consumption: 49.1928\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -789.0989664984914 Explore P: 0.5083 SOC: 0.6774 Cumulative_SOC_deviation: 73.8242 Fuel Consumption: 50.8570\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -801.2219312843387 Explore P: 0.4948 SOC: 0.6517 Cumulative_SOC_deviation: 75.2676 Fuel Consumption: 48.5456\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -820.6206675982601 Explore P: 0.4817 SOC: 0.6503 Cumulative_SOC_deviation: 77.2094 Fuel Consumption: 48.5264\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -811.4433655722272 Explore P: 0.4689 SOC: 0.6599 Cumulative_SOC_deviation: 76.2236 Fuel Consumption: 49.2070\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -765.0030564601503 Explore P: 0.4565 SOC: 0.6516 Cumulative_SOC_deviation: 71.6390 Fuel Consumption: 48.6130\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -689.3992946163797 Explore P: 0.4444 SOC: 0.6640 Cumulative_SOC_deviation: 64.0013 Fuel Consumption: 49.3859\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -692.1465127208188 Explore P: 0.4326 SOC: 0.6741 Cumulative_SOC_deviation: 64.1718 Fuel Consumption: 50.4283\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -610.7578018690427 Explore P: 0.4212 SOC: 0.7022 Cumulative_SOC_deviation: 55.8490 Fuel Consumption: 52.2679\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -639.3804836655621 Explore P: 0.4100 SOC: 0.6988 Cumulative_SOC_deviation: 58.7301 Fuel Consumption: 52.0794\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -586.0103310078649 Explore P: 0.3992 SOC: 0.6854 Cumulative_SOC_deviation: 53.5163 Fuel Consumption: 50.8469\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -753.604660135399 Explore P: 0.3887 SOC: 0.6872 Cumulative_SOC_deviation: 70.2333 Fuel Consumption: 51.2716\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -654.3668514657855 Explore P: 0.3784 SOC: 0.6866 Cumulative_SOC_deviation: 60.3085 Fuel Consumption: 51.2822\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -681.820136165498 Explore P: 0.3684 SOC: 0.6942 Cumulative_SOC_deviation: 63.0073 Fuel Consumption: 51.7475\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -901.5996429064254 Explore P: 0.3587 SOC: 0.6649 Cumulative_SOC_deviation: 85.2020 Fuel Consumption: 49.5800\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -754.5078646092803 Explore P: 0.3493 SOC: 0.7361 Cumulative_SOC_deviation: 69.9587 Fuel Consumption: 54.9204\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -718.8756924238997 Explore P: 0.3401 SOC: 0.7475 Cumulative_SOC_deviation: 66.2830 Fuel Consumption: 56.0460\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -837.261992107898 Explore P: 0.3311 SOC: 0.7370 Cumulative_SOC_deviation: 78.2129 Fuel Consumption: 55.1333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -653.8857679587186 Explore P: 0.3224 SOC: 0.6714 Cumulative_SOC_deviation: 60.4017 Fuel Consumption: 49.8683\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -603.5325499278654 Explore P: 0.3140 SOC: 0.6555 Cumulative_SOC_deviation: 55.4640 Fuel Consumption: 48.8928\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -621.9337043563592 Explore P: 0.3057 SOC: 0.7057 Cumulative_SOC_deviation: 56.9142 Fuel Consumption: 52.7914\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -646.2663144557472 Explore P: 0.2977 SOC: 0.6835 Cumulative_SOC_deviation: 59.5196 Fuel Consumption: 51.0704\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -491.33355813892825 Explore P: 0.2899 SOC: 0.6807 Cumulative_SOC_deviation: 44.0905 Fuel Consumption: 50.4283\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -661.6152989508339 Explore P: 0.2824 SOC: 0.7050 Cumulative_SOC_deviation: 60.9138 Fuel Consumption: 52.4777\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -503.21735783232674 Explore P: 0.2750 SOC: 0.6633 Cumulative_SOC_deviation: 45.4138 Fuel Consumption: 49.0793\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -603.5419237422989 Explore P: 0.2678 SOC: 0.7151 Cumulative_SOC_deviation: 54.9902 Fuel Consumption: 53.6403\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -628.9846293711515 Explore P: 0.2608 SOC: 0.7165 Cumulative_SOC_deviation: 57.5448 Fuel Consumption: 53.5369\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -459.54811987977274 Explore P: 0.2540 SOC: 0.6504 Cumulative_SOC_deviation: 41.1627 Fuel Consumption: 47.9213\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -567.9788019754106 Explore P: 0.2474 SOC: 0.6824 Cumulative_SOC_deviation: 51.7139 Fuel Consumption: 50.8393\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -788.1588913391234 Explore P: 0.2410 SOC: 0.7709 Cumulative_SOC_deviation: 73.0053 Fuel Consumption: 58.1060\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -688.4974005309592 Explore P: 0.2347 SOC: 0.6875 Cumulative_SOC_deviation: 63.6854 Fuel Consumption: 51.6436\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -694.6916641026784 Explore P: 0.2286 SOC: 0.6751 Cumulative_SOC_deviation: 64.4216 Fuel Consumption: 50.4759\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -708.4978045112034 Explore P: 0.2227 SOC: 0.6319 Cumulative_SOC_deviation: 66.1460 Fuel Consumption: 47.0374\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -670.794738631918 Explore P: 0.2170 SOC: 0.6753 Cumulative_SOC_deviation: 62.0353 Fuel Consumption: 50.4420\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -561.9922459146044 Explore P: 0.2114 SOC: 0.6783 Cumulative_SOC_deviation: 51.1159 Fuel Consumption: 50.8337\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -581.8100506623757 Explore P: 0.2059 SOC: 0.7112 Cumulative_SOC_deviation: 52.8411 Fuel Consumption: 53.3986\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -504.37836665013003 Explore P: 0.2006 SOC: 0.6301 Cumulative_SOC_deviation: 45.7848 Fuel Consumption: 46.5306\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -560.3385533935901 Explore P: 0.1954 SOC: 0.6686 Cumulative_SOC_deviation: 51.0417 Fuel Consumption: 49.9220\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -403.79258339657304 Explore P: 0.1904 SOC: 0.6398 Cumulative_SOC_deviation: 35.6447 Fuel Consumption: 47.3461\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -434.4293172973195 Explore P: 0.1855 SOC: 0.6435 Cumulative_SOC_deviation: 38.6714 Fuel Consumption: 47.7155\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -468.4392252663988 Explore P: 0.1808 SOC: 0.6504 Cumulative_SOC_deviation: 41.9968 Fuel Consumption: 48.4716\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -661.3764587793735 Explore P: 0.1761 SOC: 0.6488 Cumulative_SOC_deviation: 61.2953 Fuel Consumption: 48.4235\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -471.037638836867 Explore P: 0.1716 SOC: 0.6505 Cumulative_SOC_deviation: 42.2877 Fuel Consumption: 48.1610\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -553.1865011102926 Explore P: 0.1673 SOC: 0.6391 Cumulative_SOC_deviation: 50.5682 Fuel Consumption: 47.5047\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -482.84089562774903 Explore P: 0.1630 SOC: 0.6519 Cumulative_SOC_deviation: 43.4574 Fuel Consumption: 48.2669\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -577.5323962519448 Explore P: 0.1589 SOC: 0.6946 Cumulative_SOC_deviation: 52.5682 Fuel Consumption: 51.8503\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -416.56682105296466 Explore P: 0.1548 SOC: 0.6394 Cumulative_SOC_deviation: 36.9346 Fuel Consumption: 47.2209\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -418.6681311356438 Explore P: 0.1509 SOC: 0.6351 Cumulative_SOC_deviation: 37.1644 Fuel Consumption: 47.0237\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -298.70954808761945 Explore P: 0.1471 SOC: 0.6251 Cumulative_SOC_deviation: 25.2455 Fuel Consumption: 46.2550\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -348.28979886492687 Explore P: 0.1434 SOC: 0.6180 Cumulative_SOC_deviation: 30.2369 Fuel Consumption: 45.9210\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-043062882ea0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprimary_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Song\\graduate_paper\\program\\experiment\\DDQN3_ver2\\vehicle_model_DDQN3_ref.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[0mj_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_curdensity_region\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_mot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m             \u001b[0mj_fc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculation_comp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'action_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[0mcell_voltage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_voltage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj_fc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Song\\graduate_paper\\program\\experiment\\DDQN3_ver2\\vehicle_model_DDQN3_ref.py\u001b[0m in \u001b[0;36mget_curdensity_region\u001b[1;34m(self, p_mot)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0mP_battery_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp_mot\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mP_stack_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m         \u001b[0mcondition_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcondition_check_battery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_bat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp_bat\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mP_battery_set\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition_set\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Song\\graduate_paper\\program\\experiment\\DDQN3_ver2\\vehicle_model_DDQN3_ref.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0mP_battery_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp_mot\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mP_stack_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m         \u001b[0mcondition_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcondition_check_battery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_bat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp_bat\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mP_battery_set\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition_set\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Song\\graduate_paper\\program\\experiment\\DDQN3_ver2\\vehicle_model_DDQN3_ref.py\u001b[0m in \u001b[0;36mcondition_check_battery\u001b[1;34m(self, p_bat)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcondition_check_battery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_bat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m         \u001b[0mv_dis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_cha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_dis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_cha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi_lim_dis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi_lim_cha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_battery_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m         del_i = (1 / (2 * r_cha)) * (v_cha - (v_cha ** 2 - 4 * r_cha * p_bat) ** (0.5)) * (p_bat < 0) + (1 / (\n\u001b[0;32m    240\u001b[0m                 2 * r_dis)) * (v_dis - (v_dis ** 2 - 4 * r_dis * p_bat) ** (0.5)) * (p_bat >= 0)\n",
      "\u001b[1;32m~\\Desktop\\Song\\graduate_paper\\program\\experiment\\DDQN3_ver2\\vehicle_model_DDQN3_ref.py\u001b[0m in \u001b[0;36mget_battery_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    253\u001b[0m                                      fill_value='extrapolate')(self.SOC)\n\u001b[0;32m    254\u001b[0m         r_cha = interpolate.interp1d(self.battery['SOC_ind'], self.battery['Res_cha'], assume_sorted=False,\n\u001b[1;32m--> 255\u001b[1;33m                                      fill_value='extrapolate')(self.SOC)\n\u001b[0m\u001b[0;32m    256\u001b[0m         i_lim_dis = interpolate.interp1d(self.battery['SOC_ind'], self.battery['Cur_lim_dis'], assume_sorted=False,\n\u001b[0;32m    257\u001b[0m                                          fill_value='extrapolate')(self.SOC)\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\scipy\\interpolate\\polyint.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \"\"\"\n\u001b[0;32m     78\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_x\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_finish_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\scipy\\interpolate\\interpolate.py\u001b[0m in \u001b[0;36m_evaluate\u001b[1;34m(self, x_new)\u001b[0m\n\u001b[0;32m    659\u001b[0m         \u001b[1;31m#    The behavior is set by the bounds_error variable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m         \u001b[0mx_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m         \u001b[0my_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    662\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extrapolate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m             \u001b[0mbelow_bounds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabove_bounds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_bounds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\scipy\\interpolate\\interpolate.py\u001b[0m in \u001b[0;36m_call_linear\u001b[1;34m(self, x_new)\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;31m#    self.x indices and at least 1.  Removes mis-interpolation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[1;31m#    of x_new[n] = x[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m         \u001b[0mx_new_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_new_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# 4. Calculate the slope of regions that each x_new value falls in.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "for size, history in results_dict.items(): \n",
    "    plt.plot(history, label=size, linewidth=3.0) \n",
    "\n",
    "\n",
    "plt.grid() \n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel(\"episode number\", fontsize=20) \n",
    "plt.ylabel(\"total rewards\", fontsize=20) \n",
    "plt.xlim([0, 120])\n",
    "\n",
    "\n",
    "plt.savefig(\"replay_memory_size_effect_300.png\")\n",
    "with open(\"replay_memory_size_effect_300.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-(3 > 2) * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

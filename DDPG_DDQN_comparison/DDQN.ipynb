{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: ref\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer dense_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -1024.6968674017444 Explore P: 0.9217 SOC: 0.8146 Cumulative_SOC_deviation: 96.2629 Fuel Consumption: 62.0676\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -871.799023598039 Explore P: 0.8970 SOC: 0.7787 Cumulative_SOC_deviation: 81.2599 Fuel Consumption: 59.2002\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -835.5173353805644 Explore P: 0.8730 SOC: 0.7777 Cumulative_SOC_deviation: 77.6475 Fuel Consumption: 59.0426\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -809.9011767505854 Explore P: 0.8496 SOC: 0.7554 Cumulative_SOC_deviation: 75.2696 Fuel Consumption: 57.2055\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -769.4744501292995 Explore P: 0.8269 SOC: 0.7548 Cumulative_SOC_deviation: 71.2384 Fuel Consumption: 57.0905\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -730.8341372748291 Explore P: 0.8048 SOC: 0.7046 Cumulative_SOC_deviation: 67.7781 Fuel Consumption: 53.0529\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -751.066218705238 Explore P: 0.7832 SOC: 0.7058 Cumulative_SOC_deviation: 69.7820 Fuel Consumption: 53.2465\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -720.1031794329491 Explore P: 0.7623 SOC: 0.6769 Cumulative_SOC_deviation: 66.9129 Fuel Consumption: 50.9746\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -827.263897828845 Explore P: 0.7419 SOC: 0.6746 Cumulative_SOC_deviation: 77.6558 Fuel Consumption: 50.7060\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -782.7480799925689 Explore P: 0.7221 SOC: 0.6521 Cumulative_SOC_deviation: 73.3722 Fuel Consumption: 49.0265\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -684.3665144154241 Explore P: 0.7028 SOC: 0.6799 Cumulative_SOC_deviation: 63.2907 Fuel Consumption: 51.4596\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -802.0890217402801 Explore P: 0.6840 SOC: 0.6873 Cumulative_SOC_deviation: 75.0045 Fuel Consumption: 52.0444\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -727.646280228176 Explore P: 0.6658 SOC: 0.7059 Cumulative_SOC_deviation: 67.4013 Fuel Consumption: 53.6332\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -613.6257812319296 Explore P: 0.6480 SOC: 0.6631 Cumulative_SOC_deviation: 56.3569 Fuel Consumption: 50.0568\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -783.4796322526754 Explore P: 0.6307 SOC: 0.6287 Cumulative_SOC_deviation: 73.6166 Fuel Consumption: 47.3141\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -633.8091288827072 Explore P: 0.6139 SOC: 0.6435 Cumulative_SOC_deviation: 58.5311 Fuel Consumption: 48.4985\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -693.0461869858867 Explore P: 0.5976 SOC: 0.6293 Cumulative_SOC_deviation: 64.5805 Fuel Consumption: 47.2417\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -649.1113769125914 Explore P: 0.5816 SOC: 0.6356 Cumulative_SOC_deviation: 60.1311 Fuel Consumption: 47.8001\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -687.3322372304895 Explore P: 0.5662 SOC: 0.6266 Cumulative_SOC_deviation: 64.0278 Fuel Consumption: 47.0542\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -739.6505862045362 Explore P: 0.5511 SOC: 0.6281 Cumulative_SOC_deviation: 69.2526 Fuel Consumption: 47.1241\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -751.8423738785682 Explore P: 0.5364 SOC: 0.6153 Cumulative_SOC_deviation: 70.5866 Fuel Consumption: 45.9762\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -671.1650552173154 Explore P: 0.5222 SOC: 0.6561 Cumulative_SOC_deviation: 62.2101 Fuel Consumption: 49.0640\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -576.6064475094216 Explore P: 0.5083 SOC: 0.6353 Cumulative_SOC_deviation: 52.9058 Fuel Consumption: 47.5488\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -705.4175002093327 Explore P: 0.4948 SOC: 0.6717 Cumulative_SOC_deviation: 65.5077 Fuel Consumption: 50.3401\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -561.831304476791 Explore P: 0.4817 SOC: 0.6790 Cumulative_SOC_deviation: 51.1024 Fuel Consumption: 50.8074\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -526.4466480983003 Explore P: 0.4689 SOC: 0.6781 Cumulative_SOC_deviation: 47.5501 Fuel Consumption: 50.9457\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -596.4020754548527 Explore P: 0.4565 SOC: 0.6604 Cumulative_SOC_deviation: 54.6773 Fuel Consumption: 49.6291\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -600.1118775405641 Explore P: 0.4444 SOC: 0.6377 Cumulative_SOC_deviation: 55.2404 Fuel Consumption: 47.7079\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -634.613304704649 Explore P: 0.4326 SOC: 0.6631 Cumulative_SOC_deviation: 58.5057 Fuel Consumption: 49.5561\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -661.6164549123986 Explore P: 0.4212 SOC: 0.6339 Cumulative_SOC_deviation: 61.4076 Fuel Consumption: 47.5407\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -538.8628312198744 Explore P: 0.4100 SOC: 0.6733 Cumulative_SOC_deviation: 48.8359 Fuel Consumption: 50.5043\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -592.538153982076 Explore P: 0.3992 SOC: 0.6665 Cumulative_SOC_deviation: 54.2551 Fuel Consumption: 49.9874\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -673.9670114570387 Explore P: 0.3887 SOC: 0.6480 Cumulative_SOC_deviation: 62.5436 Fuel Consumption: 48.5314\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -618.2473143890422 Explore P: 0.3784 SOC: 0.6340 Cumulative_SOC_deviation: 57.0791 Fuel Consumption: 47.4565\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -514.332142032849 Explore P: 0.3684 SOC: 0.6252 Cumulative_SOC_deviation: 46.7380 Fuel Consumption: 46.9523\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -525.8871927863923 Explore P: 0.3587 SOC: 0.6506 Cumulative_SOC_deviation: 47.7012 Fuel Consumption: 48.8750\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -575.7855294911493 Explore P: 0.3493 SOC: 0.6388 Cumulative_SOC_deviation: 52.7769 Fuel Consumption: 48.0165\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -464.30875918619694 Explore P: 0.3401 SOC: 0.6341 Cumulative_SOC_deviation: 41.6721 Fuel Consumption: 47.5873\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -579.7156343973938 Explore P: 0.3311 SOC: 0.6636 Cumulative_SOC_deviation: 53.0274 Fuel Consumption: 49.4416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -581.0428428371848 Explore P: 0.3224 SOC: 0.6502 Cumulative_SOC_deviation: 53.2601 Fuel Consumption: 48.4422\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -554.735849337543 Explore P: 0.3140 SOC: 0.6486 Cumulative_SOC_deviation: 50.6245 Fuel Consumption: 48.4904\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -588.6618275547554 Explore P: 0.3057 SOC: 0.6908 Cumulative_SOC_deviation: 53.6970 Fuel Consumption: 51.6922\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -636.0147223134295 Explore P: 0.2977 SOC: 0.6621 Cumulative_SOC_deviation: 58.6171 Fuel Consumption: 49.8440\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -423.54857963528775 Explore P: 0.2899 SOC: 0.6413 Cumulative_SOC_deviation: 37.5870 Fuel Consumption: 47.6785\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -420.9666937100531 Explore P: 0.2824 SOC: 0.6315 Cumulative_SOC_deviation: 37.3704 Fuel Consumption: 47.2624\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -769.8676712861703 Explore P: 0.2750 SOC: 0.6434 Cumulative_SOC_deviation: 72.1648 Fuel Consumption: 48.2197\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -630.59680765528 Explore P: 0.2678 SOC: 0.6490 Cumulative_SOC_deviation: 58.1702 Fuel Consumption: 48.8948\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -507.743004750536 Explore P: 0.2608 SOC: 0.6354 Cumulative_SOC_deviation: 46.0484 Fuel Consumption: 47.2589\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -475.5692405415126 Explore P: 0.2540 SOC: 0.6315 Cumulative_SOC_deviation: 42.8792 Fuel Consumption: 46.7769\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -524.0432455572192 Explore P: 0.2474 SOC: 0.6480 Cumulative_SOC_deviation: 47.5662 Fuel Consumption: 48.3809\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -616.078416110381 Explore P: 0.2410 SOC: 0.6597 Cumulative_SOC_deviation: 56.6845 Fuel Consumption: 49.2333\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -508.94974526490796 Explore P: 0.2347 SOC: 0.6529 Cumulative_SOC_deviation: 46.0379 Fuel Consumption: 48.5709\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -399.9703502306867 Explore P: 0.2286 SOC: 0.6284 Cumulative_SOC_deviation: 35.3272 Fuel Consumption: 46.6979\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -448.0844172287446 Explore P: 0.2227 SOC: 0.6440 Cumulative_SOC_deviation: 40.0228 Fuel Consumption: 47.8564\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -450.3986748228792 Explore P: 0.2170 SOC: 0.6234 Cumulative_SOC_deviation: 40.3984 Fuel Consumption: 46.4151\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -450.25077978203507 Explore P: 0.2114 SOC: 0.6662 Cumulative_SOC_deviation: 40.0627 Fuel Consumption: 49.6240\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -603.6254453252578 Explore P: 0.2059 SOC: 0.6513 Cumulative_SOC_deviation: 55.4679 Fuel Consumption: 48.9460\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -522.2108053907551 Explore P: 0.2006 SOC: 0.6548 Cumulative_SOC_deviation: 47.3480 Fuel Consumption: 48.7306\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

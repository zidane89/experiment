{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG2 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64): \n",
    "        self.power_mean = 0 \n",
    "        self.power_std = 0\n",
    "        self.sum = 0 \n",
    "        self.sum_deviation = 0 \n",
    "        self.N = 0 \n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        self.N += 1 \n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "        power = obs_tuple[0][0] \n",
    "        \n",
    "        self.sum += power \n",
    "        self.power_mean = self.sum / self.N \n",
    "        self.sum_deviation += (power - self.power_mean) ** 2  \n",
    "        self.power_std = np.sqrt(self.sum_deviation / self.N) \n",
    "            \n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        state_batch = self.state_buffer[batch_indices]\n",
    "        power_batch = (state_batch[:, 0] - self.power_mean) / self.power_std\n",
    "        state_batch[:, 0] = power_batch \n",
    "        \n",
    "        next_state_batch = self.next_state_buffer[batch_indices]\n",
    "        power_batch = (next_state_batch[:, 0] - self.power_mean) / self.power_std\n",
    "        next_state_batch[:, 0] = power_batch \n",
    "#         print(state_batch)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(state_batch)\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(next_state_batch)\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_input)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 200\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 15.880\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -936.5783302173362 SOC: 0.8065 Cumulative_SOC_deviation: 87.5035 Fuel Consumption: 61.5431 Mean: 2.1068, STD: 5.0094\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 16.178\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -1049.3265296846062 SOC: 0.8066 Cumulative_SOC_deviation: 98.7724 Fuel Consumption: 61.6030 Mean: 2.1068, STD: 5.0131\n",
      "WARNING:tensorflow:Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_13 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 56.408\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -729.8824731044655 SOC: 0.7467 Cumulative_SOC_deviation: 67.3206 Fuel Consumption: 56.6765 Mean: 2.1068, STD: 5.0145\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 63.954\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -742.788686813894 SOC: 0.7413 Cumulative_SOC_deviation: 68.6424 Fuel Consumption: 56.3646 Mean: 2.1068, STD: 5.0153\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.271\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -758.6179799452369 SOC: 0.7299 Cumulative_SOC_deviation: 70.3097 Fuel Consumption: 55.5206 Mean: 2.1068, STD: 5.0157\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.426\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -760.4315028185513 SOC: 0.7201 Cumulative_SOC_deviation: 70.5662 Fuel Consumption: 54.7700 Mean: 2.1068, STD: 5.0161\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.250\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -877.8140482498488 SOC: 0.6531 Cumulative_SOC_deviation: 82.8229 Fuel Consumption: 49.5853 Mean: 2.1068, STD: 5.0163\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.812\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -865.1756410344478 SOC: 0.6458 Cumulative_SOC_deviation: 81.6330 Fuel Consumption: 48.8454 Mean: 2.1068, STD: 5.0165\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 60.315\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -884.7371486067265 SOC: 0.6290 Cumulative_SOC_deviation: 83.7087 Fuel Consumption: 47.6506 Mean: 2.1068, STD: 5.0167\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 62.998\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -886.3248355739682 SOC: 0.6304 Cumulative_SOC_deviation: 83.8527 Fuel Consumption: 47.7976 Mean: 2.1068, STD: 5.0168\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.259\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -1055.01737398701 SOC: 0.6113 Cumulative_SOC_deviation: 100.8708 Fuel Consumption: 46.3092 Mean: 2.1068, STD: 5.0169\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.837\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -1189.1599225737223 SOC: 0.5970 Cumulative_SOC_deviation: 114.3969 Fuel Consumption: 45.1912 Mean: 2.1068, STD: 5.0170\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.867\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -1140.271672624314 SOC: 0.6038 Cumulative_SOC_deviation: 109.4380 Fuel Consumption: 45.8919 Mean: 2.1068, STD: 5.0171\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.705\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -1354.9319613170508 SOC: 0.5577 Cumulative_SOC_deviation: 131.2708 Fuel Consumption: 42.2244 Mean: 2.1068, STD: 5.0171\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.910\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -1392.9540140544693 SOC: 0.5420 Cumulative_SOC_deviation: 135.1903 Fuel Consumption: 41.0508 Mean: 2.1068, STD: 5.0172\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.064\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -1475.489887192727 SOC: 0.5400 Cumulative_SOC_deviation: 143.4511 Fuel Consumption: 40.9791 Mean: 2.1068, STD: 5.0172\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.535\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -1685.3848092760977 SOC: 0.5092 Cumulative_SOC_deviation: 164.6783 Fuel Consumption: 38.6018 Mean: 2.1068, STD: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.817\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -1672.297954852348 SOC: 0.5090 Cumulative_SOC_deviation: 163.3521 Fuel Consumption: 38.7773 Mean: 2.1068, STD: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.749\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -1669.2700454895207 SOC: 0.5170 Cumulative_SOC_deviation: 162.9809 Fuel Consumption: 39.4609 Mean: 2.1068, STD: 5.0173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.872\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -1927.930282746597 SOC: 0.4766 Cumulative_SOC_deviation: 189.1639 Fuel Consumption: 36.2909 Mean: 2.1068, STD: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.030\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -1813.407513805352 SOC: 0.4899 Cumulative_SOC_deviation: 177.6051 Fuel Consumption: 37.3565 Mean: 2.1068, STD: 5.0174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 56.694\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -2124.9982613602906 SOC: 0.4545 Cumulative_SOC_deviation: 209.0442 Fuel Consumption: 34.5566 Mean: 2.1068, STD: 5.0174\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.105\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -2257.623528117878 SOC: 0.4426 Cumulative_SOC_deviation: 222.3793 Fuel Consumption: 33.8302 Mean: 2.1068, STD: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.335\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -2221.8656824974314 SOC: 0.4274 Cumulative_SOC_deviation: 218.9182 Fuel Consumption: 32.6833 Mean: 2.1068, STD: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.530\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -2246.3108769564624 SOC: 0.5396 Cumulative_SOC_deviation: 220.4704 Fuel Consumption: 41.6071 Mean: 2.1068, STD: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.446\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -723.0402614815155 SOC: 0.7165 Cumulative_SOC_deviation: 66.9153 Fuel Consumption: 53.8873 Mean: 2.1068, STD: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.209\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -649.6582383142976 SOC: 0.6877 Cumulative_SOC_deviation: 59.7895 Fuel Consumption: 51.7629 Mean: 2.1068, STD: 5.0175\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.272\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -578.4308158728882 SOC: 0.6792 Cumulative_SOC_deviation: 52.7396 Fuel Consumption: 51.0352 Mean: 2.1068, STD: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.256\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -616.7603113129577 SOC: 0.6814 Cumulative_SOC_deviation: 56.5309 Fuel Consumption: 51.4511 Mean: 2.1068, STD: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.350\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -589.5678838341062 SOC: 0.6858 Cumulative_SOC_deviation: 53.7792 Fuel Consumption: 51.7754 Mean: 2.1068, STD: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.475\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -533.813619984506 SOC: 0.6625 Cumulative_SOC_deviation: 48.3901 Fuel Consumption: 49.9125 Mean: 2.1068, STD: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.258\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -507.53574254373314 SOC: 0.6567 Cumulative_SOC_deviation: 45.7972 Fuel Consumption: 49.5641 Mean: 2.1068, STD: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.462\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -525.8330604062767 SOC: 0.6613 Cumulative_SOC_deviation: 47.5827 Fuel Consumption: 50.0065 Mean: 2.1068, STD: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.320\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -463.75311554155627 SOC: 0.6478 Cumulative_SOC_deviation: 41.4962 Fuel Consumption: 48.7907 Mean: 2.1068, STD: 5.0176\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.360\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -468.87045775363345 SOC: 0.6494 Cumulative_SOC_deviation: 41.9979 Fuel Consumption: 48.8915 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.411\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -459.9218237109261 SOC: 0.6510 Cumulative_SOC_deviation: 41.0920 Fuel Consumption: 49.0023 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.689\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -427.5645351196208 SOC: 0.6417 Cumulative_SOC_deviation: 37.9378 Fuel Consumption: 48.1861 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.249\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -442.31665833804345 SOC: 0.6414 Cumulative_SOC_deviation: 39.4043 Fuel Consumption: 48.2739 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.363\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -450.26919113594806 SOC: 0.6537 Cumulative_SOC_deviation: 40.1081 Fuel Consumption: 49.1880 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.469\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -421.99073275425843 SOC: 0.6412 Cumulative_SOC_deviation: 37.3596 Fuel Consumption: 48.3952 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.537\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -438.5735928962626 SOC: 0.6396 Cumulative_SOC_deviation: 39.0361 Fuel Consumption: 48.2122 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.422\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -456.2789452521621 SOC: 0.6475 Cumulative_SOC_deviation: 40.7508 Fuel Consumption: 48.7713 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.293\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -426.09619612716665 SOC: 0.6440 Cumulative_SOC_deviation: 37.7664 Fuel Consumption: 48.4320 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.584\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -421.0018823211901 SOC: 0.6457 Cumulative_SOC_deviation: 37.2427 Fuel Consumption: 48.5749 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.683\n",
      "Episode: 45 Exploration P: 0.2977 Total reward: -414.28046106249644 SOC: 0.6402 Cumulative_SOC_deviation: 36.6133 Fuel Consumption: 48.1473 Mean: 2.1068, STD: 5.0177\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.621\n",
      "Episode: 46 Exploration P: 0.2899 Total reward: -401.17687429074874 SOC: 0.6451 Cumulative_SOC_deviation: 35.2727 Fuel Consumption: 48.4497 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.443\n",
      "Episode: 47 Exploration P: 0.2824 Total reward: -371.58996317159404 SOC: 0.6325 Cumulative_SOC_deviation: 32.4085 Fuel Consumption: 47.5050 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.336\n",
      "Episode: 48 Exploration P: 0.2750 Total reward: -393.00643716539435 SOC: 0.6329 Cumulative_SOC_deviation: 34.5449 Fuel Consumption: 47.5574 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.511\n",
      "Episode: 49 Exploration P: 0.2678 Total reward: -421.4171081849166 SOC: 0.6428 Cumulative_SOC_deviation: 37.3085 Fuel Consumption: 48.3322 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.583\n",
      "Episode: 50 Exploration P: 0.2608 Total reward: -430.9495737246978 SOC: 0.6394 Cumulative_SOC_deviation: 38.2775 Fuel Consumption: 48.1746 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.637\n",
      "Episode: 51 Exploration P: 0.2540 Total reward: -403.27239323543404 SOC: 0.6354 Cumulative_SOC_deviation: 35.5486 Fuel Consumption: 47.7866 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 68.201\n",
      "Episode: 52 Exploration P: 0.2474 Total reward: -380.2395507164224 SOC: 0.6343 Cumulative_SOC_deviation: 33.2715 Fuel Consumption: 47.5242 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 68.113\n",
      "Episode: 53 Exploration P: 0.2410 Total reward: -378.12248625555634 SOC: 0.6287 Cumulative_SOC_deviation: 33.1009 Fuel Consumption: 47.1132 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.507\n",
      "Episode: 54 Exploration P: 0.2347 Total reward: -380.4139327838147 SOC: 0.6307 Cumulative_SOC_deviation: 33.3089 Fuel Consumption: 47.3254 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.096\n",
      "Episode: 55 Exploration P: 0.2286 Total reward: -377.99104219550884 SOC: 0.6283 Cumulative_SOC_deviation: 33.0896 Fuel Consumption: 47.0951 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.461\n",
      "Episode: 56 Exploration P: 0.2227 Total reward: -368.253791139266 SOC: 0.6263 Cumulative_SOC_deviation: 32.1232 Fuel Consumption: 47.0219 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.334\n",
      "Episode: 57 Exploration P: 0.2170 Total reward: -383.7411415861079 SOC: 0.6394 Cumulative_SOC_deviation: 33.5822 Fuel Consumption: 47.9191 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.418\n",
      "Episode: 58 Exploration P: 0.2114 Total reward: -370.88894181865874 SOC: 0.6365 Cumulative_SOC_deviation: 32.3191 Fuel Consumption: 47.6983 Mean: 2.1068, STD: 5.0178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 68.168\n",
      "Episode: 59 Exploration P: 0.2059 Total reward: -387.6699903972658 SOC: 0.6325 Cumulative_SOC_deviation: 34.0267 Fuel Consumption: 47.4030 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 68.043\n",
      "Episode: 60 Exploration P: 0.2006 Total reward: -339.0815472096962 SOC: 0.6296 Cumulative_SOC_deviation: 29.1984 Fuel Consumption: 47.0972 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 70.148\n",
      "Episode: 61 Exploration P: 0.1954 Total reward: -324.7600159218198 SOC: 0.6201 Cumulative_SOC_deviation: 27.8259 Fuel Consumption: 46.5010 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 68.222\n",
      "Episode: 62 Exploration P: 0.1904 Total reward: -310.6116833211705 SOC: 0.6307 Cumulative_SOC_deviation: 26.3381 Fuel Consumption: 47.2310 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.788\n",
      "Episode: 63 Exploration P: 0.1855 Total reward: -348.821666821921 SOC: 0.6286 Cumulative_SOC_deviation: 30.1797 Fuel Consumption: 47.0248 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.471\n",
      "Episode: 64 Exploration P: 0.1808 Total reward: -343.8976116547006 SOC: 0.6338 Cumulative_SOC_deviation: 29.6506 Fuel Consumption: 47.3916 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.265\n",
      "Episode: 65 Exploration P: 0.1761 Total reward: -346.2416894264308 SOC: 0.6252 Cumulative_SOC_deviation: 29.9544 Fuel Consumption: 46.6974 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 70.569\n",
      "Episode: 66 Exploration P: 0.1716 Total reward: -384.11213467721933 SOC: 0.6364 Cumulative_SOC_deviation: 33.6292 Fuel Consumption: 47.8199 Mean: 2.1068, STD: 5.0178\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.340\n",
      "Episode: 67 Exploration P: 0.1673 Total reward: -348.90821997624255 SOC: 0.6255 Cumulative_SOC_deviation: 30.2125 Fuel Consumption: 46.7829 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.435\n",
      "Episode: 68 Exploration P: 0.1630 Total reward: -319.19002901367696 SOC: 0.6249 Cumulative_SOC_deviation: 27.2557 Fuel Consumption: 46.6327 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 68.638\n",
      "Episode: 69 Exploration P: 0.1589 Total reward: -357.23232809067576 SOC: 0.6312 Cumulative_SOC_deviation: 31.0049 Fuel Consumption: 47.1829 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.554\n",
      "Episode: 70 Exploration P: 0.1548 Total reward: -340.2828384748239 SOC: 0.6236 Cumulative_SOC_deviation: 29.3906 Fuel Consumption: 46.3770 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 68.887\n",
      "Episode: 71 Exploration P: 0.1509 Total reward: -304.66368289018897 SOC: 0.6227 Cumulative_SOC_deviation: 25.8456 Fuel Consumption: 46.2076 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.695\n",
      "Episode: 72 Exploration P: 0.1471 Total reward: -306.27092037340856 SOC: 0.6222 Cumulative_SOC_deviation: 26.0079 Fuel Consumption: 46.1920 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.408\n",
      "Episode: 73 Exploration P: 0.1434 Total reward: -318.70857020542945 SOC: 0.6230 Cumulative_SOC_deviation: 27.2335 Fuel Consumption: 46.3732 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.412\n",
      "Episode: 74 Exploration P: 0.1398 Total reward: -341.7737389402791 SOC: 0.6323 Cumulative_SOC_deviation: 29.4689 Fuel Consumption: 47.0844 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 68.153\n",
      "Episode: 75 Exploration P: 0.1362 Total reward: -308.88248247642184 SOC: 0.6205 Cumulative_SOC_deviation: 26.2773 Fuel Consumption: 46.1096 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 68.135\n",
      "Episode: 76 Exploration P: 0.1328 Total reward: -290.2651889242647 SOC: 0.6197 Cumulative_SOC_deviation: 24.4321 Fuel Consumption: 45.9443 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.768\n",
      "Episode: 77 Exploration P: 0.1295 Total reward: -312.2545831130516 SOC: 0.6232 Cumulative_SOC_deviation: 26.5912 Fuel Consumption: 46.3426 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 68.971\n",
      "Episode: 78 Exploration P: 0.1263 Total reward: -343.94195806083 SOC: 0.6287 Cumulative_SOC_deviation: 29.7083 Fuel Consumption: 46.8594 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.787\n",
      "Episode: 79 Exploration P: 0.1231 Total reward: -312.5596600220301 SOC: 0.6218 Cumulative_SOC_deviation: 26.6261 Fuel Consumption: 46.2989 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 68.274\n",
      "Episode: 80 Exploration P: 0.1200 Total reward: -291.6894334280292 SOC: 0.6250 Cumulative_SOC_deviation: 24.5333 Fuel Consumption: 46.3561 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 67.358\n",
      "Episode: 81 Exploration P: 0.1171 Total reward: -306.9561109288068 SOC: 0.6213 Cumulative_SOC_deviation: 26.0825 Fuel Consumption: 46.1314 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 70.237\n",
      "Episode: 82 Exploration P: 0.1142 Total reward: -291.482284787337 SOC: 0.6231 Cumulative_SOC_deviation: 24.5287 Fuel Consumption: 46.1953 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 58.149\n",
      "Episode: 83 Exploration P: 0.1113 Total reward: -329.4822874539333 SOC: 0.6223 Cumulative_SOC_deviation: 28.3125 Fuel Consumption: 46.3572 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 56.502\n",
      "Episode: 84 Exploration P: 0.1086 Total reward: -308.1921133304016 SOC: 0.6218 Cumulative_SOC_deviation: 26.2086 Fuel Consumption: 46.1062 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.886\n",
      "Episode: 85 Exploration P: 0.1059 Total reward: -315.06281695146475 SOC: 0.6233 Cumulative_SOC_deviation: 26.8636 Fuel Consumption: 46.4270 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 56.690\n",
      "Episode: 86 Exploration P: 0.1033 Total reward: -317.88301157743115 SOC: 0.6226 Cumulative_SOC_deviation: 27.1615 Fuel Consumption: 46.2683 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.526\n",
      "Episode: 87 Exploration P: 0.1008 Total reward: -306.4909105303508 SOC: 0.6219 Cumulative_SOC_deviation: 26.0325 Fuel Consumption: 46.1660 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.433\n",
      "Episode: 88 Exploration P: 0.0983 Total reward: -316.51894599976504 SOC: 0.6258 Cumulative_SOC_deviation: 26.9953 Fuel Consumption: 46.5662 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 53.691\n",
      "Episode: 89 Exploration P: 0.0960 Total reward: -298.23260791799515 SOC: 0.6188 Cumulative_SOC_deviation: 25.2408 Fuel Consumption: 45.8246 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 55.139\n",
      "Episode: 90 Exploration P: 0.0936 Total reward: -303.9042316381393 SOC: 0.6191 Cumulative_SOC_deviation: 25.7980 Fuel Consumption: 45.9245 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 55.790\n",
      "Episode: 91 Exploration P: 0.0914 Total reward: -310.21539916098556 SOC: 0.6274 Cumulative_SOC_deviation: 26.3520 Fuel Consumption: 46.6954 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 57.753\n",
      "Episode: 92 Exploration P: 0.0892 Total reward: -298.73615887106956 SOC: 0.6201 Cumulative_SOC_deviation: 25.2431 Fuel Consumption: 46.3055 Mean: 2.1068, STD: 5.0179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.596\n",
      "Episode: 93 Exploration P: 0.0870 Total reward: -301.6703308189891 SOC: 0.6218 Cumulative_SOC_deviation: 25.5200 Fuel Consumption: 46.4705 Mean: 2.1068, STD: 5.0179\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e9fd2cec96dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mtf_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_epsilon_greedy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;31m#         print(action)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-7299d92a1910>\u001b[0m in \u001b[0;36mpolicy_epsilon_greedy\u001b[1;34m(state, eps)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0msampled_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mlegal_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msampled_action\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mj_max\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mlegal_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlegal_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36msqueeze_v2\u001b[1;34m(input, axis, name)\u001b[0m\n\u001b[0;32m   3699\u001b[0m   \"\"\"\n\u001b[0;32m   3700\u001b[0m   \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3701\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 instructions)\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36msqueeze\u001b[1;34m(input, axis, name, squeeze_dims)\u001b[0m\n\u001b[0;32m   3647\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3648\u001b[0m     \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3649\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36msqueeze\u001b[1;34m(input, axis, name)\u001b[0m\n\u001b[0;32m  10046\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m  10047\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Squeeze\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10048\u001b[1;33m         name, _ctx._post_execution_callbacks, input, \"squeeze_dims\", axis)\n\u001b[0m\u001b[0;32m  10049\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10050\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 3\n",
    "results_dict = {} \n",
    "for trial in range(num_trials): \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer = initialization()\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    for ep in range(total_episodes): \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "              \"Mean: {:.4f}, STD: {:.4f}\".format(buffer.power_mean, buffer.power_std)\n",
    "        )\n",
    "\n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG2.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

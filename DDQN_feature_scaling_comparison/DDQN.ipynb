{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN_SOC_dev import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 1000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 3\n",
      "WARNING:tensorflow:Layer dense_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 1 Total reward: -991.8552231993656 Explore P: 0.9732 SOC: 0.8102 Cumulative_SOC_deviation: 93.0113 Fuel Consumption: 61.7427\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 2 Total reward: -941.8503278572666 Explore P: 0.9471 SOC: 0.7891 Cumulative_SOC_deviation: 88.1937 Fuel Consumption: 59.9137\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -1020.7120294318878 Explore P: 0.9217 SOC: 0.7976 Cumulative_SOC_deviation: 96.0088 Fuel Consumption: 60.6242\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -971.6961625802862 Explore P: 0.8970 SOC: 0.7998 Cumulative_SOC_deviation: 91.0999 Fuel Consumption: 60.6967\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -950.2715168114964 Explore P: 0.8730 SOC: 0.7903 Cumulative_SOC_deviation: 89.0319 Fuel Consumption: 59.9523\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -928.0794197511658 Explore P: 0.8496 SOC: 0.7809 Cumulative_SOC_deviation: 86.8817 Fuel Consumption: 59.2620\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -988.0080465524138 Explore P: 0.8269 SOC: 0.8072 Cumulative_SOC_deviation: 92.6670 Fuel Consumption: 61.3378\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -1025.8871185048192 Explore P: 0.8048 SOC: 0.8081 Cumulative_SOC_deviation: 96.4448 Fuel Consumption: 61.4386\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -1095.3397868918366 Explore P: 0.7832 SOC: 0.8303 Cumulative_SOC_deviation: 103.1964 Fuel Consumption: 63.3761\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -985.3081194078321 Explore P: 0.7623 SOC: 0.8047 Cumulative_SOC_deviation: 92.3913 Fuel Consumption: 61.3956\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -1004.1683455415671 Explore P: 0.7419 SOC: 0.8029 Cumulative_SOC_deviation: 94.3105 Fuel Consumption: 61.0631\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -822.7944108569995 Explore P: 0.7221 SOC: 0.7570 Cumulative_SOC_deviation: 76.5452 Fuel Consumption: 57.3423\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -881.9854483805544 Explore P: 0.7028 SOC: 0.7887 Cumulative_SOC_deviation: 82.2205 Fuel Consumption: 59.7805\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -817.403940691908 Explore P: 0.6840 SOC: 0.7554 Cumulative_SOC_deviation: 76.0342 Fuel Consumption: 57.0621\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -925.1249915867636 Explore P: 0.6658 SOC: 0.7939 Cumulative_SOC_deviation: 86.4836 Fuel Consumption: 60.2888\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -841.6988776827955 Explore P: 0.6480 SOC: 0.7565 Cumulative_SOC_deviation: 78.4503 Fuel Consumption: 57.1959\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -835.6563432501359 Explore P: 0.6307 SOC: 0.7716 Cumulative_SOC_deviation: 77.7226 Fuel Consumption: 58.4304\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -1144.7005097937408 Explore P: 0.6139 SOC: 0.8286 Cumulative_SOC_deviation: 108.1186 Fuel Consumption: 63.5144\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -945.4079513755894 Explore P: 0.5976 SOC: 0.8031 Cumulative_SOC_deviation: 88.4045 Fuel Consumption: 61.3631\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -1206.6798634718937 Explore P: 0.5816 SOC: 0.8457 Cumulative_SOC_deviation: 114.1604 Fuel Consumption: 65.0758\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -1161.9987934048684 Explore P: 0.5662 SOC: 0.8286 Cumulative_SOC_deviation: 109.8636 Fuel Consumption: 63.3624\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -1001.0065781845248 Explore P: 0.5511 SOC: 0.8109 Cumulative_SOC_deviation: 93.9155 Fuel Consumption: 61.8517\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -990.2799869698268 Explore P: 0.5364 SOC: 0.8123 Cumulative_SOC_deviation: 92.8615 Fuel Consumption: 61.6647\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -932.5335100477316 Explore P: 0.5222 SOC: 0.7951 Cumulative_SOC_deviation: 87.2234 Fuel Consumption: 60.2994\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -949.055959387537 Explore P: 0.5083 SOC: 0.7875 Cumulative_SOC_deviation: 88.9322 Fuel Consumption: 59.7338\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -1058.4935200733073 Explore P: 0.4948 SOC: 0.8124 Cumulative_SOC_deviation: 99.6619 Fuel Consumption: 61.8745\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -886.9266567304521 Explore P: 0.4817 SOC: 0.7865 Cumulative_SOC_deviation: 82.7488 Fuel Consumption: 59.4389\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -865.500813391775 Explore P: 0.4689 SOC: 0.7330 Cumulative_SOC_deviation: 81.0227 Fuel Consumption: 55.2742\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -1084.3671092480843 Explore P: 0.4565 SOC: 0.8215 Cumulative_SOC_deviation: 102.1920 Fuel Consumption: 62.4466\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -923.9275044862317 Explore P: 0.4444 SOC: 0.8008 Cumulative_SOC_deviation: 86.3215 Fuel Consumption: 60.7129\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -886.4258658700674 Explore P: 0.4326 SOC: 0.7716 Cumulative_SOC_deviation: 82.8151 Fuel Consumption: 58.2748\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -957.6304668002482 Explore P: 0.4212 SOC: 0.7908 Cumulative_SOC_deviation: 89.7702 Fuel Consumption: 59.9289\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -909.2112395502169 Explore P: 0.4100 SOC: 0.8024 Cumulative_SOC_deviation: 84.8502 Fuel Consumption: 60.7089\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -881.8091853501645 Explore P: 0.3992 SOC: 0.7826 Cumulative_SOC_deviation: 82.2608 Fuel Consumption: 59.2007\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -1081.4858245290752 Explore P: 0.3887 SOC: 0.8184 Cumulative_SOC_deviation: 101.8931 Fuel Consumption: 62.5551\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -1114.6236391131824 Explore P: 0.3784 SOC: 0.8450 Cumulative_SOC_deviation: 104.9767 Fuel Consumption: 64.8569\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -941.9075019272819 Explore P: 0.3684 SOC: 0.7995 Cumulative_SOC_deviation: 88.0915 Fuel Consumption: 60.9927\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -1104.5912350659482 Explore P: 0.3587 SOC: 0.8185 Cumulative_SOC_deviation: 104.2111 Fuel Consumption: 62.4801\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -1006.2402233114703 Explore P: 0.3493 SOC: 0.7805 Cumulative_SOC_deviation: 94.6988 Fuel Consumption: 59.2519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -831.8058641027955 Explore P: 0.3401 SOC: 0.7327 Cumulative_SOC_deviation: 77.6530 Fuel Consumption: 55.2757\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -872.3983871191703 Explore P: 0.3311 SOC: 0.7859 Cumulative_SOC_deviation: 81.2800 Fuel Consumption: 59.5980\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -748.2492975753456 Explore P: 0.3224 SOC: 0.7393 Cumulative_SOC_deviation: 69.2412 Fuel Consumption: 55.8372\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -860.751698101478 Explore P: 0.3140 SOC: 0.7452 Cumulative_SOC_deviation: 80.4382 Fuel Consumption: 56.3698\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -855.4508835384 Explore P: 0.3057 SOC: 0.7309 Cumulative_SOC_deviation: 80.0249 Fuel Consumption: 55.2022\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -988.0937514146059 Explore P: 0.2977 SOC: 0.7854 Cumulative_SOC_deviation: 92.8767 Fuel Consumption: 59.3264\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -1089.733703203644 Explore P: 0.2899 SOC: 0.8163 Cumulative_SOC_deviation: 102.7248 Fuel Consumption: 62.4857\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -1538.240245545991 Explore P: 0.2824 SOC: 0.8876 Cumulative_SOC_deviation: 146.9911 Fuel Consumption: 68.3294\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -1134.128605167341 Explore P: 0.2750 SOC: 0.8107 Cumulative_SOC_deviation: 107.2270 Fuel Consumption: 61.8588\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -1357.2083890207591 Explore P: 0.2678 SOC: 0.8552 Cumulative_SOC_deviation: 129.1367 Fuel Consumption: 65.8411\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -1233.6382429298733 Explore P: 0.2608 SOC: 0.8638 Cumulative_SOC_deviation: 116.7057 Fuel Consumption: 66.5810\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -1450.389683992685 Explore P: 0.2540 SOC: 0.8728 Cumulative_SOC_deviation: 138.3227 Fuel Consumption: 67.1622\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -1198.0925842798918 Explore P: 0.2474 SOC: 0.8674 Cumulative_SOC_deviation: 113.1107 Fuel Consumption: 66.9859\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -657.0649213880605 Explore P: 0.2410 SOC: 0.7173 Cumulative_SOC_deviation: 60.2763 Fuel Consumption: 54.3016\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -863.2179658759693 Explore P: 0.2347 SOC: 0.6975 Cumulative_SOC_deviation: 81.0736 Fuel Consumption: 52.4823\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -1086.630165503961 Explore P: 0.2286 SOC: 0.7942 Cumulative_SOC_deviation: 102.6273 Fuel Consumption: 60.3567\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -1116.3964691268284 Explore P: 0.2227 SOC: 0.8358 Cumulative_SOC_deviation: 105.3056 Fuel Consumption: 63.3406\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -982.3032977707403 Explore P: 0.2170 SOC: 0.7986 Cumulative_SOC_deviation: 92.1617 Fuel Consumption: 60.6866\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -818.2892491964427 Explore P: 0.2114 SOC: 0.7604 Cumulative_SOC_deviation: 76.0941 Fuel Consumption: 57.3484\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -929.9426231970527 Explore P: 0.2059 SOC: 0.7564 Cumulative_SOC_deviation: 87.2794 Fuel Consumption: 57.1482\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -973.198004656655 Explore P: 0.2006 SOC: 0.7875 Cumulative_SOC_deviation: 91.3575 Fuel Consumption: 59.6228\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -737.5672699407706 Explore P: 0.1954 SOC: 0.7261 Cumulative_SOC_deviation: 68.2924 Fuel Consumption: 54.6437\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -875.3001271499534 Explore P: 0.1904 SOC: 0.7358 Cumulative_SOC_deviation: 81.9948 Fuel Consumption: 55.3522\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -875.7357675439263 Explore P: 0.1855 SOC: 0.7855 Cumulative_SOC_deviation: 81.6172 Fuel Consumption: 59.5636\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -923.242838422806 Explore P: 0.1808 SOC: 0.7786 Cumulative_SOC_deviation: 86.4418 Fuel Consumption: 58.8252\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -1034.9076393201829 Explore P: 0.1761 SOC: 0.8110 Cumulative_SOC_deviation: 97.3773 Fuel Consumption: 61.1346\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -1081.8693930211498 Explore P: 0.1716 SOC: 0.8408 Cumulative_SOC_deviation: 101.7565 Fuel Consumption: 64.3045\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -943.7608677474851 Explore P: 0.1673 SOC: 0.7836 Cumulative_SOC_deviation: 88.4243 Fuel Consumption: 59.5174\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -1036.7892664885499 Explore P: 0.1630 SOC: 0.8190 Cumulative_SOC_deviation: 97.4426 Fuel Consumption: 62.3630\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -823.8495000407605 Explore P: 0.1589 SOC: 0.7534 Cumulative_SOC_deviation: 76.6868 Fuel Consumption: 56.9815\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -959.8279066861687 Explore P: 0.1548 SOC: 0.7993 Cumulative_SOC_deviation: 89.9598 Fuel Consumption: 60.2295\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -735.0166306806349 Explore P: 0.1509 SOC: 0.7005 Cumulative_SOC_deviation: 68.2282 Fuel Consumption: 52.7342\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -764.4449632892989 Explore P: 0.1471 SOC: 0.6938 Cumulative_SOC_deviation: 71.2449 Fuel Consumption: 51.9958\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -747.4087026778255 Explore P: 0.1434 SOC: 0.7342 Cumulative_SOC_deviation: 69.2122 Fuel Consumption: 55.2863\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -706.1204173296818 Explore P: 0.1398 SOC: 0.6892 Cumulative_SOC_deviation: 65.4636 Fuel Consumption: 51.4839\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -715.3796469780333 Explore P: 0.1362 SOC: 0.7181 Cumulative_SOC_deviation: 66.1534 Fuel Consumption: 53.8455\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -856.5620163692317 Explore P: 0.1328 SOC: 0.7402 Cumulative_SOC_deviation: 80.0693 Fuel Consumption: 55.8691\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -887.4716921600982 Explore P: 0.1295 SOC: 0.7277 Cumulative_SOC_deviation: 83.2783 Fuel Consumption: 54.6883\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -739.873974928115 Explore P: 0.1263 SOC: 0.7208 Cumulative_SOC_deviation: 68.5530 Fuel Consumption: 54.3442\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -791.0768190290985 Explore P: 0.1231 SOC: 0.7606 Cumulative_SOC_deviation: 73.3403 Fuel Consumption: 57.6738\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -843.6029435282538 Explore P: 0.1200 SOC: 0.7554 Cumulative_SOC_deviation: 78.6477 Fuel Consumption: 57.1264\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -884.4948698435741 Explore P: 0.1171 SOC: 0.7524 Cumulative_SOC_deviation: 82.7606 Fuel Consumption: 56.8893\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -1074.6546279431057 Explore P: 0.1142 SOC: 0.8252 Cumulative_SOC_deviation: 101.1521 Fuel Consumption: 63.1338\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -741.4781908796939 Explore P: 0.1113 SOC: 0.7369 Cumulative_SOC_deviation: 68.5726 Fuel Consumption: 55.7520\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -853.960844215321 Explore P: 0.1086 SOC: 0.7124 Cumulative_SOC_deviation: 80.0646 Fuel Consumption: 53.3144\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -864.337950731397 Explore P: 0.1059 SOC: 0.7405 Cumulative_SOC_deviation: 80.8600 Fuel Consumption: 55.7379\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -736.4453935452243 Explore P: 0.1033 SOC: 0.7444 Cumulative_SOC_deviation: 68.0399 Fuel Consumption: 56.0465\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -920.5131113524545 Explore P: 0.1008 SOC: 0.7732 Cumulative_SOC_deviation: 86.2196 Fuel Consumption: 58.3174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -696.2516964799481 Explore P: 0.0983 SOC: 0.7183 Cumulative_SOC_deviation: 64.2468 Fuel Consumption: 53.7837\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -684.1149832697204 Explore P: 0.0960 SOC: 0.6952 Cumulative_SOC_deviation: 63.2083 Fuel Consumption: 52.0323\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -806.2906053587953 Explore P: 0.0936 SOC: 0.7054 Cumulative_SOC_deviation: 75.3406 Fuel Consumption: 52.8842\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -758.3520768105654 Explore P: 0.0914 SOC: 0.7020 Cumulative_SOC_deviation: 70.5817 Fuel Consumption: 52.5355\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -775.169247329825 Explore P: 0.0892 SOC: 0.7146 Cumulative_SOC_deviation: 72.1672 Fuel Consumption: 53.4969\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -631.2324196222174 Explore P: 0.0870 SOC: 0.7088 Cumulative_SOC_deviation: 57.8069 Fuel Consumption: 53.1634\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -785.6482901735747 Explore P: 0.0849 SOC: 0.7181 Cumulative_SOC_deviation: 73.1707 Fuel Consumption: 53.9408\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -891.0184341867904 Explore P: 0.0829 SOC: 0.7271 Cumulative_SOC_deviation: 83.6303 Fuel Consumption: 54.7152\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -897.3156058276761 Explore P: 0.0809 SOC: 0.7764 Cumulative_SOC_deviation: 83.8694 Fuel Consumption: 58.6220\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -839.0139245516447 Explore P: 0.0790 SOC: 0.7399 Cumulative_SOC_deviation: 78.3567 Fuel Consumption: 55.4465\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -752.4428523349687 Explore P: 0.0771 SOC: 0.7198 Cumulative_SOC_deviation: 69.8264 Fuel Consumption: 54.1785\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -790.1319255850462 Explore P: 0.0753 SOC: 0.7192 Cumulative_SOC_deviation: 73.5992 Fuel Consumption: 54.1400\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -759.7641055252158 Explore P: 0.0735 SOC: 0.7194 Cumulative_SOC_deviation: 70.5511 Fuel Consumption: 54.2535\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -837.2734977947453 Explore P: 0.0718 SOC: 0.7131 Cumulative_SOC_deviation: 78.3704 Fuel Consumption: 53.5698\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -857.4473771853635 Explore P: 0.0701 SOC: 0.7559 Cumulative_SOC_deviation: 80.0193 Fuel Consumption: 57.2547\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -886.2484793061867 Explore P: 0.0685 SOC: 0.7878 Cumulative_SOC_deviation: 82.6674 Fuel Consumption: 59.5747\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -829.7571080901898 Explore P: 0.0669 SOC: 0.7465 Cumulative_SOC_deviation: 77.3564 Fuel Consumption: 56.1929\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -787.5921685252833 Explore P: 0.0654 SOC: 0.7275 Cumulative_SOC_deviation: 73.2841 Fuel Consumption: 54.7517\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -758.1625658975025 Explore P: 0.0639 SOC: 0.6979 Cumulative_SOC_deviation: 70.5799 Fuel Consumption: 52.3632\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -783.4572569865509 Explore P: 0.0624 SOC: 0.7497 Cumulative_SOC_deviation: 72.7070 Fuel Consumption: 56.3870\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -806.9625613795934 Explore P: 0.0610 SOC: 0.7429 Cumulative_SOC_deviation: 75.1287 Fuel Consumption: 55.6760\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -690.8210526794466 Explore P: 0.0596 SOC: 0.6918 Cumulative_SOC_deviation: 63.9390 Fuel Consumption: 51.4307\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -711.175676918449 Explore P: 0.0583 SOC: 0.7332 Cumulative_SOC_deviation: 65.6061 Fuel Consumption: 55.1150\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -742.7284065837873 Explore P: 0.0570 SOC: 0.7341 Cumulative_SOC_deviation: 68.7633 Fuel Consumption: 55.0958\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -769.0416058056485 Explore P: 0.0557 SOC: 0.7293 Cumulative_SOC_deviation: 71.4188 Fuel Consumption: 54.8535\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -829.6619013291195 Explore P: 0.0545 SOC: 0.7598 Cumulative_SOC_deviation: 77.2540 Fuel Consumption: 57.1219\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -963.5717819773635 Explore P: 0.0533 SOC: 0.8145 Cumulative_SOC_deviation: 90.1913 Fuel Consumption: 61.6586\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -968.4452288195678 Explore P: 0.0521 SOC: 0.7941 Cumulative_SOC_deviation: 90.8541 Fuel Consumption: 59.9046\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -1070.030290998902 Explore P: 0.0510 SOC: 0.8326 Cumulative_SOC_deviation: 100.6416 Fuel Consumption: 63.6143\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -862.365900326345 Explore P: 0.0498 SOC: 0.7768 Cumulative_SOC_deviation: 80.3651 Fuel Consumption: 58.7147\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -1093.6682824703346 Explore P: 0.0488 SOC: 0.8291 Cumulative_SOC_deviation: 103.0523 Fuel Consumption: 63.1455\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -795.1280454067312 Explore P: 0.0477 SOC: 0.7625 Cumulative_SOC_deviation: 73.7575 Fuel Consumption: 57.5526\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -766.0933538654689 Explore P: 0.0467 SOC: 0.7318 Cumulative_SOC_deviation: 71.1204 Fuel Consumption: 54.8890\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -717.0208382779019 Explore P: 0.0457 SOC: 0.7562 Cumulative_SOC_deviation: 66.0269 Fuel Consumption: 56.7519\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -768.115898402424 Explore P: 0.0447 SOC: 0.7432 Cumulative_SOC_deviation: 71.1998 Fuel Consumption: 56.1179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -771.8571258859317 Explore P: 0.0438 SOC: 0.7116 Cumulative_SOC_deviation: 71.8947 Fuel Consumption: 52.9105\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -808.964760677663 Explore P: 0.0429 SOC: 0.7201 Cumulative_SOC_deviation: 75.5360 Fuel Consumption: 53.6048\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -691.4805424586165 Explore P: 0.0420 SOC: 0.7183 Cumulative_SOC_deviation: 63.7585 Fuel Consumption: 53.8952\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -1025.6626546927669 Explore P: 0.0411 SOC: 0.8175 Cumulative_SOC_deviation: 96.2718 Fuel Consumption: 62.9443\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -1273.2590763197836 Explore P: 0.0403 SOC: 0.8708 Cumulative_SOC_deviation: 120.5975 Fuel Consumption: 67.2839\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -1079.831986701296 Explore P: 0.0395 SOC: 0.8525 Cumulative_SOC_deviation: 101.4107 Fuel Consumption: 65.7245\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -954.6321185340306 Explore P: 0.0387 SOC: 0.8024 Cumulative_SOC_deviation: 89.3714 Fuel Consumption: 60.9182\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -1382.8764129410254 Explore P: 0.0379 SOC: 0.8785 Cumulative_SOC_deviation: 131.6665 Fuel Consumption: 66.2115\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -1523.0889199877397 Explore P: 0.0371 SOC: 0.9114 Cumulative_SOC_deviation: 145.4088 Fuel Consumption: 69.0014\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -1455.3625727174806 Explore P: 0.0364 SOC: 0.5052 Cumulative_SOC_deviation: 141.8626 Fuel Consumption: 36.7361\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -557.4585892242975 Explore P: 0.0357 SOC: 0.6802 Cumulative_SOC_deviation: 50.7742 Fuel Consumption: 49.7163\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -1671.9027209399908 Explore P: 0.0350 SOC: 0.9115 Cumulative_SOC_deviation: 160.2562 Fuel Consumption: 69.3409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -1191.901988524425 Explore P: 0.0343 SOC: 0.6554 Cumulative_SOC_deviation: 114.1773 Fuel Consumption: 50.1288\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -1825.193727660532 Explore P: 0.0336 SOC: 0.9138 Cumulative_SOC_deviation: 175.3899 Fuel Consumption: 71.2945\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -2199.940686429622 Explore P: 0.0330 SOC: 1.0000 Cumulative_SOC_deviation: 211.9312 Fuel Consumption: 80.6284\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -1317.911854643511 Explore P: 0.0324 SOC: 0.8387 Cumulative_SOC_deviation: 125.5136 Fuel Consumption: 62.7755\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -1211.6887132269474 Explore P: 0.0318 SOC: 0.8163 Cumulative_SOC_deviation: 115.0440 Fuel Consumption: 61.2486\n",
      "Available condition is not avail... SOC: 0.995269197220357\n",
      "Episode: 140 Total reward: -2310.769006519962 Explore P: 0.0312 SOC: 0.9953 Cumulative_SOC_deviation: 223.3877 Fuel Consumption: 76.8918\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -1283.4747130358965 Explore P: 0.0306 SOC: 0.6305 Cumulative_SOC_deviation: 123.6817 Fuel Consumption: 46.6578\n",
      "Available condition is not avail... SOC: 0.9872145887172006\n",
      "Episode: 142 Total reward: -2563.523005685822 Explore P: 0.0301 SOC: 0.9872 Cumulative_SOC_deviation: 248.5002 Fuel Consumption: 78.5212\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -1671.4090546730272 Explore P: 0.0296 SOC: 0.5179 Cumulative_SOC_deviation: 163.3538 Fuel Consumption: 37.8708\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -1242.4243458293024 Explore P: 0.0290 SOC: 0.5944 Cumulative_SOC_deviation: 119.8765 Fuel Consumption: 43.6597\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -1424.4538168321901 Explore P: 0.0285 SOC: 0.9021 Cumulative_SOC_deviation: 135.5997 Fuel Consumption: 68.4566\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -1844.6057884907354 Explore P: 0.0280 SOC: 0.9959 Cumulative_SOC_deviation: 176.6504 Fuel Consumption: 78.1021\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -1493.634574765516 Explore P: 0.0275 SOC: 0.7440 Cumulative_SOC_deviation: 143.7395 Fuel Consumption: 56.2396\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -1232.7469266457447 Explore P: 0.0271 SOC: 0.7801 Cumulative_SOC_deviation: 117.2935 Fuel Consumption: 59.8119\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -2548.5974214006974 Explore P: 0.0266 SOC: 0.9812 Cumulative_SOC_deviation: 247.3165 Fuel Consumption: 75.4324\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 150 Total reward: -2253.775274059376 Explore P: 0.0262 SOC: 1.0000 Cumulative_SOC_deviation: 217.3071 Fuel Consumption: 80.7039\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -2562.18526317796 Explore P: 0.0257 SOC: 0.5125 Cumulative_SOC_deviation: 252.3559 Fuel Consumption: 38.6264\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -1042.5510349280778 Explore P: 0.0253 SOC: 0.5041 Cumulative_SOC_deviation: 100.6002 Fuel Consumption: 36.5491\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -855.7019640873804 Explore P: 0.0249 SOC: 0.5534 Cumulative_SOC_deviation: 81.5417 Fuel Consumption: 40.2846\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -2191.2758166226504 Explore P: 0.0245 SOC: 0.9003 Cumulative_SOC_deviation: 212.1954 Fuel Consumption: 69.3221\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 155 Total reward: -2247.6295907607223 Explore P: 0.0241 SOC: 1.0000 Cumulative_SOC_deviation: 216.2913 Fuel Consumption: 84.7171\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -1981.2284431165458 Explore P: 0.0237 SOC: 0.9898 Cumulative_SOC_deviation: 190.5378 Fuel Consumption: 75.8505\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -1822.9799595783406 Explore P: 0.0234 SOC: 0.9921 Cumulative_SOC_deviation: 174.5559 Fuel Consumption: 77.4210\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 158 Total reward: -2699.243873877251 Explore P: 0.0230 SOC: 1.0000 Cumulative_SOC_deviation: 261.0725 Fuel Consumption: 88.5189\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -939.9440827566671 Explore P: 0.0227 SOC: 0.4968 Cumulative_SOC_deviation: 90.3223 Fuel Consumption: 36.7209\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -1821.10860374816 Explore P: 0.0223 SOC: 0.5812 Cumulative_SOC_deviation: 177.6992 Fuel Consumption: 44.1163\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -1407.115657789145 Explore P: 0.0220 SOC: 0.8169 Cumulative_SOC_deviation: 134.4437 Fuel Consumption: 62.6782\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -897.1879826651999 Explore P: 0.0217 SOC: 0.6038 Cumulative_SOC_deviation: 85.2083 Fuel Consumption: 45.1046\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -1310.332209034373 Explore P: 0.0213 SOC: 0.5754 Cumulative_SOC_deviation: 126.8260 Fuel Consumption: 42.0725\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 164 Total reward: -2240.555650354747 Explore P: 0.0211 SOC: 1.0000 Cumulative_SOC_deviation: 215.6472 Fuel Consumption: 84.0836\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -2077.565541705562 Explore P: 0.0208 SOC: 0.9576 Cumulative_SOC_deviation: 200.3481 Fuel Consumption: 74.0849\n",
      "Available condition is not avail... SOC: 0.9974484529573558\n",
      "Episode: 166 Total reward: -2878.484084044149 Explore P: 0.0205 SOC: 0.9974 Cumulative_SOC_deviation: 279.6519 Fuel Consumption: 81.9652\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -1180.97586752657 Explore P: 0.0202 SOC: 0.8270 Cumulative_SOC_deviation: 111.8789 Fuel Consumption: 62.1867\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -704.9472547408557 Explore P: 0.0199 SOC: 0.6133 Cumulative_SOC_deviation: 66.0543 Fuel Consumption: 44.4047\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -1307.8378403347087 Explore P: 0.0196 SOC: 0.6465 Cumulative_SOC_deviation: 125.9168 Fuel Consumption: 48.6703\n",
      "Available condition is not avail... SOC: 0.9894219997117539\n",
      "Episode: 170 Total reward: -2202.6152753140645 Explore P: 0.0194 SOC: 0.9894 Cumulative_SOC_deviation: 212.4429 Fuel Consumption: 78.1867\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -1741.3914747296922 Explore P: 0.0191 SOC: 0.5151 Cumulative_SOC_deviation: 170.3605 Fuel Consumption: 37.7866\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -1394.9400262924557 Explore P: 0.0189 SOC: 0.6805 Cumulative_SOC_deviation: 134.4313 Fuel Consumption: 50.6275\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -791.0902206897432 Explore P: 0.0187 SOC: 0.7510 Cumulative_SOC_deviation: 73.5822 Fuel Consumption: 55.2686\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -922.4226466789308 Explore P: 0.0184 SOC: 0.7543 Cumulative_SOC_deviation: 86.6637 Fuel Consumption: 55.7855\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -1513.2020450892292 Explore P: 0.0182 SOC: 0.8745 Cumulative_SOC_deviation: 144.6852 Fuel Consumption: 66.3499\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -2039.5126567065447 Explore P: 0.0180 SOC: 0.9131 Cumulative_SOC_deviation: 196.9869 Fuel Consumption: 69.6440\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -2058.4693075466257 Explore P: 0.0178 SOC: 0.9882 Cumulative_SOC_deviation: 198.2930 Fuel Consumption: 75.5388\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -1917.832211609388 Explore P: 0.0175 SOC: 0.9483 Cumulative_SOC_deviation: 184.4323 Fuel Consumption: 73.5092\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -959.8294074637337 Explore P: 0.0173 SOC: 0.6703 Cumulative_SOC_deviation: 91.0686 Fuel Consumption: 49.1436\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -1183.0600598231813 Explore P: 0.0171 SOC: 0.8496 Cumulative_SOC_deviation: 111.9432 Fuel Consumption: 63.6279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -1812.8559229318478 Explore P: 0.0169 SOC: 0.4974 Cumulative_SOC_deviation: 177.4769 Fuel Consumption: 38.0872\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -2018.7134942125501 Explore P: 0.0168 SOC: 0.5683 Cumulative_SOC_deviation: 197.6668 Fuel Consumption: 42.0456\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -349.51261584027696 Explore P: 0.0166 SOC: 0.6154 Cumulative_SOC_deviation: 30.4807 Fuel Consumption: 44.7057\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -1842.6876921684213 Explore P: 0.0164 SOC: 0.5327 Cumulative_SOC_deviation: 180.3749 Fuel Consumption: 38.9391\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -1183.5113511960665 Explore P: 0.0162 SOC: 0.8572 Cumulative_SOC_deviation: 111.9395 Fuel Consumption: 64.1160\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -786.135788709153 Explore P: 0.0161 SOC: 0.7797 Cumulative_SOC_deviation: 72.8597 Fuel Consumption: 57.5385\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -847.6395397259722 Explore P: 0.0159 SOC: 0.5720 Cumulative_SOC_deviation: 80.6102 Fuel Consumption: 41.5373\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -1368.5775100184749 Explore P: 0.0157 SOC: 0.6960 Cumulative_SOC_deviation: 131.6414 Fuel Consumption: 52.1635\n",
      "Available condition is not avail... SOC: 0.9846625096735311\n",
      "Episode: 189 Total reward: -1737.9634680647312 Explore P: 0.0156 SOC: 0.9847 Cumulative_SOC_deviation: 166.2496 Fuel Consumption: 75.4673\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -2674.0474985558003 Explore P: 0.0154 SOC: 0.9992 Cumulative_SOC_deviation: 259.6347 Fuel Consumption: 77.7008\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 191 Total reward: -2512.159835240717 Explore P: 0.0153 SOC: 1.0000 Cumulative_SOC_deviation: 243.1796 Fuel Consumption: 80.3638\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -2673.5193972830516 Explore P: 0.0152 SOC: 0.4810 Cumulative_SOC_deviation: 263.7279 Fuel Consumption: 36.2404\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -703.1001938795647 Explore P: 0.0150 SOC: 0.6060 Cumulative_SOC_deviation: 65.9246 Fuel Consumption: 43.8543\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -1654.5489137145878 Explore P: 0.0149 SOC: 0.4973 Cumulative_SOC_deviation: 161.8381 Fuel Consumption: 36.1680\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -1432.9061502517927 Explore P: 0.0147 SOC: 0.8482 Cumulative_SOC_deviation: 136.8191 Fuel Consumption: 64.7150\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -557.7300257448115 Explore P: 0.0146 SOC: 0.6220 Cumulative_SOC_deviation: 51.2397 Fuel Consumption: 45.3326\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -2021.6171515566734 Explore P: 0.0145 SOC: 0.5178 Cumulative_SOC_deviation: 198.3936 Fuel Consumption: 37.6812\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -1008.5518485281274 Explore P: 0.0144 SOC: 0.6516 Cumulative_SOC_deviation: 96.1057 Fuel Consumption: 47.4945\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -1907.1249526381298 Explore P: 0.0142 SOC: 1.0000 Cumulative_SOC_deviation: 182.9503 Fuel Consumption: 77.6222\n",
      "Available condition is not avail... SOC: 1\n",
      "Episode: 200 Total reward: -2655.4562030028915 Explore P: 0.0141 SOC: 1.0000 Cumulative_SOC_deviation: 256.1726 Fuel Consumption: 93.7302\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {i}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results_unNorm.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4b3ac91aa511>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2761\u001b[0m     return gca().plot(\n\u001b[0;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m-> 2763\u001b[1;33m         is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1647\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1648\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1649\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1650\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request_autoscale_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1651\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36madd_line\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   1848\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1850\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1851\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_line%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_update_line_limits\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   1870\u001b[0m         \u001b[0mFigures\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdating\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1871\u001b[0m         \"\"\"\n\u001b[1;32m-> 1872\u001b[1;33m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1873\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1874\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mget_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \"\"\"\n\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1027\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1028\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mrecache\u001b[1;34m(self, always)\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0malways\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m             \u001b[0myconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_yorig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_to_unmasked_float_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myconv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\matplotlib\\cbook\\__init__.py\u001b[0m in \u001b[0;36m_to_unmasked_float_array\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1315\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1317\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'dict'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "for size, history in results_dict.items(): \n",
    "    plt.plot(history, label=size, linewidth=3.0) \n",
    "\n",
    "\n",
    "plt.grid() \n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel(\"episode number\", fontsize=20) \n",
    "plt.ylabel(\"total rewards\", fontsize=20) \n",
    "plt.xlim([0, 120])\n",
    "\n",
    "\n",
    "plt.savefig(\"replay_memory_size_effect_300.png\")\n",
    "with open(\"replay_memory_size_effect_300.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-(3 > 2) * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN_SOC_dev import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 1000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 1\n",
      "WARNING:tensorflow:Layer sequential_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 1 Total reward: -999.731182548498 Explore P: 0.9732 SOC: 0.8147 Cumulative_SOC_deviation: 93.7740 Fuel Consumption: 61.9915\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 2 Total reward: -998.8643038184443 Explore P: 0.9471 SOC: 0.8088 Cumulative_SOC_deviation: 93.7290 Fuel Consumption: 61.5740\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -871.9897378802115 Explore P: 0.9217 SOC: 0.7755 Cumulative_SOC_deviation: 81.3259 Fuel Consumption: 58.7309\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -832.9361972830919 Explore P: 0.8970 SOC: 0.7764 Cumulative_SOC_deviation: 77.4033 Fuel Consumption: 58.9027\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -818.9854196116939 Explore P: 0.8730 SOC: 0.7549 Cumulative_SOC_deviation: 76.1835 Fuel Consumption: 57.1503\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -813.8665441186937 Explore P: 0.8496 SOC: 0.7603 Cumulative_SOC_deviation: 75.6252 Fuel Consumption: 57.6150\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -742.5347930435721 Explore P: 0.8269 SOC: 0.7131 Cumulative_SOC_deviation: 68.8694 Fuel Consumption: 53.8405\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -702.4890923142772 Explore P: 0.8048 SOC: 0.7003 Cumulative_SOC_deviation: 64.9652 Fuel Consumption: 52.8370\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -915.6764290007083 Explore P: 0.7832 SOC: 0.7937 Cumulative_SOC_deviation: 85.5169 Fuel Consumption: 60.5072\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -673.4484118712141 Explore P: 0.7623 SOC: 0.7296 Cumulative_SOC_deviation: 61.8390 Fuel Consumption: 55.0588\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -677.6867305552462 Explore P: 0.7419 SOC: 0.7376 Cumulative_SOC_deviation: 62.1716 Fuel Consumption: 55.9710\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -675.9959787837846 Explore P: 0.7221 SOC: 0.7108 Cumulative_SOC_deviation: 62.2250 Fuel Consumption: 53.7457\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -733.6621047244411 Explore P: 0.7028 SOC: 0.7174 Cumulative_SOC_deviation: 67.9468 Fuel Consumption: 54.1937\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -639.1097097843632 Explore P: 0.6840 SOC: 0.6842 Cumulative_SOC_deviation: 58.7751 Fuel Consumption: 51.3588\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -760.8595323184771 Explore P: 0.6658 SOC: 0.7784 Cumulative_SOC_deviation: 70.1780 Fuel Consumption: 59.0791\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -700.4490885323852 Explore P: 0.6480 SOC: 0.7661 Cumulative_SOC_deviation: 64.2211 Fuel Consumption: 58.2383\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -854.2385786689853 Explore P: 0.6307 SOC: 0.7862 Cumulative_SOC_deviation: 79.4249 Fuel Consumption: 59.9898\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -640.758589034032 Explore P: 0.6139 SOC: 0.6769 Cumulative_SOC_deviation: 58.9916 Fuel Consumption: 50.8429\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -679.8784933497484 Explore P: 0.5976 SOC: 0.6824 Cumulative_SOC_deviation: 62.8853 Fuel Consumption: 51.0253\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -692.2530565196535 Explore P: 0.5816 SOC: 0.7054 Cumulative_SOC_deviation: 63.9120 Fuel Consumption: 53.1335\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -574.1903474964437 Explore P: 0.5662 SOC: 0.6743 Cumulative_SOC_deviation: 52.3405 Fuel Consumption: 50.7856\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -682.2615483285513 Explore P: 0.5511 SOC: 0.6937 Cumulative_SOC_deviation: 62.9572 Fuel Consumption: 52.6891\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -662.7453609008992 Explore P: 0.5364 SOC: 0.6380 Cumulative_SOC_deviation: 61.4849 Fuel Consumption: 47.8964\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -698.2508713678131 Explore P: 0.5222 SOC: 0.6418 Cumulative_SOC_deviation: 64.9947 Fuel Consumption: 48.3039\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -689.2894166444795 Explore P: 0.5083 SOC: 0.6780 Cumulative_SOC_deviation: 63.8109 Fuel Consumption: 51.1804\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -629.750175551023 Explore P: 0.4948 SOC: 0.6537 Cumulative_SOC_deviation: 58.0538 Fuel Consumption: 49.2125\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -609.2799521200825 Explore P: 0.4817 SOC: 0.6486 Cumulative_SOC_deviation: 56.0491 Fuel Consumption: 48.7889\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -603.4140037273808 Explore P: 0.4689 SOC: 0.6557 Cumulative_SOC_deviation: 55.3929 Fuel Consumption: 49.4852\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -700.6175264707377 Explore P: 0.4565 SOC: 0.6621 Cumulative_SOC_deviation: 65.0764 Fuel Consumption: 49.8531\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -573.4898013460081 Explore P: 0.4444 SOC: 0.6549 Cumulative_SOC_deviation: 52.4246 Fuel Consumption: 49.2440\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -596.0122928049528 Explore P: 0.4326 SOC: 0.6603 Cumulative_SOC_deviation: 54.6258 Fuel Consumption: 49.7548\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -486.7538559215733 Explore P: 0.4212 SOC: 0.6548 Cumulative_SOC_deviation: 43.7820 Fuel Consumption: 48.9343\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -527.2234600308436 Explore P: 0.4100 SOC: 0.6580 Cumulative_SOC_deviation: 47.8115 Fuel Consumption: 49.1086\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -468.5964285754626 Explore P: 0.3992 SOC: 0.6551 Cumulative_SOC_deviation: 41.9902 Fuel Consumption: 48.6946\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -616.2313805047875 Explore P: 0.3887 SOC: 0.6551 Cumulative_SOC_deviation: 56.7400 Fuel Consumption: 48.8309\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -537.6991622198311 Explore P: 0.3784 SOC: 0.6757 Cumulative_SOC_deviation: 48.7408 Fuel Consumption: 50.2915\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -522.7888965562636 Explore P: 0.3684 SOC: 0.6496 Cumulative_SOC_deviation: 47.4570 Fuel Consumption: 48.2187\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -622.3196792943247 Explore P: 0.3587 SOC: 0.6611 Cumulative_SOC_deviation: 57.3018 Fuel Consumption: 49.3017\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -552.8906397003609 Explore P: 0.3493 SOC: 0.7017 Cumulative_SOC_deviation: 50.0599 Fuel Consumption: 52.2917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -576.0995623254883 Explore P: 0.3401 SOC: 0.6692 Cumulative_SOC_deviation: 52.5852 Fuel Consumption: 50.2474\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -479.8192949075201 Explore P: 0.3311 SOC: 0.6476 Cumulative_SOC_deviation: 43.1477 Fuel Consumption: 48.3424\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -691.7936680553495 Explore P: 0.3224 SOC: 0.6606 Cumulative_SOC_deviation: 64.2355 Fuel Consumption: 49.4386\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -503.04083663525677 Explore P: 0.3140 SOC: 0.6787 Cumulative_SOC_deviation: 45.2501 Fuel Consumption: 50.5403\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -535.6098979083347 Explore P: 0.3057 SOC: 0.6178 Cumulative_SOC_deviation: 48.9343 Fuel Consumption: 46.2666\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -449.5194599499482 Explore P: 0.2977 SOC: 0.6341 Cumulative_SOC_deviation: 40.2257 Fuel Consumption: 47.2629\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -519.9284698318535 Explore P: 0.2899 SOC: 0.6335 Cumulative_SOC_deviation: 47.2843 Fuel Consumption: 47.0851\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -549.0653382047689 Explore P: 0.2824 SOC: 0.6249 Cumulative_SOC_deviation: 50.2427 Fuel Consumption: 46.6386\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -506.15209635757014 Explore P: 0.2750 SOC: 0.6270 Cumulative_SOC_deviation: 45.9049 Fuel Consumption: 47.1028\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -419.41152043816476 Explore P: 0.2678 SOC: 0.6129 Cumulative_SOC_deviation: 37.3874 Fuel Consumption: 45.5374\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -436.4209246465197 Explore P: 0.2608 SOC: 0.6310 Cumulative_SOC_deviation: 38.9277 Fuel Consumption: 47.1439\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -407.0419939718145 Explore P: 0.2540 SOC: 0.6319 Cumulative_SOC_deviation: 36.0099 Fuel Consumption: 46.9432\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -425.96115104256626 Explore P: 0.2474 SOC: 0.6138 Cumulative_SOC_deviation: 38.0122 Fuel Consumption: 45.8389\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -545.4188696184273 Explore P: 0.2410 SOC: 0.6210 Cumulative_SOC_deviation: 49.9127 Fuel Consumption: 46.2914\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -405.31156349199523 Explore P: 0.2347 SOC: 0.6124 Cumulative_SOC_deviation: 35.9666 Fuel Consumption: 45.6458\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -451.8242961392817 Explore P: 0.2286 SOC: 0.6141 Cumulative_SOC_deviation: 40.6166 Fuel Consumption: 45.6580\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -435.5571659542503 Explore P: 0.2227 SOC: 0.6137 Cumulative_SOC_deviation: 39.0017 Fuel Consumption: 45.5399\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -474.05202875817184 Explore P: 0.2170 SOC: 0.6126 Cumulative_SOC_deviation: 42.8575 Fuel Consumption: 45.4765\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -391.7542223815686 Explore P: 0.2114 SOC: 0.6113 Cumulative_SOC_deviation: 34.6382 Fuel Consumption: 45.3721\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -389.32484162654094 Explore P: 0.2059 SOC: 0.6078 Cumulative_SOC_deviation: 34.4121 Fuel Consumption: 45.2039\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -363.2515563666003 Explore P: 0.2006 SOC: 0.6161 Cumulative_SOC_deviation: 31.7148 Fuel Consumption: 46.1039\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -313.30973331942323 Explore P: 0.1954 SOC: 0.6127 Cumulative_SOC_deviation: 26.7695 Fuel Consumption: 45.6149\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -360.2015933169441 Explore P: 0.1904 SOC: 0.6137 Cumulative_SOC_deviation: 31.4566 Fuel Consumption: 45.6357\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -379.7496744575632 Explore P: 0.1855 SOC: 0.6124 Cumulative_SOC_deviation: 33.4148 Fuel Consumption: 45.6017\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -551.1010201648031 Explore P: 0.1808 SOC: 0.6069 Cumulative_SOC_deviation: 50.6027 Fuel Consumption: 45.0742\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -460.2786564166501 Explore P: 0.1761 SOC: 0.6144 Cumulative_SOC_deviation: 41.4725 Fuel Consumption: 45.5536\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -313.6474140047453 Explore P: 0.1716 SOC: 0.6068 Cumulative_SOC_deviation: 26.8439 Fuel Consumption: 45.2079\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -436.657097503459 Explore P: 0.1673 SOC: 0.6089 Cumulative_SOC_deviation: 39.1103 Fuel Consumption: 45.5546\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -524.0533074674893 Explore P: 0.1630 SOC: 0.6092 Cumulative_SOC_deviation: 47.7987 Fuel Consumption: 46.0664\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -417.1346806739888 Explore P: 0.1589 SOC: 0.6047 Cumulative_SOC_deviation: 37.1833 Fuel Consumption: 45.3017\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -530.9474827022531 Explore P: 0.1548 SOC: 0.6117 Cumulative_SOC_deviation: 48.5177 Fuel Consumption: 45.7710\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -434.7759271549466 Explore P: 0.1509 SOC: 0.6156 Cumulative_SOC_deviation: 38.8305 Fuel Consumption: 46.4708\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -483.6775000241477 Explore P: 0.1471 SOC: 0.6136 Cumulative_SOC_deviation: 43.7984 Fuel Consumption: 45.6934\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -481.2742325213778 Explore P: 0.1434 SOC: 0.6082 Cumulative_SOC_deviation: 43.6219 Fuel Consumption: 45.0554\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -283.9114262205697 Explore P: 0.1398 SOC: 0.6118 Cumulative_SOC_deviation: 23.8453 Fuel Consumption: 45.4588\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -383.28657344576925 Explore P: 0.1362 SOC: 0.6096 Cumulative_SOC_deviation: 33.8005 Fuel Consumption: 45.2814\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -398.80246892300295 Explore P: 0.1328 SOC: 0.6088 Cumulative_SOC_deviation: 35.3208 Fuel Consumption: 45.5946\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -362.8281420205573 Explore P: 0.1295 SOC: 0.6073 Cumulative_SOC_deviation: 31.7228 Fuel Consumption: 45.5997\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -349.3364064101095 Explore P: 0.1263 SOC: 0.6157 Cumulative_SOC_deviation: 30.3316 Fuel Consumption: 46.0208\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -285.2656311821071 Explore P: 0.1231 SOC: 0.6073 Cumulative_SOC_deviation: 24.0063 Fuel Consumption: 45.2024\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -450.77624424486646 Explore P: 0.1200 SOC: 0.6092 Cumulative_SOC_deviation: 40.5294 Fuel Consumption: 45.4821\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -310.28141608172695 Explore P: 0.1171 SOC: 0.6070 Cumulative_SOC_deviation: 26.5028 Fuel Consumption: 45.2530\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -350.63979262391575 Explore P: 0.1142 SOC: 0.6167 Cumulative_SOC_deviation: 30.4684 Fuel Consumption: 45.9560\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -331.323716943815 Explore P: 0.1113 SOC: 0.6114 Cumulative_SOC_deviation: 28.6002 Fuel Consumption: 45.3215\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -372.553021793996 Explore P: 0.1086 SOC: 0.6158 Cumulative_SOC_deviation: 32.6774 Fuel Consumption: 45.7786\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -457.60799204121594 Explore P: 0.1059 SOC: 0.6112 Cumulative_SOC_deviation: 41.1814 Fuel Consumption: 45.7938\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -525.2212457597341 Explore P: 0.1033 SOC: 0.6106 Cumulative_SOC_deviation: 47.9807 Fuel Consumption: 45.4142\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -431.6124553787893 Explore P: 0.1008 SOC: 0.6111 Cumulative_SOC_deviation: 38.6261 Fuel Consumption: 45.3519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -424.6107883287098 Explore P: 0.0983 SOC: 0.6085 Cumulative_SOC_deviation: 37.9455 Fuel Consumption: 45.1557\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -398.1237422991574 Explore P: 0.0960 SOC: 0.6095 Cumulative_SOC_deviation: 35.2930 Fuel Consumption: 45.1938\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -346.14354655208405 Explore P: 0.0936 SOC: 0.6081 Cumulative_SOC_deviation: 30.0739 Fuel Consumption: 45.4041\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -367.2442966526339 Explore P: 0.0914 SOC: 0.6025 Cumulative_SOC_deviation: 32.2187 Fuel Consumption: 45.0574\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -388.61394195608153 Explore P: 0.0892 SOC: 0.6064 Cumulative_SOC_deviation: 34.2897 Fuel Consumption: 45.7173\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -370.4257326393985 Explore P: 0.0870 SOC: 0.6104 Cumulative_SOC_deviation: 32.4936 Fuel Consumption: 45.4902\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -522.4279562443857 Explore P: 0.0849 SOC: 0.6083 Cumulative_SOC_deviation: 47.6995 Fuel Consumption: 45.4330\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -258.84944226954383 Explore P: 0.0829 SOC: 0.6093 Cumulative_SOC_deviation: 21.3749 Fuel Consumption: 45.1000\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -362.7959101342492 Explore P: 0.0809 SOC: 0.6060 Cumulative_SOC_deviation: 31.7422 Fuel Consumption: 45.3737\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -563.3631166867799 Explore P: 0.0790 SOC: 0.6125 Cumulative_SOC_deviation: 51.7113 Fuel Consumption: 46.2504\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -477.6417918955649 Explore P: 0.0771 SOC: 0.6120 Cumulative_SOC_deviation: 43.2283 Fuel Consumption: 45.3585\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -350.17606992084893 Explore P: 0.0753 SOC: 0.6112 Cumulative_SOC_deviation: 30.4872 Fuel Consumption: 45.3042\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -331.5363200425876 Explore P: 0.0735 SOC: 0.6065 Cumulative_SOC_deviation: 28.6627 Fuel Consumption: 44.9095\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -315.43616371194344 Explore P: 0.0718 SOC: 0.6071 Cumulative_SOC_deviation: 27.0589 Fuel Consumption: 44.8476\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -367.7462776394505 Explore P: 0.0701 SOC: 0.6255 Cumulative_SOC_deviation: 32.1306 Fuel Consumption: 46.4404\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -333.1214739667425 Explore P: 0.0685 SOC: 0.6079 Cumulative_SOC_deviation: 28.8020 Fuel Consumption: 45.1015\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -305.31594163881306 Explore P: 0.0669 SOC: 0.6051 Cumulative_SOC_deviation: 26.0734 Fuel Consumption: 44.5821\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -379.62742457514855 Explore P: 0.0654 SOC: 0.6119 Cumulative_SOC_deviation: 33.4281 Fuel Consumption: 45.3463\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -309.4282229306415 Explore P: 0.0639 SOC: 0.6126 Cumulative_SOC_deviation: 26.4109 Fuel Consumption: 45.3189\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -486.376079391977 Explore P: 0.0624 SOC: 0.6045 Cumulative_SOC_deviation: 44.1728 Fuel Consumption: 44.6480\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -369.0979083635641 Explore P: 0.0610 SOC: 0.6086 Cumulative_SOC_deviation: 32.3666 Fuel Consumption: 45.4314\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -494.65311182936875 Explore P: 0.0596 SOC: 0.6077 Cumulative_SOC_deviation: 44.9668 Fuel Consumption: 44.9855\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -340.7578134909175 Explore P: 0.0583 SOC: 0.6053 Cumulative_SOC_deviation: 29.5882 Fuel Consumption: 44.8760\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -312.2979589028964 Explore P: 0.0570 SOC: 0.6041 Cumulative_SOC_deviation: 26.7591 Fuel Consumption: 44.7072\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -310.4318133363046 Explore P: 0.0557 SOC: 0.6038 Cumulative_SOC_deviation: 26.5634 Fuel Consumption: 44.7980\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -300.65644107699285 Explore P: 0.0545 SOC: 0.6063 Cumulative_SOC_deviation: 25.5733 Fuel Consumption: 44.9236\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -284.2355600289466 Explore P: 0.0533 SOC: 0.6168 Cumulative_SOC_deviation: 23.8528 Fuel Consumption: 45.7071\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -353.07425162075685 Explore P: 0.0521 SOC: 0.6132 Cumulative_SOC_deviation: 30.7916 Fuel Consumption: 45.1578\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -505.44668901111464 Explore P: 0.0510 SOC: 0.6054 Cumulative_SOC_deviation: 46.0231 Fuel Consumption: 45.2155\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -292.0554752079313 Explore P: 0.0498 SOC: 0.6115 Cumulative_SOC_deviation: 24.6903 Fuel Consumption: 45.1522\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -284.34005323034984 Explore P: 0.0488 SOC: 0.6098 Cumulative_SOC_deviation: 23.9241 Fuel Consumption: 45.0995\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -406.8549364494371 Explore P: 0.0477 SOC: 0.6140 Cumulative_SOC_deviation: 36.1094 Fuel Consumption: 45.7608\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -336.4614182777238 Explore P: 0.0467 SOC: 0.5990 Cumulative_SOC_deviation: 29.2138 Fuel Consumption: 44.3236\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -351.18633161217485 Explore P: 0.0457 SOC: 0.6065 Cumulative_SOC_deviation: 30.6098 Fuel Consumption: 45.0878\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -461.17751811968014 Explore P: 0.0447 SOC: 0.6038 Cumulative_SOC_deviation: 41.6369 Fuel Consumption: 44.8086\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -472.55251742022233 Explore P: 0.0438 SOC: 0.6047 Cumulative_SOC_deviation: 42.7429 Fuel Consumption: 45.1238\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -433.5952372352333 Explore P: 0.0429 SOC: 0.6040 Cumulative_SOC_deviation: 38.8828 Fuel Consumption: 44.7676\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -269.8828432775946 Explore P: 0.0420 SOC: 0.6019 Cumulative_SOC_deviation: 22.5393 Fuel Consumption: 44.4898\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -273.9903665506015 Explore P: 0.0411 SOC: 0.6045 Cumulative_SOC_deviation: 22.9285 Fuel Consumption: 44.7057\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -312.12592586779135 Explore P: 0.0403 SOC: 0.6040 Cumulative_SOC_deviation: 26.7321 Fuel Consumption: 44.8045\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -371.93144270290225 Explore P: 0.0395 SOC: 0.6024 Cumulative_SOC_deviation: 32.7353 Fuel Consumption: 44.5785\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -371.9756806076389 Explore P: 0.0387 SOC: 0.6075 Cumulative_SOC_deviation: 32.6777 Fuel Consumption: 45.1988\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -372.5583813447353 Explore P: 0.0379 SOC: 0.6088 Cumulative_SOC_deviation: 32.7268 Fuel Consumption: 45.2906\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -362.08193545634253 Explore P: 0.0371 SOC: 0.6118 Cumulative_SOC_deviation: 31.6873 Fuel Consumption: 45.2090\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -319.49137757737236 Explore P: 0.0364 SOC: 0.6032 Cumulative_SOC_deviation: 27.5128 Fuel Consumption: 44.3631\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -284.18494618506486 Explore P: 0.0357 SOC: 0.5998 Cumulative_SOC_deviation: 23.9998 Fuel Consumption: 44.1868\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -285.39891625944364 Explore P: 0.0350 SOC: 0.6020 Cumulative_SOC_deviation: 24.1199 Fuel Consumption: 44.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -252.60536826136138 Explore P: 0.0343 SOC: 0.6050 Cumulative_SOC_deviation: 20.8157 Fuel Consumption: 44.4488\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -249.7510696147303 Explore P: 0.0336 SOC: 0.6072 Cumulative_SOC_deviation: 20.5114 Fuel Consumption: 44.6373\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -304.40538227553503 Explore P: 0.0330 SOC: 0.6051 Cumulative_SOC_deviation: 25.9804 Fuel Consumption: 44.6018\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -274.9203252031932 Explore P: 0.0324 SOC: 0.6059 Cumulative_SOC_deviation: 23.0071 Fuel Consumption: 44.8491\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -276.281086832618 Explore P: 0.0318 SOC: 0.6062 Cumulative_SOC_deviation: 23.1388 Fuel Consumption: 44.8927\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -243.9225900497165 Explore P: 0.0312 SOC: 0.6037 Cumulative_SOC_deviation: 19.9128 Fuel Consumption: 44.7944\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -297.67275795519845 Explore P: 0.0306 SOC: 0.6060 Cumulative_SOC_deviation: 25.3041 Fuel Consumption: 44.6317\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -311.62715255674493 Explore P: 0.0301 SOC: 0.6086 Cumulative_SOC_deviation: 26.6499 Fuel Consumption: 45.1284\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10, 50, 100, 1000]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {i}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "for size, history in results_dict.items(): \n",
    "    plt.plot(history, label=size, linewidth=3.0) \n",
    "\n",
    "\n",
    "plt.grid() \n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel(\"episode number\", fontsize=20) \n",
    "plt.ylabel(\"total rewards\", fontsize=20) \n",
    "plt.xlim([0, 120])\n",
    "\n",
    "\n",
    "plt.savefig(\"replay_memory_size_effect_300.png\")\n",
    "with open(\"replay_memory_size_effect_300.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-(3 > 2) * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

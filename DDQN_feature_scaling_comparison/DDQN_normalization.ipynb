{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN_SOC_dev_norm import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 1000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 3\n",
      "WARNING:tensorflow:Layer dense_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 1 Total reward: -934.9321850591859 Explore P: 0.9732 SOC: 0.7952 Cumulative_SOC_deviation: 87.4411 Fuel Consumption: 60.5209\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 2 Total reward: -907.3873785905996 Explore P: 0.9471 SOC: 0.7853 Cumulative_SOC_deviation: 84.7764 Fuel Consumption: 59.6239\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -944.0243204654662 Explore P: 0.9217 SOC: 0.7879 Cumulative_SOC_deviation: 88.4243 Fuel Consumption: 59.7815\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -995.4877684475989 Explore P: 0.8970 SOC: 0.7955 Cumulative_SOC_deviation: 93.4931 Fuel Consumption: 60.5568\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -792.3319830175496 Explore P: 0.8730 SOC: 0.7593 Cumulative_SOC_deviation: 73.4758 Fuel Consumption: 57.5744\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -692.4637412032396 Explore P: 0.8496 SOC: 0.7081 Cumulative_SOC_deviation: 63.9061 Fuel Consumption: 53.4026\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -757.9927217996628 Explore P: 0.8269 SOC: 0.7060 Cumulative_SOC_deviation: 70.4623 Fuel Consumption: 53.3702\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -714.0042976243245 Explore P: 0.8048 SOC: 0.6998 Cumulative_SOC_deviation: 66.1146 Fuel Consumption: 52.8578\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -696.0543148253715 Explore P: 0.7832 SOC: 0.6972 Cumulative_SOC_deviation: 64.3524 Fuel Consumption: 52.5299\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -675.2419060462258 Explore P: 0.7623 SOC: 0.6903 Cumulative_SOC_deviation: 62.3095 Fuel Consumption: 52.1468\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -757.7320149938449 Explore P: 0.7419 SOC: 0.6787 Cumulative_SOC_deviation: 70.6676 Fuel Consumption: 51.0557\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -643.601541187893 Explore P: 0.7221 SOC: 0.6982 Cumulative_SOC_deviation: 59.0954 Fuel Consumption: 52.6480\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -838.7844490139178 Explore P: 0.7028 SOC: 0.7010 Cumulative_SOC_deviation: 78.5975 Fuel Consumption: 52.8092\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -667.1388596991151 Explore P: 0.6840 SOC: 0.6791 Cumulative_SOC_deviation: 61.5938 Fuel Consumption: 51.2006\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -649.3529685575594 Explore P: 0.6658 SOC: 0.7147 Cumulative_SOC_deviation: 59.5412 Fuel Consumption: 53.9408\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -748.0354271552599 Explore P: 0.6480 SOC: 0.7630 Cumulative_SOC_deviation: 69.0193 Fuel Consumption: 57.8425\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -723.6727395596579 Explore P: 0.6307 SOC: 0.7535 Cumulative_SOC_deviation: 66.6529 Fuel Consumption: 57.1442\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -772.1386663204302 Explore P: 0.6139 SOC: 0.7803 Cumulative_SOC_deviation: 71.2496 Fuel Consumption: 59.6431\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -708.7879169928112 Explore P: 0.5976 SOC: 0.7680 Cumulative_SOC_deviation: 65.0459 Fuel Consumption: 58.3285\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -620.0093842081975 Explore P: 0.5816 SOC: 0.6600 Cumulative_SOC_deviation: 57.0526 Fuel Consumption: 49.4832\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -614.194518382771 Explore P: 0.5662 SOC: 0.6745 Cumulative_SOC_deviation: 56.3543 Fuel Consumption: 50.6513\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -633.4681816944554 Explore P: 0.5511 SOC: 0.6871 Cumulative_SOC_deviation: 58.1939 Fuel Consumption: 51.5295\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -675.633748584613 Explore P: 0.5364 SOC: 0.7159 Cumulative_SOC_deviation: 62.1890 Fuel Consumption: 53.7437\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -579.5956816206045 Explore P: 0.5222 SOC: 0.7163 Cumulative_SOC_deviation: 52.5974 Fuel Consumption: 53.6215\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -647.9917233244358 Explore P: 0.5083 SOC: 0.7057 Cumulative_SOC_deviation: 59.4825 Fuel Consumption: 53.1670\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -559.7403354480565 Explore P: 0.4948 SOC: 0.6837 Cumulative_SOC_deviation: 50.8098 Fuel Consumption: 51.6420\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -583.169937011641 Explore P: 0.4817 SOC: 0.6773 Cumulative_SOC_deviation: 53.2536 Fuel Consumption: 50.6341\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -601.7059433152764 Explore P: 0.4689 SOC: 0.7251 Cumulative_SOC_deviation: 54.7094 Fuel Consumption: 54.6123\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -449.04195827187095 Explore P: 0.4565 SOC: 0.6906 Cumulative_SOC_deviation: 39.6878 Fuel Consumption: 52.1640\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -500.13861328067776 Explore P: 0.4444 SOC: 0.6673 Cumulative_SOC_deviation: 44.9750 Fuel Consumption: 50.3888\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -636.5981741351317 Explore P: 0.4326 SOC: 0.7222 Cumulative_SOC_deviation: 58.1926 Fuel Consumption: 54.6726\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -708.7041553699016 Explore P: 0.4212 SOC: 0.7093 Cumulative_SOC_deviation: 65.5154 Fuel Consumption: 53.5501\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -877.596723812585 Explore P: 0.4100 SOC: 0.7851 Cumulative_SOC_deviation: 81.7313 Fuel Consumption: 60.2837\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -465.9890393685061 Explore P: 0.3992 SOC: 0.6367 Cumulative_SOC_deviation: 41.8331 Fuel Consumption: 47.6582\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -546.0195651587936 Explore P: 0.3887 SOC: 0.6416 Cumulative_SOC_deviation: 49.7958 Fuel Consumption: 48.0616\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -574.8289482127115 Explore P: 0.3784 SOC: 0.6783 Cumulative_SOC_deviation: 52.4090 Fuel Consumption: 50.7390\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -478.58497651808153 Explore P: 0.3684 SOC: 0.6493 Cumulative_SOC_deviation: 43.0227 Fuel Consumption: 48.3576\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -751.6121021476893 Explore P: 0.3587 SOC: 0.7383 Cumulative_SOC_deviation: 69.6171 Fuel Consumption: 55.4414\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -619.2620041949351 Explore P: 0.3493 SOC: 0.7039 Cumulative_SOC_deviation: 56.6819 Fuel Consumption: 52.4433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -526.666265231921 Explore P: 0.3401 SOC: 0.6669 Cumulative_SOC_deviation: 47.6882 Fuel Consumption: 49.7847\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -474.67383944789293 Explore P: 0.3311 SOC: 0.6322 Cumulative_SOC_deviation: 42.7387 Fuel Consumption: 47.2873\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -506.1486174728872 Explore P: 0.3224 SOC: 0.6599 Cumulative_SOC_deviation: 45.7018 Fuel Consumption: 49.1309\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -484.0218945682734 Explore P: 0.3140 SOC: 0.6782 Cumulative_SOC_deviation: 43.3489 Fuel Consumption: 50.5327\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -335.39782155803437 Explore P: 0.3057 SOC: 0.6292 Cumulative_SOC_deviation: 28.8544 Fuel Consumption: 46.8535\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -534.4235792912514 Explore P: 0.2977 SOC: 0.6541 Cumulative_SOC_deviation: 48.5449 Fuel Consumption: 48.9743\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -881.9843410388095 Explore P: 0.2899 SOC: 0.7896 Cumulative_SOC_deviation: 82.2306 Fuel Consumption: 59.6781\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -575.2345459609564 Explore P: 0.2824 SOC: 0.6514 Cumulative_SOC_deviation: 52.6738 Fuel Consumption: 48.4970\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -537.0998184824243 Explore P: 0.2750 SOC: 0.6690 Cumulative_SOC_deviation: 48.7057 Fuel Consumption: 50.0432\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -612.4677602506844 Explore P: 0.2678 SOC: 0.6342 Cumulative_SOC_deviation: 56.4813 Fuel Consumption: 47.6552\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -504.27080519290445 Explore P: 0.2608 SOC: 0.6256 Cumulative_SOC_deviation: 45.7888 Fuel Consumption: 46.3832\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -575.8967475874915 Explore P: 0.2540 SOC: 0.6274 Cumulative_SOC_deviation: 52.9378 Fuel Consumption: 46.5185\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -358.9695401395821 Explore P: 0.2474 SOC: 0.6219 Cumulative_SOC_deviation: 31.3093 Fuel Consumption: 45.8769\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -444.7974776929677 Explore P: 0.2410 SOC: 0.6305 Cumulative_SOC_deviation: 39.7890 Fuel Consumption: 46.9077\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -382.98882399234225 Explore P: 0.2347 SOC: 0.6177 Cumulative_SOC_deviation: 33.7042 Fuel Consumption: 45.9468\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -479.28717149738867 Explore P: 0.2286 SOC: 0.6227 Cumulative_SOC_deviation: 43.2522 Fuel Consumption: 46.7653\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -393.10194429922853 Explore P: 0.2227 SOC: 0.6159 Cumulative_SOC_deviation: 34.7285 Fuel Consumption: 45.8171\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -358.9049330725132 Explore P: 0.2170 SOC: 0.6174 Cumulative_SOC_deviation: 31.2921 Fuel Consumption: 45.9838\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -462.8704549374253 Explore P: 0.2114 SOC: 0.6178 Cumulative_SOC_deviation: 41.6636 Fuel Consumption: 46.2342\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -377.5789816423852 Explore P: 0.2059 SOC: 0.6216 Cumulative_SOC_deviation: 33.1008 Fuel Consumption: 46.5707\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -441.89552488650514 Explore P: 0.2006 SOC: 0.6089 Cumulative_SOC_deviation: 39.6363 Fuel Consumption: 45.5328\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -353.84593257023494 Explore P: 0.1954 SOC: 0.6158 Cumulative_SOC_deviation: 30.7626 Fuel Consumption: 46.2200\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -364.4413213856179 Explore P: 0.1904 SOC: 0.6064 Cumulative_SOC_deviation: 31.8808 Fuel Consumption: 45.6336\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -492.7480347792384 Explore P: 0.1855 SOC: 0.6108 Cumulative_SOC_deviation: 44.7290 Fuel Consumption: 45.4578\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -463.972358480483 Explore P: 0.1808 SOC: 0.6138 Cumulative_SOC_deviation: 41.8006 Fuel Consumption: 45.9661\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -445.43457467560546 Explore P: 0.1761 SOC: 0.6076 Cumulative_SOC_deviation: 40.0385 Fuel Consumption: 45.0498\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -357.020841833915 Explore P: 0.1716 SOC: 0.6123 Cumulative_SOC_deviation: 31.1482 Fuel Consumption: 45.5384\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -325.2108632772732 Explore P: 0.1673 SOC: 0.6076 Cumulative_SOC_deviation: 28.0094 Fuel Consumption: 45.1172\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -357.2574391094842 Explore P: 0.1630 SOC: 0.6086 Cumulative_SOC_deviation: 31.1885 Fuel Consumption: 45.3726\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -568.3443228005368 Explore P: 0.1589 SOC: 0.6080 Cumulative_SOC_deviation: 52.2905 Fuel Consumption: 45.4390\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -340.4629807574316 Explore P: 0.1548 SOC: 0.6020 Cumulative_SOC_deviation: 29.5742 Fuel Consumption: 44.7209\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -565.4609146106092 Explore P: 0.1509 SOC: 0.6085 Cumulative_SOC_deviation: 52.0040 Fuel Consumption: 45.4213\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -374.62174488438905 Explore P: 0.1471 SOC: 0.6054 Cumulative_SOC_deviation: 32.9275 Fuel Consumption: 45.3463\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -447.30558889211767 Explore P: 0.1434 SOC: 0.6077 Cumulative_SOC_deviation: 40.2359 Fuel Consumption: 44.9470\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -484.73482885359084 Explore P: 0.1398 SOC: 0.6092 Cumulative_SOC_deviation: 43.9422 Fuel Consumption: 45.3123\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -392.45955548193314 Explore P: 0.1362 SOC: 0.6107 Cumulative_SOC_deviation: 34.7106 Fuel Consumption: 45.3539\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -383.8836856855057 Explore P: 0.1328 SOC: 0.6146 Cumulative_SOC_deviation: 33.8295 Fuel Consumption: 45.5885\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -376.1832841770468 Explore P: 0.1295 SOC: 0.6109 Cumulative_SOC_deviation: 33.1155 Fuel Consumption: 45.0285\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -432.4297853916743 Explore P: 0.1263 SOC: 0.6106 Cumulative_SOC_deviation: 38.7190 Fuel Consumption: 45.2399\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -404.1195125533568 Explore P: 0.1231 SOC: 0.6075 Cumulative_SOC_deviation: 35.8686 Fuel Consumption: 45.4335\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -387.2418427295254 Explore P: 0.1200 SOC: 0.6069 Cumulative_SOC_deviation: 34.1947 Fuel Consumption: 45.2946\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -404.7987907076296 Explore P: 0.1171 SOC: 0.6070 Cumulative_SOC_deviation: 35.9357 Fuel Consumption: 45.4416\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -302.6839539805558 Explore P: 0.1142 SOC: 0.6078 Cumulative_SOC_deviation: 25.7427 Fuel Consumption: 45.2566\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -427.94514042561076 Explore P: 0.1113 SOC: 0.6075 Cumulative_SOC_deviation: 38.2603 Fuel Consumption: 45.3422\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -276.3307663233715 Explore P: 0.1086 SOC: 0.6061 Cumulative_SOC_deviation: 23.1291 Fuel Consumption: 45.0402\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -262.91418866110934 Explore P: 0.1059 SOC: 0.6062 Cumulative_SOC_deviation: 21.7851 Fuel Consumption: 45.0630\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -280.2019270560471 Explore P: 0.1033 SOC: 0.6062 Cumulative_SOC_deviation: 23.5091 Fuel Consumption: 45.1106\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -275.9122179487394 Explore P: 0.1008 SOC: 0.6056 Cumulative_SOC_deviation: 23.0803 Fuel Consumption: 45.1091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -359.4948813846589 Explore P: 0.0983 SOC: 0.6072 Cumulative_SOC_deviation: 31.4466 Fuel Consumption: 45.0291\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -455.9313140329937 Explore P: 0.0960 SOC: 0.6092 Cumulative_SOC_deviation: 41.0316 Fuel Consumption: 45.6149\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -321.63726808321945 Explore P: 0.0936 SOC: 0.6067 Cumulative_SOC_deviation: 27.6688 Fuel Consumption: 44.9495\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -261.73567321924685 Explore P: 0.0914 SOC: 0.6094 Cumulative_SOC_deviation: 21.6359 Fuel Consumption: 45.3767\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -323.23006191584534 Explore P: 0.0892 SOC: 0.6055 Cumulative_SOC_deviation: 27.8283 Fuel Consumption: 44.9470\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -295.9296048987878 Explore P: 0.0870 SOC: 0.6073 Cumulative_SOC_deviation: 25.0873 Fuel Consumption: 45.0569\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -355.34940028402025 Explore P: 0.0849 SOC: 0.6103 Cumulative_SOC_deviation: 30.9810 Fuel Consumption: 45.5399\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -321.43084082535313 Explore P: 0.0829 SOC: 0.6103 Cumulative_SOC_deviation: 27.6274 Fuel Consumption: 45.1573\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -380.8256136432592 Explore P: 0.0809 SOC: 0.6117 Cumulative_SOC_deviation: 33.5307 Fuel Consumption: 45.5191\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -419.9172710746638 Explore P: 0.0790 SOC: 0.6050 Cumulative_SOC_deviation: 37.5195 Fuel Consumption: 44.7219\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -390.3469097481112 Explore P: 0.0771 SOC: 0.6069 Cumulative_SOC_deviation: 34.5218 Fuel Consumption: 45.1294\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -338.3302381970028 Explore P: 0.0753 SOC: 0.6071 Cumulative_SOC_deviation: 29.3100 Fuel Consumption: 45.2302\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -318.6973404954669 Explore P: 0.0735 SOC: 0.6135 Cumulative_SOC_deviation: 27.3505 Fuel Consumption: 45.1927\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -390.7072530690817 Explore P: 0.0718 SOC: 0.6068 Cumulative_SOC_deviation: 34.5793 Fuel Consumption: 44.9145\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -403.9243149175628 Explore P: 0.0701 SOC: 0.6087 Cumulative_SOC_deviation: 35.8875 Fuel Consumption: 45.0488\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -345.8412476136314 Explore P: 0.0685 SOC: 0.6046 Cumulative_SOC_deviation: 30.1127 Fuel Consumption: 44.7143\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -442.16138351531515 Explore P: 0.0669 SOC: 0.6042 Cumulative_SOC_deviation: 39.7425 Fuel Consumption: 44.7361\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -361.52665314933006 Explore P: 0.0654 SOC: 0.6068 Cumulative_SOC_deviation: 31.6471 Fuel Consumption: 45.0554\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -303.6597349656535 Explore P: 0.0639 SOC: 0.6050 Cumulative_SOC_deviation: 25.8880 Fuel Consumption: 44.7802\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -338.5780626626231 Explore P: 0.0624 SOC: 0.6066 Cumulative_SOC_deviation: 29.3584 Fuel Consumption: 44.9941\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -385.2136298312495 Explore P: 0.0610 SOC: 0.6324 Cumulative_SOC_deviation: 33.8222 Fuel Consumption: 46.9918\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -303.39657383509035 Explore P: 0.0596 SOC: 0.6069 Cumulative_SOC_deviation: 25.8444 Fuel Consumption: 44.9525\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -395.8359469925128 Explore P: 0.0583 SOC: 0.6039 Cumulative_SOC_deviation: 35.1351 Fuel Consumption: 44.4853\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -441.6852264573544 Explore P: 0.0570 SOC: 0.6047 Cumulative_SOC_deviation: 39.6569 Fuel Consumption: 45.1167\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -338.36420247581293 Explore P: 0.0557 SOC: 0.6045 Cumulative_SOC_deviation: 29.3492 Fuel Consumption: 44.8719\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -472.7924881633594 Explore P: 0.0545 SOC: 0.6069 Cumulative_SOC_deviation: 42.7745 Fuel Consumption: 45.0473\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -268.4750907152136 Explore P: 0.0533 SOC: 0.6034 Cumulative_SOC_deviation: 22.3616 Fuel Consumption: 44.8588\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -266.3456883201422 Explore P: 0.0521 SOC: 0.6028 Cumulative_SOC_deviation: 22.1367 Fuel Consumption: 44.9784\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -371.89866289334554 Explore P: 0.0510 SOC: 0.6083 Cumulative_SOC_deviation: 32.6407 Fuel Consumption: 45.4912\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -321.17076285132254 Explore P: 0.0498 SOC: 0.6055 Cumulative_SOC_deviation: 27.6083 Fuel Consumption: 45.0878\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -323.70112887067796 Explore P: 0.0488 SOC: 0.6060 Cumulative_SOC_deviation: 27.8485 Fuel Consumption: 45.2161\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -329.6322824387107 Explore P: 0.0477 SOC: 0.6050 Cumulative_SOC_deviation: 28.4696 Fuel Consumption: 44.9358\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -258.1066830108466 Explore P: 0.0467 SOC: 0.6071 Cumulative_SOC_deviation: 21.3074 Fuel Consumption: 45.0331\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -298.4041965036278 Explore P: 0.0457 SOC: 0.6096 Cumulative_SOC_deviation: 25.3432 Fuel Consumption: 44.9718\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -260.7894753650744 Explore P: 0.0447 SOC: 0.6030 Cumulative_SOC_deviation: 21.6251 Fuel Consumption: 44.5385\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -360.55194731752056 Explore P: 0.0438 SOC: 0.6062 Cumulative_SOC_deviation: 31.5512 Fuel Consumption: 45.0402\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -265.3679074974175 Explore P: 0.0429 SOC: 0.6070 Cumulative_SOC_deviation: 22.0525 Fuel Consumption: 44.8431\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -322.9075891672841 Explore P: 0.0420 SOC: 0.6036 Cumulative_SOC_deviation: 27.8396 Fuel Consumption: 44.5111\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -319.2173594416337 Explore P: 0.0411 SOC: 0.6061 Cumulative_SOC_deviation: 27.4143 Fuel Consumption: 45.0747\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -237.29025659763707 Explore P: 0.0403 SOC: 0.6073 Cumulative_SOC_deviation: 19.2414 Fuel Consumption: 44.8765\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -227.7488624462791 Explore P: 0.0395 SOC: 0.6053 Cumulative_SOC_deviation: 18.3088 Fuel Consumption: 44.6611\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -328.4921711129178 Explore P: 0.0387 SOC: 0.6051 Cumulative_SOC_deviation: 28.3611 Fuel Consumption: 44.8816\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -314.39085958716953 Explore P: 0.0379 SOC: 0.6048 Cumulative_SOC_deviation: 26.9729 Fuel Consumption: 44.6616\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -330.76382971016994 Explore P: 0.0371 SOC: 0.6087 Cumulative_SOC_deviation: 28.5493 Fuel Consumption: 45.2708\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -303.2491912900285 Explore P: 0.0364 SOC: 0.6041 Cumulative_SOC_deviation: 25.8493 Fuel Consumption: 44.7559\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -295.51438559888214 Explore P: 0.0357 SOC: 0.6063 Cumulative_SOC_deviation: 25.0630 Fuel Consumption: 44.8846\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -319.7379248421778 Explore P: 0.0350 SOC: 0.6080 Cumulative_SOC_deviation: 27.4644 Fuel Consumption: 45.0939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -261.71110551215986 Explore P: 0.0343 SOC: 0.6044 Cumulative_SOC_deviation: 21.7023 Fuel Consumption: 44.6885\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -250.91495608640744 Explore P: 0.0336 SOC: 0.6042 Cumulative_SOC_deviation: 20.6061 Fuel Consumption: 44.8537\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -376.9395748464415 Explore P: 0.0330 SOC: 0.6033 Cumulative_SOC_deviation: 33.2112 Fuel Consumption: 44.8279\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -241.85417896026306 Explore P: 0.0324 SOC: 0.6046 Cumulative_SOC_deviation: 19.7179 Fuel Consumption: 44.6753\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -268.2520178140009 Explore P: 0.0318 SOC: 0.6024 Cumulative_SOC_deviation: 22.3204 Fuel Consumption: 45.0478\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -309.8075322912945 Explore P: 0.0312 SOC: 0.6038 Cumulative_SOC_deviation: 26.4846 Fuel Consumption: 44.9611\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -262.2920936097577 Explore P: 0.0306 SOC: 0.6048 Cumulative_SOC_deviation: 21.7377 Fuel Consumption: 44.9150\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -343.02719810516396 Explore P: 0.0301 SOC: 0.6056 Cumulative_SOC_deviation: 29.8107 Fuel Consumption: 44.9201\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -308.63764988349504 Explore P: 0.0295 SOC: 0.6069 Cumulative_SOC_deviation: 26.3589 Fuel Consumption: 45.0483\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -346.84554727332824 Explore P: 0.0290 SOC: 0.6050 Cumulative_SOC_deviation: 30.1610 Fuel Consumption: 45.2358\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -515.2793901776254 Explore P: 0.0285 SOC: 0.6053 Cumulative_SOC_deviation: 46.9783 Fuel Consumption: 45.4968\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -236.99637472312682 Explore P: 0.0280 SOC: 0.6052 Cumulative_SOC_deviation: 19.1940 Fuel Consumption: 45.0564\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -292.07804401181943 Explore P: 0.0275 SOC: 0.6028 Cumulative_SOC_deviation: 24.7263 Fuel Consumption: 44.8152\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -267.2797293668725 Explore P: 0.0270 SOC: 0.6030 Cumulative_SOC_deviation: 22.2334 Fuel Consumption: 44.9454\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -288.83560693567665 Explore P: 0.0265 SOC: 0.6059 Cumulative_SOC_deviation: 24.3744 Fuel Consumption: 45.0914\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -299.5608805479011 Explore P: 0.0261 SOC: 0.6039 Cumulative_SOC_deviation: 25.4592 Fuel Consumption: 44.9693\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -273.68142433328904 Explore P: 0.0257 SOC: 0.6043 Cumulative_SOC_deviation: 22.8876 Fuel Consumption: 44.8056\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -332.7079709763671 Explore P: 0.0252 SOC: 0.6045 Cumulative_SOC_deviation: 28.7900 Fuel Consumption: 44.8076\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -288.1155041696871 Explore P: 0.0248 SOC: 0.6031 Cumulative_SOC_deviation: 24.3366 Fuel Consumption: 44.7493\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -367.0483595360957 Explore P: 0.0244 SOC: 0.6031 Cumulative_SOC_deviation: 32.2000 Fuel Consumption: 45.0483\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -353.2416186244621 Explore P: 0.0240 SOC: 0.6023 Cumulative_SOC_deviation: 30.8718 Fuel Consumption: 44.5233\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -322.555867786287 Explore P: 0.0237 SOC: 0.6043 Cumulative_SOC_deviation: 27.7817 Fuel Consumption: 44.7387\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -327.69986187750163 Explore P: 0.0233 SOC: 0.6065 Cumulative_SOC_deviation: 28.2966 Fuel Consumption: 44.7336\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -275.2239269720412 Explore P: 0.0229 SOC: 0.6056 Cumulative_SOC_deviation: 23.0424 Fuel Consumption: 44.7995\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -289.3534065784198 Explore P: 0.0226 SOC: 0.6061 Cumulative_SOC_deviation: 24.4659 Fuel Consumption: 44.6946\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -264.6806693245583 Explore P: 0.0222 SOC: 0.6036 Cumulative_SOC_deviation: 22.0076 Fuel Consumption: 44.6044\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -236.0095438978569 Explore P: 0.0219 SOC: 0.6045 Cumulative_SOC_deviation: 19.1359 Fuel Consumption: 44.6505\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -347.55205182838733 Explore P: 0.0216 SOC: 0.6046 Cumulative_SOC_deviation: 30.2822 Fuel Consumption: 44.7301\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -328.60372521100203 Explore P: 0.0213 SOC: 0.6029 Cumulative_SOC_deviation: 28.3792 Fuel Consumption: 44.8121\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -319.3357057630945 Explore P: 0.0210 SOC: 0.6047 Cumulative_SOC_deviation: 27.4415 Fuel Consumption: 44.9206\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -306.9993603424527 Explore P: 0.0207 SOC: 0.6044 Cumulative_SOC_deviation: 26.2237 Fuel Consumption: 44.7625\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -392.37661690041415 Explore P: 0.0204 SOC: 0.6041 Cumulative_SOC_deviation: 34.7652 Fuel Consumption: 44.7245\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -261.42846643656986 Explore P: 0.0201 SOC: 0.6045 Cumulative_SOC_deviation: 21.6589 Fuel Consumption: 44.8390\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -296.05550989770586 Explore P: 0.0198 SOC: 0.6033 Cumulative_SOC_deviation: 25.1284 Fuel Consumption: 44.7716\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -332.73762382929556 Explore P: 0.0196 SOC: 0.6032 Cumulative_SOC_deviation: 28.7849 Fuel Consumption: 44.8882\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -392.72498611805 Explore P: 0.0193 SOC: 0.6038 Cumulative_SOC_deviation: 34.7740 Fuel Consumption: 44.9850\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -269.3927865238052 Explore P: 0.0190 SOC: 0.6035 Cumulative_SOC_deviation: 22.4805 Fuel Consumption: 44.5876\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -298.6449638428373 Explore P: 0.0188 SOC: 0.6041 Cumulative_SOC_deviation: 25.3613 Fuel Consumption: 45.0316\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -400.6136820260281 Explore P: 0.0186 SOC: 0.6034 Cumulative_SOC_deviation: 35.5517 Fuel Consumption: 45.0965\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -280.9732008498865 Explore P: 0.0183 SOC: 0.6032 Cumulative_SOC_deviation: 23.6332 Fuel Consumption: 44.6409\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -317.1478314815992 Explore P: 0.0181 SOC: 0.6028 Cumulative_SOC_deviation: 27.2211 Fuel Consumption: 44.9373\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -381.2039345163673 Explore P: 0.0179 SOC: 0.6031 Cumulative_SOC_deviation: 33.6080 Fuel Consumption: 45.1243\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -237.84672112170304 Explore P: 0.0177 SOC: 0.6023 Cumulative_SOC_deviation: 19.3203 Fuel Consumption: 44.6439\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -260.9717539033617 Explore P: 0.0175 SOC: 0.6016 Cumulative_SOC_deviation: 21.6588 Fuel Consumption: 44.3839\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -427.1362990991119 Explore P: 0.0173 SOC: 0.6020 Cumulative_SOC_deviation: 38.2109 Fuel Consumption: 45.0270\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -350.17408488114546 Explore P: 0.0171 SOC: 0.6027 Cumulative_SOC_deviation: 30.5217 Fuel Consumption: 44.9571\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -495.99201311931046 Explore P: 0.0169 SOC: 0.6043 Cumulative_SOC_deviation: 45.0949 Fuel Consumption: 45.0427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -287.95564647759767 Explore P: 0.0167 SOC: 0.6040 Cumulative_SOC_deviation: 24.3221 Fuel Consumption: 44.7351\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -262.1296909959366 Explore P: 0.0165 SOC: 0.6052 Cumulative_SOC_deviation: 21.7264 Fuel Consumption: 44.8659\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -303.79024453007355 Explore P: 0.0163 SOC: 0.6038 Cumulative_SOC_deviation: 25.8831 Fuel Consumption: 44.9596\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -307.82246799754864 Explore P: 0.0162 SOC: 0.6068 Cumulative_SOC_deviation: 26.2585 Fuel Consumption: 45.2378\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -297.47741032071553 Explore P: 0.0160 SOC: 0.6040 Cumulative_SOC_deviation: 25.2641 Fuel Consumption: 44.8360\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -301.14302975683944 Explore P: 0.0158 SOC: 0.6037 Cumulative_SOC_deviation: 25.6509 Fuel Consumption: 44.6343\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -325.3408663514426 Explore P: 0.0157 SOC: 0.6049 Cumulative_SOC_deviation: 28.0186 Fuel Consumption: 45.1552\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -372.61013499547715 Explore P: 0.0155 SOC: 0.6035 Cumulative_SOC_deviation: 32.7785 Fuel Consumption: 44.8248\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -280.2081077768741 Explore P: 0.0154 SOC: 0.6051 Cumulative_SOC_deviation: 23.5293 Fuel Consumption: 44.9155\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -265.5852983240651 Explore P: 0.0152 SOC: 0.6056 Cumulative_SOC_deviation: 22.0550 Fuel Consumption: 45.0356\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -211.57225948803972 Explore P: 0.0151 SOC: 0.6049 Cumulative_SOC_deviation: 16.6771 Fuel Consumption: 44.8010\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -231.22694379637514 Explore P: 0.0149 SOC: 0.6038 Cumulative_SOC_deviation: 18.6680 Fuel Consumption: 44.5471\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -226.95769638399742 Explore P: 0.0148 SOC: 0.6024 Cumulative_SOC_deviation: 18.2256 Fuel Consumption: 44.7017\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -313.60969776426236 Explore P: 0.0147 SOC: 0.6060 Cumulative_SOC_deviation: 26.8658 Fuel Consumption: 44.9520\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -221.71648795740307 Explore P: 0.0146 SOC: 0.6062 Cumulative_SOC_deviation: 17.6741 Fuel Consumption: 44.9758\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -255.22787531178588 Explore P: 0.0144 SOC: 0.6066 Cumulative_SOC_deviation: 21.0343 Fuel Consumption: 44.8851\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -227.07890172111212 Explore P: 0.0143 SOC: 0.6051 Cumulative_SOC_deviation: 18.2413 Fuel Consumption: 44.6662\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -261.4175311147767 Explore P: 0.0142 SOC: 0.6032 Cumulative_SOC_deviation: 21.6934 Fuel Consumption: 44.4832\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -261.50095298758754 Explore P: 0.0141 SOC: 0.6028 Cumulative_SOC_deviation: 21.7235 Fuel Consumption: 44.2663\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {i}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-4b3ac91aa511>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2761\u001b[0m     return gca().plot(\n\u001b[0;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m-> 2763\u001b[1;33m         is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1647\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1648\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1649\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1650\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request_autoscale_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1651\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36madd_line\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   1848\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1850\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1851\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_line%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_update_line_limits\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   1870\u001b[0m         \u001b[0mFigures\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdating\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1871\u001b[0m         \"\"\"\n\u001b[1;32m-> 1872\u001b[1;33m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1873\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1874\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mget_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \"\"\"\n\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1027\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1028\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mrecache\u001b[1;34m(self, always)\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0malways\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m             \u001b[0myconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_yorig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_to_unmasked_float_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myconv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\matplotlib\\cbook\\__init__.py\u001b[0m in \u001b[0;36m_to_unmasked_float_array\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1315\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1317\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2.1_song\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'dict'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "for size, history in results_dict.items(): \n",
    "    plt.plot(history, label=size, linewidth=3.0) \n",
    "\n",
    "\n",
    "plt.grid() \n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel(\"episode number\", fontsize=20) \n",
    "plt.ylabel(\"total rewards\", fontsize=20) \n",
    "plt.xlim([0, 120])\n",
    "\n",
    "\n",
    "plt.savefig(\"replay_memory_size_effect_300.png\")\n",
    "with open(\"replay_memory_size_effect_300.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-(3 > 2) * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

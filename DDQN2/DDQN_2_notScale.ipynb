{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN_notScaling import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 1000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 1 Total reward: -4849.612190121086 Explore P: 0.9732 SOC: 1.0000 Cumulative_SOC_deviation: 472.3491 Fuel Consumption: 126.1215\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 2 Total reward: -4856.575975481001 Explore P: 0.9471 SOC: 1.0000 Cumulative_SOC_deviation: 472.6838 Fuel Consumption: 129.7382\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -4892.649502883731 Explore P: 0.9217 SOC: 1.0000 Cumulative_SOC_deviation: 476.8415 Fuel Consumption: 124.2349\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -4928.114804635577 Explore P: 0.8970 SOC: 1.0000 Cumulative_SOC_deviation: 480.4613 Fuel Consumption: 123.5017\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -4958.630320606198 Explore P: 0.8730 SOC: 1.0000 Cumulative_SOC_deviation: 483.3301 Fuel Consumption: 125.3297\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -4759.196180593954 Explore P: 0.8496 SOC: 1.0000 Cumulative_SOC_deviation: 463.4736 Fuel Consumption: 124.4597\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -4807.649903089565 Explore P: 0.8269 SOC: 1.0000 Cumulative_SOC_deviation: 468.6563 Fuel Consumption: 121.0873\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -4825.6222010470565 Explore P: 0.8048 SOC: 1.0000 Cumulative_SOC_deviation: 470.1426 Fuel Consumption: 124.1958\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -4875.939626230637 Explore P: 0.7832 SOC: 1.0000 Cumulative_SOC_deviation: 475.1157 Fuel Consumption: 124.7823\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -4976.505726210958 Explore P: 0.7623 SOC: 1.0000 Cumulative_SOC_deviation: 484.8478 Fuel Consumption: 128.0276\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -4941.760585322456 Explore P: 0.7419 SOC: 1.0000 Cumulative_SOC_deviation: 481.0497 Fuel Consumption: 131.2632\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -4898.637963651478 Explore P: 0.7221 SOC: 1.0000 Cumulative_SOC_deviation: 476.9467 Fuel Consumption: 129.1713\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -4843.406053650543 Explore P: 0.7028 SOC: 1.0000 Cumulative_SOC_deviation: 471.9601 Fuel Consumption: 123.8048\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -4924.96915946333 Explore P: 0.6840 SOC: 1.0000 Cumulative_SOC_deviation: 479.9405 Fuel Consumption: 125.5643\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -4869.785866852595 Explore P: 0.6658 SOC: 1.0000 Cumulative_SOC_deviation: 474.3264 Fuel Consumption: 126.5222\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -4921.969142721964 Explore P: 0.6480 SOC: 1.0000 Cumulative_SOC_deviation: 479.6522 Fuel Consumption: 125.4470\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -4886.281156468929 Explore P: 0.6307 SOC: 1.0000 Cumulative_SOC_deviation: 476.1489 Fuel Consumption: 124.7921\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -4926.725960813578 Explore P: 0.6139 SOC: 1.0000 Cumulative_SOC_deviation: 479.4730 Fuel Consumption: 131.9963\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -5013.632414571834 Explore P: 0.5976 SOC: 1.0000 Cumulative_SOC_deviation: 488.1382 Fuel Consumption: 132.2504\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -5014.129376691162 Explore P: 0.5816 SOC: 1.0000 Cumulative_SOC_deviation: 488.3658 Fuel Consumption: 130.4714\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -5015.265731469958 Explore P: 0.5662 SOC: 1.0000 Cumulative_SOC_deviation: 488.5655 Fuel Consumption: 129.6112\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -5003.860072486095 Explore P: 0.5511 SOC: 1.0000 Cumulative_SOC_deviation: 486.8188 Fuel Consumption: 135.6717\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -4904.458159571626 Explore P: 0.5364 SOC: 1.0000 Cumulative_SOC_deviation: 477.1044 Fuel Consumption: 133.4137\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -4744.91992497978 Explore P: 0.5222 SOC: 1.0000 Cumulative_SOC_deviation: 462.6853 Fuel Consumption: 118.0668\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -4911.734122692702 Explore P: 0.5083 SOC: 1.0000 Cumulative_SOC_deviation: 479.0852 Fuel Consumption: 120.8820\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -4732.895748925179 Explore P: 0.4948 SOC: 1.0000 Cumulative_SOC_deviation: 460.7595 Fuel Consumption: 125.3004\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -4901.876996219665 Explore P: 0.4817 SOC: 1.0000 Cumulative_SOC_deviation: 478.2324 Fuel Consumption: 119.5526\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -4916.416325502397 Explore P: 0.4689 SOC: 1.0000 Cumulative_SOC_deviation: 478.7656 Fuel Consumption: 128.7607\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -4878.719427088242 Explore P: 0.4565 SOC: 1.0000 Cumulative_SOC_deviation: 475.5374 Fuel Consumption: 123.3453\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -4557.186771588927 Explore P: 0.4444 SOC: 1.0000 Cumulative_SOC_deviation: 443.5601 Fuel Consumption: 121.5858\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -4949.990291458326 Explore P: 0.4326 SOC: 1.0000 Cumulative_SOC_deviation: 482.6577 Fuel Consumption: 123.4138\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -4763.213501324008 Explore P: 0.4212 SOC: 1.0000 Cumulative_SOC_deviation: 464.2048 Fuel Consumption: 121.1655\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -4998.635622769304 Explore P: 0.4100 SOC: 1.0000 Cumulative_SOC_deviation: 487.4645 Fuel Consumption: 123.9905\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -4874.87709691118 Explore P: 0.3992 SOC: 1.0000 Cumulative_SOC_deviation: 474.5481 Fuel Consumption: 129.3961\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -4823.709062821522 Explore P: 0.3887 SOC: 1.0000 Cumulative_SOC_deviation: 469.7940 Fuel Consumption: 125.7696\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -4786.629770904537 Explore P: 0.3784 SOC: 1.0000 Cumulative_SOC_deviation: 466.4506 Fuel Consumption: 122.1235\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -4801.43215004123 Explore P: 0.3684 SOC: 1.0000 Cumulative_SOC_deviation: 467.9680 Fuel Consumption: 121.7520\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -4705.273095463747 Explore P: 0.3587 SOC: 1.0000 Cumulative_SOC_deviation: 458.7265 Fuel Consumption: 118.0081\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -4669.076912049152 Explore P: 0.3493 SOC: 1.0000 Cumulative_SOC_deviation: 454.1890 Fuel Consumption: 127.1869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -4685.998528035451 Explore P: 0.3401 SOC: 1.0000 Cumulative_SOC_deviation: 457.6123 Fuel Consumption: 109.8753\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -4712.573878267805 Explore P: 0.3311 SOC: 1.0000 Cumulative_SOC_deviation: 459.1653 Fuel Consumption: 120.9211\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -4743.962006191131 Explore P: 0.3224 SOC: 1.0000 Cumulative_SOC_deviation: 462.6257 Fuel Consumption: 117.7051\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -4847.147205339332 Explore P: 0.3140 SOC: 1.0000 Cumulative_SOC_deviation: 473.2140 Fuel Consumption: 115.0072\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -4781.799621872613 Explore P: 0.3057 SOC: 1.0000 Cumulative_SOC_deviation: 466.2941 Fuel Consumption: 118.8586\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -4721.931871608275 Explore P: 0.2977 SOC: 1.0000 Cumulative_SOC_deviation: 460.1578 Fuel Consumption: 120.3542\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -4861.449046428759 Explore P: 0.2899 SOC: 1.0000 Cumulative_SOC_deviation: 473.9189 Fuel Consumption: 122.2603\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -4789.671133816966 Explore P: 0.2824 SOC: 1.0000 Cumulative_SOC_deviation: 467.5553 Fuel Consumption: 114.1177\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -5001.601164044853 Explore P: 0.2750 SOC: 1.0000 Cumulative_SOC_deviation: 487.1081 Fuel Consumption: 130.5203\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -5021.783298455035 Explore P: 0.2678 SOC: 1.0000 Cumulative_SOC_deviation: 489.0129 Fuel Consumption: 131.6542\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -4674.842157511808 Explore P: 0.2608 SOC: 1.0000 Cumulative_SOC_deviation: 455.5074 Fuel Consumption: 119.7677\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -4822.623084962538 Explore P: 0.2540 SOC: 1.0000 Cumulative_SOC_deviation: 469.5026 Fuel Consumption: 127.5975\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -4932.96589684274 Explore P: 0.2474 SOC: 1.0000 Cumulative_SOC_deviation: 481.3374 Fuel Consumption: 119.5917\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -4834.430647429212 Explore P: 0.2410 SOC: 1.0000 Cumulative_SOC_deviation: 471.1251 Fuel Consumption: 123.1792\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -4826.417061139797 Explore P: 0.2347 SOC: 1.0000 Cumulative_SOC_deviation: 469.9592 Fuel Consumption: 126.8253\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -4439.579133913122 Explore P: 0.2286 SOC: 1.0000 Cumulative_SOC_deviation: 433.3370 Fuel Consumption: 106.2096\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -4856.292041566494 Explore P: 0.2227 SOC: 1.0000 Cumulative_SOC_deviation: 473.0337 Fuel Consumption: 125.9553\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -4869.224936779527 Explore P: 0.2170 SOC: 1.0000 Cumulative_SOC_deviation: 474.4276 Fuel Consumption: 124.9485\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -4815.56875769943 Explore P: 0.2114 SOC: 1.0000 Cumulative_SOC_deviation: 469.9965 Fuel Consumption: 115.6035\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -4800.302896220136 Explore P: 0.2059 SOC: 1.0000 Cumulative_SOC_deviation: 468.1767 Fuel Consumption: 118.5360\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -4824.714812891127 Explore P: 0.2006 SOC: 1.0000 Cumulative_SOC_deviation: 469.9942 Fuel Consumption: 124.7725\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -4790.726422870833 Explore P: 0.1954 SOC: 1.0000 Cumulative_SOC_deviation: 466.8867 Fuel Consumption: 121.8595\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -4682.951703264076 Explore P: 0.1904 SOC: 1.0000 Cumulative_SOC_deviation: 457.0154 Fuel Consumption: 112.7980\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -4754.462134554238 Explore P: 0.1855 SOC: 1.0000 Cumulative_SOC_deviation: 463.1518 Fuel Consumption: 122.9446\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -4791.154785730339 Explore P: 0.1808 SOC: 1.0000 Cumulative_SOC_deviation: 467.6910 Fuel Consumption: 114.2447\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -4816.8043167115975 Explore P: 0.1761 SOC: 1.0000 Cumulative_SOC_deviation: 468.8327 Fuel Consumption: 128.4773\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -4689.971719229807 Explore P: 0.1716 SOC: 1.0000 Cumulative_SOC_deviation: 456.7926 Fuel Consumption: 122.0453\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -4823.509893287137 Explore P: 0.1673 SOC: 1.0000 Cumulative_SOC_deviation: 469.9695 Fuel Consumption: 123.8145\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -4646.983039620798 Explore P: 0.1630 SOC: 1.0000 Cumulative_SOC_deviation: 452.8007 Fuel Consumption: 118.9759\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -4809.726049184705 Explore P: 0.1589 SOC: 1.0000 Cumulative_SOC_deviation: 468.6654 Fuel Consumption: 123.0716\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -4451.837058999475 Explore P: 0.1548 SOC: 1.0000 Cumulative_SOC_deviation: 434.8521 Fuel Consumption: 103.3162\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -2919.6304863868627 Explore P: 0.1509 SOC: 1.0000 Cumulative_SOC_deviation: 283.6871 Fuel Consumption: 82.7592\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -2459.5622592800078 Explore P: 0.1471 SOC: 1.0000 Cumulative_SOC_deviation: 238.3519 Fuel Consumption: 76.0437\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -2208.672368758078 Explore P: 0.1434 SOC: 1.0000 Cumulative_SOC_deviation: 213.2922 Fuel Consumption: 75.7505\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -3098.9769664517094 Explore P: 0.1398 SOC: 1.0000 Cumulative_SOC_deviation: 301.9981 Fuel Consumption: 78.9958\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -2391.594391626959 Explore P: 0.1362 SOC: 1.0000 Cumulative_SOC_deviation: 231.0213 Fuel Consumption: 81.3809\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -822.4319352001078 Explore P: 0.1328 SOC: 0.7606 Cumulative_SOC_deviation: 76.7502 Fuel Consumption: 54.9296\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -929.0840410522861 Explore P: 0.1295 SOC: 0.6282 Cumulative_SOC_deviation: 88.2932 Fuel Consumption: 46.1515\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -908.2957567520397 Explore P: 0.1263 SOC: 0.6843 Cumulative_SOC_deviation: 85.9163 Fuel Consumption: 49.1329\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -1201.127865738439 Explore P: 0.1231 SOC: 0.8047 Cumulative_SOC_deviation: 114.4634 Fuel Consumption: 56.4936\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -1046.1851915898399 Explore P: 0.1200 SOC: 0.7730 Cumulative_SOC_deviation: 99.1764 Fuel Consumption: 54.4213\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -1675.2255770765819 Explore P: 0.1171 SOC: 0.9101 Cumulative_SOC_deviation: 161.1156 Fuel Consumption: 64.0693\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -2338.9960208329567 Explore P: 0.1142 SOC: 1.0000 Cumulative_SOC_deviation: 226.3597 Fuel Consumption: 75.3986\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -667.066562584249 Explore P: 0.1113 SOC: 0.7665 Cumulative_SOC_deviation: 61.2821 Fuel Consumption: 54.2453\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -3611.2673769966977 Explore P: 0.1086 SOC: 1.0000 Cumulative_SOC_deviation: 353.5145 Fuel Consumption: 76.1219\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -3462.92332448097 Explore P: 0.1059 SOC: 1.0000 Cumulative_SOC_deviation: 338.1963 Fuel Consumption: 80.9606\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -4364.655161488183 Explore P: 0.1033 SOC: 1.0000 Cumulative_SOC_deviation: 427.6021 Fuel Consumption: 88.6340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -2434.389070211449 Explore P: 0.1008 SOC: 1.0000 Cumulative_SOC_deviation: 235.2392 Fuel Consumption: 81.9968\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -4299.855138909701 Explore P: 0.0983 SOC: 1.0000 Cumulative_SOC_deviation: 420.0547 Fuel Consumption: 99.3084\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -1603.3101956985877 Explore P: 0.0960 SOC: 1.0000 Cumulative_SOC_deviation: 152.8674 Fuel Consumption: 74.6361\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -3371.8161888655322 Explore P: 0.0936 SOC: 1.0000 Cumulative_SOC_deviation: 329.6682 Fuel Consumption: 75.1347\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -2679.864257221907 Explore P: 0.0914 SOC: 1.0000 Cumulative_SOC_deviation: 260.6919 Fuel Consumption: 72.9450\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -4362.27951627063 Explore P: 0.0892 SOC: 1.0000 Cumulative_SOC_deviation: 427.1925 Fuel Consumption: 90.3545\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -2278.4403726044566 Explore P: 0.0870 SOC: 0.9890 Cumulative_SOC_deviation: 221.0031 Fuel Consumption: 68.4094\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -422.7517637687218 Explore P: 0.0849 SOC: 0.7098 Cumulative_SOC_deviation: 37.3169 Fuel Consumption: 49.5826\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -4028.9257416872647 Explore P: 0.0829 SOC: 1.0000 Cumulative_SOC_deviation: 393.6675 Fuel Consumption: 92.2508\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -4399.099140164443 Explore P: 0.0809 SOC: 1.0000 Cumulative_SOC_deviation: 431.1423 Fuel Consumption: 87.6761\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -4448.446878611844 Explore P: 0.0790 SOC: 1.0000 Cumulative_SOC_deviation: 434.3381 Fuel Consumption: 105.0659\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -4171.304200166785 Explore P: 0.0771 SOC: 1.0000 Cumulative_SOC_deviation: 408.3423 Fuel Consumption: 87.8814\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -4023.95760030556 Explore P: 0.0753 SOC: 1.0000 Cumulative_SOC_deviation: 394.0143 Fuel Consumption: 83.8149\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -2795.215458363696 Explore P: 0.0735 SOC: 1.0000 Cumulative_SOC_deviation: 272.2466 Fuel Consumption: 72.7495\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -1398.8108668590808 Explore P: 0.0718 SOC: 0.8838 Cumulative_SOC_deviation: 133.7361 Fuel Consumption: 61.4495\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -3624.0769072812327 Explore P: 0.0701 SOC: 1.0000 Cumulative_SOC_deviation: 353.8327 Fuel Consumption: 85.7504\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -2164.4009421997152 Explore P: 0.0685 SOC: 0.9485 Cumulative_SOC_deviation: 209.8474 Fuel Consumption: 65.9265\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -2011.2475595277992 Explore P: 0.0669 SOC: 0.9163 Cumulative_SOC_deviation: 194.6807 Fuel Consumption: 64.4407\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -1525.5053854482214 Explore P: 0.0654 SOC: 0.8551 Cumulative_SOC_deviation: 146.4173 Fuel Consumption: 61.3322\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -1250.9483953595118 Explore P: 0.0639 SOC: 0.8301 Cumulative_SOC_deviation: 119.1571 Fuel Consumption: 59.3772\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -2545.632898650606 Explore P: 0.0624 SOC: 1.0000 Cumulative_SOC_deviation: 247.3313 Fuel Consumption: 72.3194\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -422.564361293086 Explore P: 0.0610 SOC: 0.6501 Cumulative_SOC_deviation: 37.5934 Fuel Consumption: 46.6305\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -1848.1838847745057 Explore P: 0.0596 SOC: 1.0000 Cumulative_SOC_deviation: 177.3010 Fuel Consumption: 75.1738\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -3536.2193599636553 Explore P: 0.0583 SOC: 1.0000 Cumulative_SOC_deviation: 345.7312 Fuel Consumption: 78.9078\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -1434.9288550015415 Explore P: 0.0570 SOC: 0.5576 Cumulative_SOC_deviation: 139.4936 Fuel Consumption: 39.9932\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -419.64358932178897 Explore P: 0.0557 SOC: 0.6106 Cumulative_SOC_deviation: 37.7089 Fuel Consumption: 42.5543\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -2012.4006499281104 Explore P: 0.0545 SOC: 0.9250 Cumulative_SOC_deviation: 194.9749 Fuel Consumption: 62.6519\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -3305.326607454322 Explore P: 0.0533 SOC: 0.9253 Cumulative_SOC_deviation: 324.1277 Fuel Consumption: 64.0497\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -1267.1819569472839 Explore P: 0.0521 SOC: 0.5648 Cumulative_SOC_deviation: 122.7335 Fuel Consumption: 39.8466\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -994.9819822943149 Explore P: 0.0510 SOC: 0.8280 Cumulative_SOC_deviation: 93.8166 Fuel Consumption: 56.8162\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -3169.764695896695 Explore P: 0.0498 SOC: 1.0000 Cumulative_SOC_deviation: 309.6390 Fuel Consumption: 73.3751\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -2075.4964569716867 Explore P: 0.0488 SOC: 0.9655 Cumulative_SOC_deviation: 200.9335 Fuel Consumption: 66.1611\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -1560.8698573202823 Explore P: 0.0477 SOC: 0.9199 Cumulative_SOC_deviation: 149.7084 Fuel Consumption: 63.7858\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -4619.4156144202525 Explore P: 0.0467 SOC: 1.0000 Cumulative_SOC_deviation: 451.9442 Fuel Consumption: 99.9731\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -2462.6942163175586 Explore P: 0.0457 SOC: 0.9700 Cumulative_SOC_deviation: 239.6797 Fuel Consumption: 65.8972\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -719.6578918062073 Explore P: 0.0447 SOC: 0.6877 Cumulative_SOC_deviation: 67.1766 Fuel Consumption: 47.8915\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -2746.160867670754 Explore P: 0.0438 SOC: 1.0000 Cumulative_SOC_deviation: 267.0723 Fuel Consumption: 75.4377\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -4511.598544613085 Explore P: 0.0429 SOC: 0.9980 Cumulative_SOC_deviation: 443.0296 Fuel Consumption: 81.3027\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -2370.0563239681196 Explore P: 0.0420 SOC: 0.8869 Cumulative_SOC_deviation: 230.8460 Fuel Consumption: 61.5962\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -3914.1158689405347 Explore P: 0.0411 SOC: 1.0000 Cumulative_SOC_deviation: 382.4876 Fuel Consumption: 89.2401\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -4099.25041196016 Explore P: 0.0403 SOC: 1.0000 Cumulative_SOC_deviation: 401.2200 Fuel Consumption: 87.0505\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -1964.9136619503404 Explore P: 0.0395 SOC: 0.9880 Cumulative_SOC_deviation: 189.6191 Fuel Consumption: 68.7222\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -4108.925398908319 Explore P: 0.0387 SOC: 1.0000 Cumulative_SOC_deviation: 403.1513 Fuel Consumption: 77.4122\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -4378.951883433137 Explore P: 0.0379 SOC: 1.0000 Cumulative_SOC_deviation: 427.9468 Fuel Consumption: 99.4844\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -4787.663045135272 Explore P: 0.0371 SOC: 1.0000 Cumulative_SOC_deviation: 469.1297 Fuel Consumption: 96.3661\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -4509.7349910410385 Explore P: 0.0364 SOC: 1.0000 Cumulative_SOC_deviation: 442.3936 Fuel Consumption: 85.7993\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -3944.3891911222595 Explore P: 0.0357 SOC: 1.0000 Cumulative_SOC_deviation: 384.7632 Fuel Consumption: 96.7571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -4579.895142297271 Explore P: 0.0350 SOC: 1.0000 Cumulative_SOC_deviation: 448.0411 Fuel Consumption: 99.4844\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -3886.864486673853 Explore P: 0.0343 SOC: 1.0000 Cumulative_SOC_deviation: 380.3147 Fuel Consumption: 83.7172\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -4890.618142275732 Explore P: 0.0336 SOC: 1.0000 Cumulative_SOC_deviation: 477.1398 Fuel Consumption: 119.2203\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -4198.529785704102 Explore P: 0.0330 SOC: 1.0000 Cumulative_SOC_deviation: 409.6465 Fuel Consumption: 102.0650\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -4427.32915458687 Explore P: 0.0324 SOC: 1.0000 Cumulative_SOC_deviation: 431.8549 Fuel Consumption: 108.7805\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -4581.675133711028 Explore P: 0.0318 SOC: 1.0000 Cumulative_SOC_deviation: 447.5084 Fuel Consumption: 106.5909\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -4612.181547290644 Explore P: 0.0312 SOC: 1.0000 Cumulative_SOC_deviation: 449.6842 Fuel Consumption: 115.3396\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -2887.4546886695553 Explore P: 0.0306 SOC: 0.3563 Cumulative_SOC_deviation: 286.0834 Fuel Consumption: 26.6209\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -1650.6282472547477 Explore P: 0.0301 SOC: 0.4173 Cumulative_SOC_deviation: 162.0312 Fuel Consumption: 30.3159\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -3279.8084846712636 Explore P: 0.0295 SOC: 0.2998 Cumulative_SOC_deviation: 325.6061 Fuel Consumption: 23.7471\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -3710.678406500904 Explore P: 0.0290 SOC: 0.2916 Cumulative_SOC_deviation: 368.7078 Fuel Consumption: 23.6004\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -3542.97369398108 Explore P: 0.0285 SOC: 0.3064 Cumulative_SOC_deviation: 351.8738 Fuel Consumption: 24.2358\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -2119.7060966797944 Explore P: 0.0280 SOC: 0.9483 Cumulative_SOC_deviation: 205.1776 Fuel Consumption: 67.9304\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -3963.782937806555 Explore P: 0.0275 SOC: 0.9868 Cumulative_SOC_deviation: 388.9939 Fuel Consumption: 73.8443\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -2511.4915864549807 Explore P: 0.0270 SOC: 0.9927 Cumulative_SOC_deviation: 244.1978 Fuel Consumption: 69.5140\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -1057.8469176942451 Explore P: 0.0265 SOC: 0.7967 Cumulative_SOC_deviation: 100.1725 Fuel Consumption: 56.1221\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -753.8865579096437 Explore P: 0.0261 SOC: 0.6732 Cumulative_SOC_deviation: 70.7324 Fuel Consumption: 46.5621\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -576.880998290536 Explore P: 0.0257 SOC: 0.6751 Cumulative_SOC_deviation: 53.0202 Fuel Consumption: 46.6794\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -2447.726083072077 Explore P: 0.0252 SOC: 0.8486 Cumulative_SOC_deviation: 239.0059 Fuel Consumption: 57.6666\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -860.8108064504697 Explore P: 0.0248 SOC: 0.7532 Cumulative_SOC_deviation: 80.7191 Fuel Consumption: 53.6197\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -2165.76360407422 Explore P: 0.0244 SOC: 1.0000 Cumulative_SOC_deviation: 208.7296 Fuel Consumption: 78.4680\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -2048.850099367183 Explore P: 0.0240 SOC: 1.0000 Cumulative_SOC_deviation: 197.3168 Fuel Consumption: 75.6821\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -4638.881355252922 Explore P: 0.0237 SOC: 1.0000 Cumulative_SOC_deviation: 455.9436 Fuel Consumption: 79.4455\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -1871.0725890672577 Explore P: 0.0233 SOC: 1.0000 Cumulative_SOC_deviation: 179.8978 Fuel Consumption: 72.0946\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -1593.3813062339568 Explore P: 0.0229 SOC: 0.9904 Cumulative_SOC_deviation: 152.2802 Fuel Consumption: 70.5795\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -3853.4488484856556 Explore P: 0.0226 SOC: 1.0000 Cumulative_SOC_deviation: 375.5206 Fuel Consumption: 98.2429\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -3012.8963789319173 Explore P: 0.0222 SOC: 1.0000 Cumulative_SOC_deviation: 292.3138 Fuel Consumption: 89.7582\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -3365.5040489894664 Explore P: 0.0219 SOC: 1.0000 Cumulative_SOC_deviation: 326.5032 Fuel Consumption: 100.4717\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -3544.667628697277 Explore P: 0.0216 SOC: 1.0000 Cumulative_SOC_deviation: 344.2554 Fuel Consumption: 102.1139\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -1709.7032772763591 Explore P: 0.0213 SOC: 1.0000 Cumulative_SOC_deviation: 163.8391 Fuel Consumption: 71.3126\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -4772.408721031547 Explore P: 0.0210 SOC: 1.0000 Cumulative_SOC_deviation: 468.1126 Fuel Consumption: 91.2831\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -955.9938545128492 Explore P: 0.0207 SOC: 0.7493 Cumulative_SOC_deviation: 90.3850 Fuel Consumption: 52.1437\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -2877.881690801519 Explore P: 0.0204 SOC: 1.0000 Cumulative_SOC_deviation: 279.2874 Fuel Consumption: 85.0075\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -1362.9274455453535 Explore P: 0.0201 SOC: 0.9099 Cumulative_SOC_deviation: 130.0266 Fuel Consumption: 62.6617\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -3209.544476482382 Explore P: 0.0198 SOC: 1.0000 Cumulative_SOC_deviation: 313.8095 Fuel Consumption: 71.4495\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -898.7036136604978 Explore P: 0.0196 SOC: 0.7907 Cumulative_SOC_deviation: 84.3823 Fuel Consumption: 54.8807\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -790.9375893445589 Explore P: 0.0193 SOC: 0.7863 Cumulative_SOC_deviation: 73.7680 Fuel Consumption: 53.2580\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -1215.8598224751331 Explore P: 0.0190 SOC: 0.8230 Cumulative_SOC_deviation: 115.8262 Fuel Consumption: 57.5982\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -1303.169898094731 Explore P: 0.0188 SOC: 0.8566 Cumulative_SOC_deviation: 124.3568 Fuel Consumption: 59.6021\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -1173.410264933229 Explore P: 0.0186 SOC: 0.9100 Cumulative_SOC_deviation: 110.9996 Fuel Consumption: 63.4143\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -2617.2062575724567 Explore P: 0.0183 SOC: 1.0000 Cumulative_SOC_deviation: 254.1309 Fuel Consumption: 75.8971\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -4427.527179728004 Explore P: 0.0181 SOC: 1.0000 Cumulative_SOC_deviation: 432.8473 Fuel Consumption: 99.0543\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -1297.9910786802545 Explore P: 0.0179 SOC: 0.7474 Cumulative_SOC_deviation: 124.5583 Fuel Consumption: 52.4076\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -3641.6718097068747 Explore P: 0.0177 SOC: 1.0000 Cumulative_SOC_deviation: 356.4045 Fuel Consumption: 77.6273\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -1725.509985235932 Explore P: 0.0175 SOC: 0.7087 Cumulative_SOC_deviation: 167.5742 Fuel Consumption: 49.7683\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -3946.8239590547228 Explore P: 0.0173 SOC: 1.0000 Cumulative_SOC_deviation: 385.2990 Fuel Consumption: 93.8344\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -2509.0493566450345 Explore P: 0.0171 SOC: 1.0000 Cumulative_SOC_deviation: 241.6456 Fuel Consumption: 92.5929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -2814.4702262289234 Explore P: 0.0169 SOC: 1.0000 Cumulative_SOC_deviation: 273.0978 Fuel Consumption: 83.4923\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -405.128101721389 Explore P: 0.0167 SOC: 0.6209 Cumulative_SOC_deviation: 36.3082 Fuel Consumption: 42.0460\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -2401.5806641475515 Explore P: 0.0165 SOC: 0.4918 Cumulative_SOC_deviation: 236.7599 Fuel Consumption: 33.9816\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -1959.6628259063398 Explore P: 0.0163 SOC: 0.4802 Cumulative_SOC_deviation: 192.7206 Fuel Consumption: 32.4567\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -1316.7090188230102 Explore P: 0.0162 SOC: 0.5170 Cumulative_SOC_deviation: 128.2923 Fuel Consumption: 33.7861\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -1459.8030651208533 Explore P: 0.0160 SOC: 0.5126 Cumulative_SOC_deviation: 142.5929 Fuel Consumption: 33.8740\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -1375.819171481578 Explore P: 0.0158 SOC: 0.9198 Cumulative_SOC_deviation: 131.4331 Fuel Consumption: 61.4886\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -795.7528064638491 Explore P: 0.0157 SOC: 0.7829 Cumulative_SOC_deviation: 74.3590 Fuel Consumption: 52.1632\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -1773.3480316952114 Explore P: 0.0155 SOC: 0.9671 Cumulative_SOC_deviation: 170.8419 Fuel Consumption: 64.9295\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -1511.1810762838236 Explore P: 0.0154 SOC: 0.9201 Cumulative_SOC_deviation: 145.0494 Fuel Consumption: 60.6871\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -4734.031156536871 Explore P: 0.0152 SOC: 1.0000 Cumulative_SOC_deviation: 465.5739 Fuel Consumption: 78.2920\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -2285.142346924335 Explore P: 0.0151 SOC: 1.0000 Cumulative_SOC_deviation: 220.4084 Fuel Consumption: 81.0584\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -1685.4000078398096 Explore P: 0.0149 SOC: 1.0000 Cumulative_SOC_deviation: 161.2836 Fuel Consumption: 72.5638\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -1108.0049007538705 Explore P: 0.0148 SOC: 0.9724 Cumulative_SOC_deviation: 103.9850 Fuel Consumption: 68.1552\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -2288.5896910929423 Explore P: 0.0147 SOC: 1.0000 Cumulative_SOC_deviation: 221.4941 Fuel Consumption: 73.6488\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -2227.5689565554862 Explore P: 0.0146 SOC: 1.0000 Cumulative_SOC_deviation: 214.6266 Fuel Consumption: 81.3027\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -608.5434275830168 Explore P: 0.0144 SOC: 0.7441 Cumulative_SOC_deviation: 55.7162 Fuel Consumption: 51.3812\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -1194.118590796315 Explore P: 0.0143 SOC: 0.5845 Cumulative_SOC_deviation: 115.2747 Fuel Consumption: 41.3715\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -1812.0650830838347 Explore P: 0.0142 SOC: 0.9810 Cumulative_SOC_deviation: 174.5728 Fuel Consumption: 66.3371\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -729.7184057766735 Explore P: 0.0141 SOC: 0.6643 Cumulative_SOC_deviation: 68.3968 Fuel Consumption: 45.7508\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {i}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 10))\n",
    "# for size, history in results_dict.items(): \n",
    "#     plt.plot(history, label=size, linewidth=3.0) \n",
    "\n",
    "\n",
    "# plt.grid() \n",
    "# plt.legend(fontsize=20)\n",
    "# plt.xlabel(\"episode number\", fontsize=20) \n",
    "# plt.ylabel(\"total rewards\", fontsize=20) \n",
    "# plt.xlim([0, 120])\n",
    "\n",
    "\n",
    "# plt.savefig(\"replay_memory_size_effect_300.png\")\n",
    "with open(\"results_notScaling_2.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN_notScaling import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 1000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 1\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 1 Total reward: -5078.993855890328 Explore P: 0.9732 SOC: 1.0000 Cumulative_SOC_deviation: 492.9331 Fuel Consumption: 149.6630\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 2 Total reward: -5046.81499355331 Explore P: 0.9471 SOC: 1.0000 Cumulative_SOC_deviation: 489.6419 Fuel Consumption: 150.3962\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -5036.431860464665 Explore P: 0.9217 SOC: 1.0000 Cumulative_SOC_deviation: 488.5899 Fuel Consumption: 150.5330\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -5147.078362768599 Explore P: 0.8970 SOC: 1.0000 Cumulative_SOC_deviation: 499.5700 Fuel Consumption: 151.3786\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -5077.102461852976 Explore P: 0.8730 SOC: 1.0000 Cumulative_SOC_deviation: 492.8148 Fuel Consumption: 148.9543\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -5093.43243761448 Explore P: 0.8496 SOC: 1.0000 Cumulative_SOC_deviation: 494.3447 Fuel Consumption: 149.9856\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -5092.599641433346 Explore P: 0.8269 SOC: 1.0000 Cumulative_SOC_deviation: 494.2409 Fuel Consumption: 150.1909\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -5092.272525980972 Explore P: 0.8048 SOC: 1.0000 Cumulative_SOC_deviation: 494.5136 Fuel Consumption: 147.1362\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -5150.7374693692645 Explore P: 0.7832 SOC: 1.0000 Cumulative_SOC_deviation: 499.4901 Fuel Consumption: 155.8360\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -5068.674419260083 Explore P: 0.7623 SOC: 1.0000 Cumulative_SOC_deviation: 492.1440 Fuel Consumption: 147.2339\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -5160.029481715304 Explore P: 0.7419 SOC: 1.0000 Cumulative_SOC_deviation: 500.5611 Fuel Consumption: 154.4186\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -5119.837274719142 Explore P: 0.7221 SOC: 1.0000 Cumulative_SOC_deviation: 497.1548 Fuel Consumption: 148.2896\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -5165.794767220295 Explore P: 0.7028 SOC: 1.0000 Cumulative_SOC_deviation: 500.9333 Fuel Consumption: 156.4616\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -5183.4431540648875 Explore P: 0.6840 SOC: 1.0000 Cumulative_SOC_deviation: 503.1703 Fuel Consumption: 151.7402\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -5122.232050492865 Explore P: 0.6658 SOC: 1.0000 Cumulative_SOC_deviation: 498.1455 Fuel Consumption: 140.7775\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -5082.518960830774 Explore P: 0.6480 SOC: 1.0000 Cumulative_SOC_deviation: 493.7998 Fuel Consumption: 144.5214\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -5033.475966855813 Explore P: 0.6307 SOC: 1.0000 Cumulative_SOC_deviation: 488.8969 Fuel Consumption: 144.5067\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -5023.045138330388 Explore P: 0.6139 SOC: 1.0000 Cumulative_SOC_deviation: 487.2546 Fuel Consumption: 150.4988\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -5132.294574854602 Explore P: 0.5976 SOC: 1.0000 Cumulative_SOC_deviation: 498.4088 Fuel Consumption: 148.2066\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -5058.691584481466 Explore P: 0.5816 SOC: 1.0000 Cumulative_SOC_deviation: 491.1648 Fuel Consumption: 147.0433\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -5122.918822866858 Explore P: 0.5662 SOC: 1.0000 Cumulative_SOC_deviation: 496.9487 Fuel Consumption: 153.4313\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -5132.29656817568 Explore P: 0.5511 SOC: 1.0000 Cumulative_SOC_deviation: 497.7213 Fuel Consumption: 155.0833\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -5190.885038068669 Explore P: 0.5364 SOC: 1.0000 Cumulative_SOC_deviation: 503.1139 Fuel Consumption: 159.7460\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -5181.065233546523 Explore P: 0.5222 SOC: 1.0000 Cumulative_SOC_deviation: 502.6353 Fuel Consumption: 154.7119\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -5135.792322430903 Explore P: 0.5083 SOC: 1.0000 Cumulative_SOC_deviation: 498.3148 Fuel Consumption: 152.6444\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -5101.175915209869 Explore P: 0.4948 SOC: 1.0000 Cumulative_SOC_deviation: 495.1283 Fuel Consumption: 149.8928\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -4990.0820344711 Explore P: 0.4817 SOC: 1.0000 Cumulative_SOC_deviation: 485.0888 Fuel Consumption: 139.1939\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -5110.851405264023 Explore P: 0.4689 SOC: 1.0000 Cumulative_SOC_deviation: 496.1135 Fuel Consumption: 149.7168\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -5068.785728657275 Explore P: 0.4565 SOC: 1.0000 Cumulative_SOC_deviation: 492.1943 Fuel Consumption: 146.8429\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -5044.939129610725 Explore P: 0.4444 SOC: 1.0000 Cumulative_SOC_deviation: 490.2563 Fuel Consumption: 142.3757\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -4957.291263490817 Explore P: 0.4326 SOC: 1.0000 Cumulative_SOC_deviation: 481.3268 Fuel Consumption: 144.0228\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -4927.520958740718 Explore P: 0.4212 SOC: 1.0000 Cumulative_SOC_deviation: 479.1670 Fuel Consumption: 135.8509\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -5218.309095916469 Explore P: 0.4100 SOC: 1.0000 Cumulative_SOC_deviation: 506.4585 Fuel Consumption: 153.7246\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -5113.837699562708 Explore P: 0.3992 SOC: 1.0000 Cumulative_SOC_deviation: 496.7567 Fuel Consumption: 146.2711\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -5030.591955853078 Explore P: 0.3887 SOC: 1.0000 Cumulative_SOC_deviation: 488.3231 Fuel Consumption: 147.3610\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -4896.088696032195 Explore P: 0.3784 SOC: 1.0000 Cumulative_SOC_deviation: 476.6694 Fuel Consumption: 129.3944\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -5082.403298226016 Explore P: 0.3684 SOC: 1.0000 Cumulative_SOC_deviation: 494.4617 Fuel Consumption: 137.7863\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -5007.822899350382 Explore P: 0.3587 SOC: 1.0000 Cumulative_SOC_deviation: 486.1068 Fuel Consumption: 146.7550\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -4747.046244879968 Explore P: 0.3493 SOC: 1.0000 Cumulative_SOC_deviation: 460.7588 Fuel Consumption: 139.4579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -5100.972911927392 Explore P: 0.3401 SOC: 1.0000 Cumulative_SOC_deviation: 494.1427 Fuel Consumption: 159.5456\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -5046.158424321728 Explore P: 0.3311 SOC: 1.0000 Cumulative_SOC_deviation: 489.7243 Fuel Consumption: 148.9152\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -5116.034932782717 Explore P: 0.3224 SOC: 1.0000 Cumulative_SOC_deviation: 497.2467 Fuel Consumption: 143.5683\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -4994.750182711958 Explore P: 0.3140 SOC: 1.0000 Cumulative_SOC_deviation: 484.4970 Fuel Consumption: 149.7803\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -5154.567259137991 Explore P: 0.3057 SOC: 1.0000 Cumulative_SOC_deviation: 500.0940 Fuel Consumption: 153.6268\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -5140.3675549560685 Explore P: 0.2977 SOC: 1.0000 Cumulative_SOC_deviation: 498.8808 Fuel Consumption: 151.5594\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -5053.614314283218 Explore P: 0.2899 SOC: 1.0000 Cumulative_SOC_deviation: 490.9811 Fuel Consumption: 143.8029\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -5047.998202222099 Explore P: 0.2824 SOC: 1.0000 Cumulative_SOC_deviation: 491.0007 Fuel Consumption: 137.9916\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -4958.763277676722 Explore P: 0.2750 SOC: 1.0000 Cumulative_SOC_deviation: 481.4066 Fuel Consumption: 144.6973\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -5190.079732038424 Explore P: 0.2678 SOC: 1.0000 Cumulative_SOC_deviation: 503.2699 Fuel Consumption: 157.3805\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -5077.948945062923 Explore P: 0.2608 SOC: 1.0000 Cumulative_SOC_deviation: 493.0754 Fuel Consumption: 147.1948\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -5111.269538960409 Explore P: 0.2540 SOC: 1.0000 Cumulative_SOC_deviation: 496.4114 Fuel Consumption: 147.1557\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -5028.311158548093 Explore P: 0.2474 SOC: 1.0000 Cumulative_SOC_deviation: 488.0711 Fuel Consumption: 147.6005\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -5061.250206361928 Explore P: 0.2410 SOC: 1.0000 Cumulative_SOC_deviation: 491.8024 Fuel Consumption: 143.2262\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -4991.929520644721 Explore P: 0.2347 SOC: 1.0000 Cumulative_SOC_deviation: 485.6235 Fuel Consumption: 135.6945\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -5086.260568681729 Explore P: 0.2286 SOC: 1.0000 Cumulative_SOC_deviation: 494.7164 Fuel Consumption: 139.0962\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -4940.864628000686 Explore P: 0.2227 SOC: 1.0000 Cumulative_SOC_deviation: 480.1710 Fuel Consumption: 139.1548\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -4865.5115440636055 Explore P: 0.2170 SOC: 1.0000 Cumulative_SOC_deviation: 472.8854 Fuel Consumption: 136.6573\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -4966.863064952302 Explore P: 0.2114 SOC: 1.0000 Cumulative_SOC_deviation: 482.6296 Fuel Consumption: 140.5673\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -4911.908263017416 Explore P: 0.2059 SOC: 1.0000 Cumulative_SOC_deviation: 477.5007 Fuel Consumption: 136.9017\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -5097.018691006222 Explore P: 0.2006 SOC: 1.0000 Cumulative_SOC_deviation: 494.5498 Fuel Consumption: 151.5203\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -5034.718831203862 Explore P: 0.1954 SOC: 1.0000 Cumulative_SOC_deviation: 488.9010 Fuel Consumption: 145.7090\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -5023.138222053546 Explore P: 0.1904 SOC: 1.0000 Cumulative_SOC_deviation: 487.4643 Fuel Consumption: 148.4949\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -5014.266376410317 Explore P: 0.1855 SOC: 1.0000 Cumulative_SOC_deviation: 487.0610 Fuel Consumption: 143.6563\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -5078.487689561265 Explore P: 0.1808 SOC: 1.0000 Cumulative_SOC_deviation: 490.7026 Fuel Consumption: 171.4615\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -4972.800854400925 Explore P: 0.1761 SOC: 1.0000 Cumulative_SOC_deviation: 482.7028 Fuel Consumption: 145.7726\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -5019.809834638493 Explore P: 0.1716 SOC: 1.0000 Cumulative_SOC_deviation: 487.4433 Fuel Consumption: 145.3767\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -5042.750658158072 Explore P: 0.1673 SOC: 1.0000 Cumulative_SOC_deviation: 489.5267 Fuel Consumption: 147.4832\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -4916.016293085239 Explore P: 0.1630 SOC: 1.0000 Cumulative_SOC_deviation: 476.9867 Fuel Consumption: 146.1489\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -5072.049400141493 Explore P: 0.1589 SOC: 1.0000 Cumulative_SOC_deviation: 491.6678 Fuel Consumption: 155.3717\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -4779.079284052201 Explore P: 0.1548 SOC: 1.0000 Cumulative_SOC_deviation: 464.5824 Fuel Consumption: 133.2556\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -4952.70758819419 Explore P: 0.1509 SOC: 1.0000 Cumulative_SOC_deviation: 480.9535 Fuel Consumption: 143.1724\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -4786.989661079792 Explore P: 0.1471 SOC: 1.0000 Cumulative_SOC_deviation: 464.6437 Fuel Consumption: 140.5527\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -5096.890953812603 Explore P: 0.1434 SOC: 1.0000 Cumulative_SOC_deviation: 494.3567 Fuel Consumption: 153.3238\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -4924.405556115384 Explore P: 0.1398 SOC: 1.0000 Cumulative_SOC_deviation: 477.4395 Fuel Consumption: 150.0101\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -5144.5567124889285 Explore P: 0.1362 SOC: 1.0000 Cumulative_SOC_deviation: 499.2685 Fuel Consumption: 151.8722\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -5021.431580634371 Explore P: 0.1328 SOC: 1.0000 Cumulative_SOC_deviation: 488.2067 Fuel Consumption: 139.3650\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -5055.249761736593 Explore P: 0.1295 SOC: 1.0000 Cumulative_SOC_deviation: 492.1916 Fuel Consumption: 133.3338\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -4949.541709680439 Explore P: 0.1263 SOC: 1.0000 Cumulative_SOC_deviation: 480.5988 Fuel Consumption: 143.5536\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -5119.4897137759945 Explore P: 0.1231 SOC: 1.0000 Cumulative_SOC_deviation: 498.2119 Fuel Consumption: 137.3709\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -5111.546807487863 Explore P: 0.1200 SOC: 1.0000 Cumulative_SOC_deviation: 497.2074 Fuel Consumption: 139.4725\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -5019.455814196511 Explore P: 0.1171 SOC: 1.0000 Cumulative_SOC_deviation: 488.2926 Fuel Consumption: 136.5302\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -4996.495860870617 Explore P: 0.1142 SOC: 1.0000 Cumulative_SOC_deviation: 485.8763 Fuel Consumption: 137.7326\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -5033.648817888928 Explore P: 0.1113 SOC: 1.0000 Cumulative_SOC_deviation: 488.4206 Fuel Consumption: 149.4431\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -4914.769329492578 Explore P: 0.1086 SOC: 1.0000 Cumulative_SOC_deviation: 476.6079 Fuel Consumption: 148.6904\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -4985.1658839816555 Explore P: 0.1059 SOC: 1.0000 Cumulative_SOC_deviation: 483.5034 Fuel Consumption: 150.1322\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -5044.292814247531 Explore P: 0.1033 SOC: 1.0000 Cumulative_SOC_deviation: 489.1380 Fuel Consumption: 152.9133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -5037.040026774792 Explore P: 0.1008 SOC: 1.0000 Cumulative_SOC_deviation: 487.7436 Fuel Consumption: 159.6043\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -4906.38291441248 Explore P: 0.0983 SOC: 1.0000 Cumulative_SOC_deviation: 474.2908 Fuel Consumption: 163.4752\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -5024.634878410243 Explore P: 0.0960 SOC: 1.0000 Cumulative_SOC_deviation: 487.6766 Fuel Consumption: 147.8693\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -5087.264036806612 Explore P: 0.0936 SOC: 1.0000 Cumulative_SOC_deviation: 494.0460 Fuel Consumption: 146.8038\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -4820.563885250674 Explore P: 0.0914 SOC: 1.0000 Cumulative_SOC_deviation: 468.5280 Fuel Consumption: 135.2839\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -5038.964179125256 Explore P: 0.0892 SOC: 1.0000 Cumulative_SOC_deviation: 489.4741 Fuel Consumption: 144.2232\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -5009.635198381105 Explore P: 0.0870 SOC: 1.0000 Cumulative_SOC_deviation: 485.8442 Fuel Consumption: 151.1928\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -5011.028636030047 Explore P: 0.0849 SOC: 1.0000 Cumulative_SOC_deviation: 486.5843 Fuel Consumption: 145.1861\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -5092.725595539338 Explore P: 0.0829 SOC: 1.0000 Cumulative_SOC_deviation: 493.9827 Fuel Consumption: 152.8986\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -4968.980991510444 Explore P: 0.0809 SOC: 1.0000 Cumulative_SOC_deviation: 481.7426 Fuel Consumption: 151.5545\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -4861.9969371537245 Explore P: 0.0790 SOC: 1.0000 Cumulative_SOC_deviation: 471.4079 Fuel Consumption: 147.9182\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -5111.343125709649 Explore P: 0.0771 SOC: 1.0000 Cumulative_SOC_deviation: 494.5996 Fuel Consumption: 165.3472\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -4995.854366388507 Explore P: 0.0753 SOC: 1.0000 Cumulative_SOC_deviation: 485.3254 Fuel Consumption: 142.6005\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -5074.372747806217 Explore P: 0.0735 SOC: 1.0000 Cumulative_SOC_deviation: 491.5751 Fuel Consumption: 158.6219\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -5075.437971148063 Explore P: 0.0718 SOC: 1.0000 Cumulative_SOC_deviation: 492.4480 Fuel Consumption: 150.9582\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -5086.088678881127 Explore P: 0.0701 SOC: 1.0000 Cumulative_SOC_deviation: 492.3224 Fuel Consumption: 162.8643\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -5004.558048298034 Explore P: 0.0685 SOC: 1.0000 Cumulative_SOC_deviation: 485.0491 Fuel Consumption: 154.0667\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -4949.628093568103 Explore P: 0.0669 SOC: 1.0000 Cumulative_SOC_deviation: 480.5248 Fuel Consumption: 144.3796\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -5068.514397787476 Explore P: 0.0654 SOC: 1.0000 Cumulative_SOC_deviation: 492.6026 Fuel Consumption: 142.4881\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -5033.072684647396 Explore P: 0.0639 SOC: 1.0000 Cumulative_SOC_deviation: 488.6616 Fuel Consumption: 146.4568\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -5082.040234371524 Explore P: 0.0624 SOC: 1.0000 Cumulative_SOC_deviation: 492.8198 Fuel Consumption: 153.8419\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -4949.288714897042 Explore P: 0.0610 SOC: 1.0000 Cumulative_SOC_deviation: 478.9367 Fuel Consumption: 159.9220\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -5052.568582021518 Explore P: 0.0596 SOC: 1.0000 Cumulative_SOC_deviation: 489.5144 Fuel Consumption: 157.4245\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -5110.5971805528625 Explore P: 0.0583 SOC: 1.0000 Cumulative_SOC_deviation: 494.6394 Fuel Consumption: 164.2035\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -5112.533056833676 Explore P: 0.0570 SOC: 1.0000 Cumulative_SOC_deviation: 493.2176 Fuel Consumption: 180.3568\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -5142.358027803464 Explore P: 0.0557 SOC: 1.0000 Cumulative_SOC_deviation: 497.7900 Fuel Consumption: 164.4576\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -5098.419198711278 Explore P: 0.0545 SOC: 1.0000 Cumulative_SOC_deviation: 494.0320 Fuel Consumption: 158.0989\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -5080.271679100742 Explore P: 0.0533 SOC: 1.0000 Cumulative_SOC_deviation: 492.4191 Fuel Consumption: 156.0804\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -4878.634216138992 Explore P: 0.0521 SOC: 1.0000 Cumulative_SOC_deviation: 472.5071 Fuel Consumption: 153.5633\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -5093.59833030113 Explore P: 0.0510 SOC: 1.0000 Cumulative_SOC_deviation: 493.8603 Fuel Consumption: 154.9953\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -5012.517127896585 Explore P: 0.0498 SOC: 1.0000 Cumulative_SOC_deviation: 485.2199 Fuel Consumption: 160.3179\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -5066.75576251558 Explore P: 0.0488 SOC: 1.0000 Cumulative_SOC_deviation: 490.9106 Fuel Consumption: 157.6493\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -5047.715318293667 Explore P: 0.0477 SOC: 1.0000 Cumulative_SOC_deviation: 488.2270 Fuel Consumption: 165.4449\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -5124.493777703227 Explore P: 0.0467 SOC: 1.0000 Cumulative_SOC_deviation: 495.9723 Fuel Consumption: 164.7704\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -4948.469574615759 Explore P: 0.0457 SOC: 1.0000 Cumulative_SOC_deviation: 479.4501 Fuel Consumption: 153.9690\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -5061.11124592957 Explore P: 0.0447 SOC: 1.0000 Cumulative_SOC_deviation: 489.7548 Fuel Consumption: 163.5632\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -5081.884144470683 Explore P: 0.0438 SOC: 1.0000 Cumulative_SOC_deviation: 491.9030 Fuel Consumption: 162.8545\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -5048.213230461237 Explore P: 0.0429 SOC: 1.0000 Cumulative_SOC_deviation: 490.0129 Fuel Consumption: 148.0844\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -4952.370886038881 Explore P: 0.0420 SOC: 1.0000 Cumulative_SOC_deviation: 481.2825 Fuel Consumption: 139.5458\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -5042.245779821716 Explore P: 0.0411 SOC: 1.0000 Cumulative_SOC_deviation: 488.3555 Fuel Consumption: 158.6903\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -5175.936007659248 Explore P: 0.0403 SOC: 1.0000 Cumulative_SOC_deviation: 502.9162 Fuel Consumption: 146.7745\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -4851.368923455036 Explore P: 0.0395 SOC: 1.0000 Cumulative_SOC_deviation: 471.6422 Fuel Consumption: 134.9467\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -4930.37961537921 Explore P: 0.0387 SOC: 1.0000 Cumulative_SOC_deviation: 477.1978 Fuel Consumption: 158.4020\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -4987.226877411794 Explore P: 0.0379 SOC: 1.0000 Cumulative_SOC_deviation: 485.0961 Fuel Consumption: 136.2663\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -5128.5159468093125 Explore P: 0.0371 SOC: 1.0000 Cumulative_SOC_deviation: 495.4733 Fuel Consumption: 173.7830\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -5043.577607891703 Explore P: 0.0364 SOC: 1.0000 Cumulative_SOC_deviation: 488.6886 Fuel Consumption: 156.6913\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -4929.72511457602 Explore P: 0.0357 SOC: 1.0000 Cumulative_SOC_deviation: 477.3674 Fuel Consumption: 156.0511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -5082.937517046454 Explore P: 0.0350 SOC: 1.0000 Cumulative_SOC_deviation: 491.4076 Fuel Consumption: 168.8613\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -5118.809341557518 Explore P: 0.0343 SOC: 1.0000 Cumulative_SOC_deviation: 496.0505 Fuel Consumption: 158.3042\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -5164.736144652173 Explore P: 0.0336 SOC: 1.0000 Cumulative_SOC_deviation: 500.4907 Fuel Consumption: 159.8291\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -5137.846932720027 Explore P: 0.0330 SOC: 1.0000 Cumulative_SOC_deviation: 497.2734 Fuel Consumption: 165.1126\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -5141.713362535662 Explore P: 0.0324 SOC: 1.0000 Cumulative_SOC_deviation: 498.9460 Fuel Consumption: 152.2534\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -5069.542579994743 Explore P: 0.0318 SOC: 1.0000 Cumulative_SOC_deviation: 490.7509 Fuel Consumption: 162.0334\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -5141.216763246512 Explore P: 0.0312 SOC: 1.0000 Cumulative_SOC_deviation: 499.1251 Fuel Consumption: 149.9661\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -5189.216896989391 Explore P: 0.0306 SOC: 1.0000 Cumulative_SOC_deviation: 503.2584 Fuel Consumption: 156.6327\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -5117.371689273617 Explore P: 0.0301 SOC: 1.0000 Cumulative_SOC_deviation: 495.7499 Fuel Consumption: 159.8731\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -5036.784421054419 Explore P: 0.0295 SOC: 1.0000 Cumulative_SOC_deviation: 488.8123 Fuel Consumption: 148.6611\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -4635.929723409748 Explore P: 0.0290 SOC: 1.0000 Cumulative_SOC_deviation: 449.0211 Fuel Consumption: 145.7188\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -4997.693930276 Explore P: 0.0285 SOC: 1.0000 Cumulative_SOC_deviation: 484.8803 Fuel Consumption: 148.8908\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -4961.927921708846 Explore P: 0.0280 SOC: 1.0000 Cumulative_SOC_deviation: 482.3726 Fuel Consumption: 138.2018\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -5093.527500282894 Explore P: 0.0275 SOC: 1.0000 Cumulative_SOC_deviation: 492.7090 Fuel Consumption: 166.4371\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -5096.230290384484 Explore P: 0.0270 SOC: 1.0000 Cumulative_SOC_deviation: 492.6416 Fuel Consumption: 169.8144\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -5056.823429129216 Explore P: 0.0265 SOC: 1.0000 Cumulative_SOC_deviation: 489.2068 Fuel Consumption: 164.7558\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -5064.170303361521 Explore P: 0.0261 SOC: 1.0000 Cumulative_SOC_deviation: 490.1594 Fuel Consumption: 162.5759\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -4899.818784506334 Explore P: 0.0257 SOC: 1.0000 Cumulative_SOC_deviation: 473.9457 Fuel Consumption: 160.3619\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -4969.085927851358 Explore P: 0.0252 SOC: 1.0000 Cumulative_SOC_deviation: 481.0743 Fuel Consumption: 158.3433\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -4960.319769071593 Explore P: 0.0248 SOC: 1.0000 Cumulative_SOC_deviation: 482.2142 Fuel Consumption: 138.1773\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -4379.41561778782 Explore P: 0.0244 SOC: 1.0000 Cumulative_SOC_deviation: 425.2353 Fuel Consumption: 127.0631\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -5060.077598263499 Explore P: 0.0240 SOC: 1.0000 Cumulative_SOC_deviation: 490.3000 Fuel Consumption: 157.0774\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -4943.435016293197 Explore P: 0.0237 SOC: 1.0000 Cumulative_SOC_deviation: 478.7374 Fuel Consumption: 156.0608\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -4747.57676299526 Explore P: 0.0233 SOC: 1.0000 Cumulative_SOC_deviation: 460.6452 Fuel Consumption: 141.1245\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -5041.193837422315 Explore P: 0.0229 SOC: 1.0000 Cumulative_SOC_deviation: 488.3867 Fuel Consumption: 157.3267\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -4681.951785157802 Explore P: 0.0226 SOC: 1.0000 Cumulative_SOC_deviation: 455.6008 Fuel Consumption: 125.9438\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -4546.994340146111 Explore P: 0.0222 SOC: 1.0000 Cumulative_SOC_deviation: 443.1402 Fuel Consumption: 115.5920\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -4665.303358663647 Explore P: 0.0219 SOC: 1.0000 Cumulative_SOC_deviation: 453.9213 Fuel Consumption: 126.0904\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -5170.422061765152 Explore P: 0.0216 SOC: 1.0000 Cumulative_SOC_deviation: 500.4518 Fuel Consumption: 165.9043\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -4245.953406364222 Explore P: 0.0213 SOC: 1.0000 Cumulative_SOC_deviation: 414.0650 Fuel Consumption: 105.3037\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -4583.190471348416 Explore P: 0.0210 SOC: 1.0000 Cumulative_SOC_deviation: 447.1377 Fuel Consumption: 111.8139\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -5038.669706052566 Explore P: 0.0207 SOC: 1.0000 Cumulative_SOC_deviation: 489.4837 Fuel Consumption: 143.8322\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -4483.851396648211 Explore P: 0.0204 SOC: 1.0000 Cumulative_SOC_deviation: 436.3650 Fuel Consumption: 120.2010\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -4753.09072336688 Explore P: 0.0201 SOC: 1.0000 Cumulative_SOC_deviation: 461.7294 Fuel Consumption: 135.7971\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -4536.993023125859 Explore P: 0.0198 SOC: 1.0000 Cumulative_SOC_deviation: 441.7838 Fuel Consumption: 119.1550\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -4554.809645361803 Explore P: 0.0196 SOC: 1.0000 Cumulative_SOC_deviation: 443.1490 Fuel Consumption: 123.3192\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -4530.913158699001 Explore P: 0.0193 SOC: 1.0000 Cumulative_SOC_deviation: 441.9133 Fuel Consumption: 111.7797\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -4589.067605699954 Explore P: 0.0190 SOC: 1.0000 Cumulative_SOC_deviation: 447.3647 Fuel Consumption: 115.4209\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -4437.871592725609 Explore P: 0.0188 SOC: 1.0000 Cumulative_SOC_deviation: 433.2050 Fuel Consumption: 105.8218\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -4928.918194472959 Explore P: 0.0186 SOC: 1.0000 Cumulative_SOC_deviation: 479.1933 Fuel Consumption: 136.9848\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -5116.920996195434 Explore P: 0.0183 SOC: 1.0000 Cumulative_SOC_deviation: 495.5308 Fuel Consumption: 161.6131\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -4532.338862267263 Explore P: 0.0181 SOC: 1.0000 Cumulative_SOC_deviation: 441.6253 Fuel Consumption: 116.0856\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -4735.53831735722 Explore P: 0.0179 SOC: 1.0000 Cumulative_SOC_deviation: 461.5904 Fuel Consumption: 119.6340\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -3088.4713497928424 Explore P: 0.0177 SOC: 0.9983 Cumulative_SOC_deviation: 298.6413 Fuel Consumption: 102.0584\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -2307.833720818918 Explore P: 0.0175 SOC: 0.5861 Cumulative_SOC_deviation: 225.5985 Fuel Consumption: 51.8487\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -1783.140400238927 Explore P: 0.0173 SOC: 0.9870 Cumulative_SOC_deviation: 169.9342 Fuel Consumption: 83.7986\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -2527.3484495768757 Explore P: 0.0171 SOC: 1.0000 Cumulative_SOC_deviation: 244.5783 Fuel Consumption: 81.5650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -4130.947250553431 Explore P: 0.0169 SOC: 1.0000 Cumulative_SOC_deviation: 403.6948 Fuel Consumption: 93.9989\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -1816.4770839264195 Explore P: 0.0167 SOC: 1.0000 Cumulative_SOC_deviation: 173.7493 Fuel Consumption: 78.9843\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -2278.9089194429234 Explore P: 0.0165 SOC: 1.0000 Cumulative_SOC_deviation: 219.6762 Fuel Consumption: 82.1466\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -4400.397769087941 Explore P: 0.0163 SOC: 1.0000 Cumulative_SOC_deviation: 430.4297 Fuel Consumption: 96.1005\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -2631.433743127408 Explore P: 0.0162 SOC: 1.0000 Cumulative_SOC_deviation: 255.2151 Fuel Consumption: 79.2825\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -4229.069356408228 Explore P: 0.0160 SOC: 1.0000 Cumulative_SOC_deviation: 413.7099 Fuel Consumption: 91.9705\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -4369.980173177442 Explore P: 0.0158 SOC: 1.0000 Cumulative_SOC_deviation: 427.6426 Fuel Consumption: 93.5541\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -3661.7445757719993 Explore P: 0.0157 SOC: 1.0000 Cumulative_SOC_deviation: 356.4735 Fuel Consumption: 97.0096\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -2197.6319397461825 Explore P: 0.0155 SOC: 0.9982 Cumulative_SOC_deviation: 211.9425 Fuel Consumption: 78.2072\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -4283.8193033051075 Explore P: 0.0154 SOC: 1.0000 Cumulative_SOC_deviation: 418.1981 Fuel Consumption: 101.8385\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -2179.5972286980755 Explore P: 0.0152 SOC: 1.0000 Cumulative_SOC_deviation: 209.6586 Fuel Consumption: 83.0117\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -4733.235536542617 Explore P: 0.0151 SOC: 1.0000 Cumulative_SOC_deviation: 463.0781 Fuel Consumption: 102.4543\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -3670.6097971607323 Explore P: 0.0149 SOC: 1.0000 Cumulative_SOC_deviation: 357.9671 Fuel Consumption: 90.9393\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -4005.2980719730567 Explore P: 0.0148 SOC: 1.0000 Cumulative_SOC_deviation: 390.5889 Fuel Consumption: 99.4094\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -4382.0781904983005 Explore P: 0.0147 SOC: 1.0000 Cumulative_SOC_deviation: 428.6672 Fuel Consumption: 95.4065\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -4758.57836480609 Explore P: 0.0146 SOC: 1.0000 Cumulative_SOC_deviation: 463.9721 Fuel Consumption: 118.8569\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -4512.1502723487765 Explore P: 0.0144 SOC: 1.0000 Cumulative_SOC_deviation: 439.7467 Fuel Consumption: 114.6829\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -4726.220089893641 Explore P: 0.0143 SOC: 1.0000 Cumulative_SOC_deviation: 460.6244 Fuel Consumption: 119.9761\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -4687.213301003626 Explore P: 0.0142 SOC: 1.0000 Cumulative_SOC_deviation: 455.9906 Fuel Consumption: 127.3074\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -4997.812767140026 Explore P: 0.0141 SOC: 1.0000 Cumulative_SOC_deviation: 486.3409 Fuel Consumption: 134.4041\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {i}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 10))\n",
    "# for size, history in results_dict.items(): \n",
    "#     plt.plot(history, label=size, linewidth=3.0) \n",
    "\n",
    "\n",
    "# plt.grid() \n",
    "# plt.legend(fontsize=20)\n",
    "# plt.xlabel(\"episode number\", fontsize=20) \n",
    "# plt.ylabel(\"total rewards\", fontsize=20) \n",
    "# plt.xlim([0, 120])\n",
    "\n",
    "\n",
    "# plt.savefig(\"replay_memory_size_effect_300.png\")\n",
    "with open(\"results_notScaling_mass1200.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

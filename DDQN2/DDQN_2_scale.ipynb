{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN_scaling import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 1000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 2\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 1 Total reward: -5127.709039023458 Explore P: 0.9732 SOC: 1.0000 Cumulative_SOC_deviation: 497.7743 Fuel Consumption: 149.9661\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 2 Total reward: -5173.777274304419 Explore P: 0.9471 SOC: 1.0000 Cumulative_SOC_deviation: 502.3210 Fuel Consumption: 150.5672\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -5027.937156833145 Explore P: 0.9217 SOC: 1.0000 Cumulative_SOC_deviation: 488.0517 Fuel Consumption: 147.4197\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -5054.428286577509 Explore P: 0.8970 SOC: 1.0000 Cumulative_SOC_deviation: 490.8890 Fuel Consumption: 145.5380\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -5017.483651425362 Explore P: 0.8730 SOC: 1.0000 Cumulative_SOC_deviation: 487.6911 Fuel Consumption: 140.5722\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -4992.707800438706 Explore P: 0.8496 SOC: 1.0000 Cumulative_SOC_deviation: 485.0044 Fuel Consumption: 142.6641\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -5049.037296952142 Explore P: 0.8269 SOC: 1.0000 Cumulative_SOC_deviation: 490.6261 Fuel Consumption: 142.7765\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -5012.329983102846 Explore P: 0.8048 SOC: 1.0000 Cumulative_SOC_deviation: 487.2941 Fuel Consumption: 139.3894\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -5037.338942835623 Explore P: 0.7832 SOC: 1.0000 Cumulative_SOC_deviation: 489.4318 Fuel Consumption: 143.0209\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -5054.122256421923 Explore P: 0.7623 SOC: 1.0000 Cumulative_SOC_deviation: 491.5226 Fuel Consumption: 138.8958\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -5021.010724397815 Explore P: 0.7419 SOC: 1.0000 Cumulative_SOC_deviation: 487.9886 Fuel Consumption: 141.1245\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -5034.319897995036 Explore P: 0.7221 SOC: 1.0000 Cumulative_SOC_deviation: 489.4510 Fuel Consumption: 139.8098\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -5024.910921252693 Explore P: 0.7028 SOC: 1.0000 Cumulative_SOC_deviation: 488.9158 Fuel Consumption: 135.7531\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -5022.343023241394 Explore P: 0.6840 SOC: 1.0000 Cumulative_SOC_deviation: 488.1380 Fuel Consumption: 140.9632\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -4968.410367355787 Explore P: 0.6658 SOC: 1.0000 Cumulative_SOC_deviation: 483.0487 Fuel Consumption: 137.9232\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -4885.079351842171 Explore P: 0.6480 SOC: 1.0000 Cumulative_SOC_deviation: 474.8808 Fuel Consumption: 136.2712\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -5032.497837853279 Explore P: 0.6307 SOC: 1.0000 Cumulative_SOC_deviation: 489.3050 Fuel Consumption: 139.4481\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -4816.30563300602 Explore P: 0.6139 SOC: 1.0000 Cumulative_SOC_deviation: 467.6682 Fuel Consumption: 139.6240\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -4823.969025059943 Explore P: 0.5976 SOC: 1.0000 Cumulative_SOC_deviation: 468.3988 Fuel Consumption: 139.9808\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -4936.497680016224 Explore P: 0.5816 SOC: 1.0000 Cumulative_SOC_deviation: 480.0720 Fuel Consumption: 135.7775\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -4953.4101157505365 Explore P: 0.5662 SOC: 1.0000 Cumulative_SOC_deviation: 481.3180 Fuel Consumption: 140.2301\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -4722.4629748864445 Explore P: 0.5511 SOC: 1.0000 Cumulative_SOC_deviation: 458.1338 Fuel Consumption: 141.1245\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -4694.12373021402 Explore P: 0.5364 SOC: 1.0000 Cumulative_SOC_deviation: 456.2139 Fuel Consumption: 131.9848\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -4710.183910724146 Explore P: 0.5222 SOC: 1.0000 Cumulative_SOC_deviation: 458.3541 Fuel Consumption: 126.6427\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -4658.3030974297635 Explore P: 0.5083 SOC: 1.0000 Cumulative_SOC_deviation: 451.9881 Fuel Consumption: 138.4217\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -4849.451898190272 Explore P: 0.4948 SOC: 1.0000 Cumulative_SOC_deviation: 471.0820 Fuel Consumption: 138.6319\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -4596.045039895583 Explore P: 0.4817 SOC: 1.0000 Cumulative_SOC_deviation: 447.0668 Fuel Consumption: 125.3769\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -4386.3970074197105 Explore P: 0.4689 SOC: 1.0000 Cumulative_SOC_deviation: 425.1343 Fuel Consumption: 135.0542\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -4506.509089542041 Explore P: 0.4565 SOC: 1.0000 Cumulative_SOC_deviation: 438.4666 Fuel Consumption: 121.8432\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -4255.284037980401 Explore P: 0.4444 SOC: 1.0000 Cumulative_SOC_deviation: 414.6828 Fuel Consumption: 108.4562\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -3954.5464740675675 Explore P: 0.4326 SOC: 1.0000 Cumulative_SOC_deviation: 385.1711 Fuel Consumption: 102.8355\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -4431.515687034374 Explore P: 0.4212 SOC: 1.0000 Cumulative_SOC_deviation: 431.9154 Fuel Consumption: 112.3613\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -4090.0909578880082 Explore P: 0.4100 SOC: 1.0000 Cumulative_SOC_deviation: 398.2451 Fuel Consumption: 107.6400\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -4389.011421729004 Explore P: 0.3992 SOC: 1.0000 Cumulative_SOC_deviation: 428.4407 Fuel Consumption: 104.6048\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -3786.624845503916 Explore P: 0.3887 SOC: 1.0000 Cumulative_SOC_deviation: 368.8100 Fuel Consumption: 98.5247\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -4068.1536230230013 Explore P: 0.3784 SOC: 1.0000 Cumulative_SOC_deviation: 396.8666 Fuel Consumption: 99.4876\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -3983.224354059579 Explore P: 0.3684 SOC: 1.0000 Cumulative_SOC_deviation: 387.4724 Fuel Consumption: 108.5002\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -4136.813340017944 Explore P: 0.3587 SOC: 1.0000 Cumulative_SOC_deviation: 403.6084 Fuel Consumption: 100.7290\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -4133.731621333715 Explore P: 0.3493 SOC: 1.0000 Cumulative_SOC_deviation: 403.0549 Fuel Consumption: 103.1825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -3952.208656424482 Explore P: 0.3401 SOC: 1.0000 Cumulative_SOC_deviation: 384.7149 Fuel Consumption: 105.0594\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -3586.2822314834953 Explore P: 0.3311 SOC: 1.0000 Cumulative_SOC_deviation: 348.8735 Fuel Consumption: 97.5472\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -2442.735886655316 Explore P: 0.3224 SOC: 1.0000 Cumulative_SOC_deviation: 235.1249 Fuel Consumption: 91.4867\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -3159.791110757561 Explore P: 0.3140 SOC: 1.0000 Cumulative_SOC_deviation: 306.0866 Fuel Consumption: 98.9255\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -3757.1421478081497 Explore P: 0.3057 SOC: 1.0000 Cumulative_SOC_deviation: 364.4668 Fuel Consumption: 112.4738\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -2494.7169141801273 Explore P: 0.2977 SOC: 1.0000 Cumulative_SOC_deviation: 239.5747 Fuel Consumption: 98.9695\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -1913.3967251663375 Explore P: 0.2899 SOC: 1.0000 Cumulative_SOC_deviation: 182.5889 Fuel Consumption: 87.5082\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -3550.842281408769 Explore P: 0.2824 SOC: 1.0000 Cumulative_SOC_deviation: 343.5563 Fuel Consumption: 115.2792\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -4159.401894727879 Explore P: 0.2750 SOC: 1.0000 Cumulative_SOC_deviation: 404.7510 Fuel Consumption: 111.8921\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -2595.7794687880705 Explore P: 0.2678 SOC: 1.0000 Cumulative_SOC_deviation: 250.5480 Fuel Consumption: 90.2990\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -3439.999713380739 Explore P: 0.2608 SOC: 1.0000 Cumulative_SOC_deviation: 334.2716 Fuel Consumption: 97.2833\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -3644.590575679137 Explore P: 0.2540 SOC: 1.0000 Cumulative_SOC_deviation: 354.8651 Fuel Consumption: 95.9392\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -3310.9711456317896 Explore P: 0.2474 SOC: 1.0000 Cumulative_SOC_deviation: 321.4685 Fuel Consumption: 96.2862\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -2090.207030626828 Explore P: 0.2410 SOC: 1.0000 Cumulative_SOC_deviation: 200.2552 Fuel Consumption: 87.6548\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -2179.732911302574 Explore P: 0.2347 SOC: 1.0000 Cumulative_SOC_deviation: 209.4429 Fuel Consumption: 85.3039\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -2186.4848359931184 Explore P: 0.2286 SOC: 1.0000 Cumulative_SOC_deviation: 209.6073 Fuel Consumption: 90.4114\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -1046.1899489376535 Explore P: 0.2227 SOC: 0.8509 Cumulative_SOC_deviation: 97.5920 Fuel Consumption: 70.2699\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -1961.6475579726166 Explore P: 0.2170 SOC: 1.0000 Cumulative_SOC_deviation: 187.9134 Fuel Consumption: 82.5131\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -1993.1821930881847 Explore P: 0.2114 SOC: 1.0000 Cumulative_SOC_deviation: 191.2365 Fuel Consumption: 80.8172\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -2531.2455818031167 Explore P: 0.2059 SOC: 1.0000 Cumulative_SOC_deviation: 243.7261 Fuel Consumption: 93.9842\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -3293.825960763739 Explore P: 0.2006 SOC: 1.0000 Cumulative_SOC_deviation: 319.1772 Fuel Consumption: 102.0535\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -2403.4813014962533 Explore P: 0.1954 SOC: 1.0000 Cumulative_SOC_deviation: 231.5406 Fuel Consumption: 88.0752\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -2542.0314166969138 Explore P: 0.1904 SOC: 1.0000 Cumulative_SOC_deviation: 244.6038 Fuel Consumption: 95.9930\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -2241.3185005163655 Explore P: 0.1855 SOC: 1.0000 Cumulative_SOC_deviation: 214.0213 Fuel Consumption: 101.1053\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -1777.2310782396055 Explore P: 0.1808 SOC: 0.9721 Cumulative_SOC_deviation: 169.8946 Fuel Consumption: 78.2854\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -898.4196555319118 Explore P: 0.1761 SOC: 0.7794 Cumulative_SOC_deviation: 83.4196 Fuel Consumption: 64.2240\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -1108.0248123380436 Explore P: 0.1716 SOC: 0.8860 Cumulative_SOC_deviation: 103.5448 Fuel Consumption: 72.5768\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -514.6859215136085 Explore P: 0.1673 SOC: 0.7356 Cumulative_SOC_deviation: 45.2828 Fuel Consumption: 61.8584\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -1190.562825980396 Explore P: 0.1630 SOC: 0.8788 Cumulative_SOC_deviation: 111.8182 Fuel Consumption: 72.3813\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -387.64743156809743 Explore P: 0.1589 SOC: 0.7044 Cumulative_SOC_deviation: 32.8800 Fuel Consumption: 58.8477\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -590.9760146831271 Explore P: 0.1548 SOC: 0.8019 Cumulative_SOC_deviation: 52.4704 Fuel Consumption: 66.2718\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -712.4448252685087 Explore P: 0.1509 SOC: 0.7697 Cumulative_SOC_deviation: 64.8778 Fuel Consumption: 63.6668\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -1627.5894639627236 Explore P: 0.1471 SOC: 1.0000 Cumulative_SOC_deviation: 152.5668 Fuel Consumption: 101.9216\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -476.32002141317673 Explore P: 0.1434 SOC: 0.6940 Cumulative_SOC_deviation: 41.9017 Fuel Consumption: 57.3032\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -968.4727603357195 Explore P: 0.1398 SOC: 0.9484 Cumulative_SOC_deviation: 89.1160 Fuel Consumption: 77.3128\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -1660.2587517531394 Explore P: 0.1362 SOC: 1.0000 Cumulative_SOC_deviation: 157.9867 Fuel Consumption: 80.3919\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -2840.311797593266 Explore P: 0.1328 SOC: 1.0000 Cumulative_SOC_deviation: 271.4407 Fuel Consumption: 125.9047\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -728.7408254735269 Explore P: 0.1295 SOC: 0.7610 Cumulative_SOC_deviation: 66.6154 Fuel Consumption: 62.5866\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -606.4848488296516 Explore P: 0.1263 SOC: 0.7627 Cumulative_SOC_deviation: 54.3199 Fuel Consumption: 63.2856\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -1102.9435134412313 Explore P: 0.1231 SOC: 1.0000 Cumulative_SOC_deviation: 102.1276 Fuel Consumption: 81.6676\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -311.42407634169695 Explore P: 0.1200 SOC: 0.6470 Cumulative_SOC_deviation: 25.5729 Fuel Consumption: 55.6952\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -249.57734514030992 Explore P: 0.1171 SOC: 0.5984 Cumulative_SOC_deviation: 19.7269 Fuel Consumption: 52.3081\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -341.4109177183686 Explore P: 0.1142 SOC: 0.6514 Cumulative_SOC_deviation: 28.5462 Fuel Consumption: 55.9494\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -396.44609691962904 Explore P: 0.1113 SOC: 0.6127 Cumulative_SOC_deviation: 34.2090 Fuel Consumption: 54.3560\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -562.8646573774421 Explore P: 0.1086 SOC: 0.7556 Cumulative_SOC_deviation: 50.0659 Fuel Consumption: 62.2054\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -298.6934081370715 Explore P: 0.1059 SOC: 0.6432 Cumulative_SOC_deviation: 24.5447 Fuel Consumption: 53.2466\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -314.34768263296513 Explore P: 0.1033 SOC: 0.6138 Cumulative_SOC_deviation: 26.1086 Fuel Consumption: 53.2612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -410.2390985058813 Explore P: 0.1008 SOC: 0.5914 Cumulative_SOC_deviation: 35.7403 Fuel Consumption: 52.8360\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -583.0314640427656 Explore P: 0.0983 SOC: 0.5802 Cumulative_SOC_deviation: 53.2214 Fuel Consumption: 50.8174\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -523.8302450175831 Explore P: 0.0960 SOC: 0.5826 Cumulative_SOC_deviation: 47.3057 Fuel Consumption: 50.7735\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -592.6746424487143 Explore P: 0.0936 SOC: 0.5816 Cumulative_SOC_deviation: 54.3617 Fuel Consumption: 49.0579\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -561.4312398459833 Explore P: 0.0914 SOC: 0.5846 Cumulative_SOC_deviation: 51.1283 Fuel Consumption: 50.1479\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -543.2533523340159 Explore P: 0.0892 SOC: 0.5709 Cumulative_SOC_deviation: 49.4210 Fuel Consumption: 49.0433\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -580.499549810224 Explore P: 0.0870 SOC: 0.5710 Cumulative_SOC_deviation: 53.0577 Fuel Consumption: 49.9230\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -676.7099338021935 Explore P: 0.0849 SOC: 0.5575 Cumulative_SOC_deviation: 62.7618 Fuel Consumption: 49.0921\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -644.5909613428632 Explore P: 0.0829 SOC: 0.5589 Cumulative_SOC_deviation: 59.5382 Fuel Consumption: 49.2094\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -716.7501968076053 Explore P: 0.0809 SOC: 0.5615 Cumulative_SOC_deviation: 66.7912 Fuel Consumption: 48.8380\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -728.6084130312537 Explore P: 0.0790 SOC: 0.5739 Cumulative_SOC_deviation: 67.8421 Fuel Consumption: 50.1870\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -700.2144563256418 Explore P: 0.0771 SOC: 0.5515 Cumulative_SOC_deviation: 65.0834 Fuel Consumption: 49.3805\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -772.3694914383353 Explore P: 0.0753 SOC: 0.5486 Cumulative_SOC_deviation: 72.2148 Fuel Consumption: 50.2212\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -818.0179487950686 Explore P: 0.0735 SOC: 0.5445 Cumulative_SOC_deviation: 77.0123 Fuel Consumption: 47.8947\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -792.0334952165164 Explore P: 0.0718 SOC: 0.5637 Cumulative_SOC_deviation: 74.1920 Fuel Consumption: 50.1136\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -817.7390980186692 Explore P: 0.0701 SOC: 0.5477 Cumulative_SOC_deviation: 76.7523 Fuel Consumption: 50.2163\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -845.4171502512111 Explore P: 0.0685 SOC: 0.5403 Cumulative_SOC_deviation: 79.7009 Fuel Consumption: 48.4079\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -822.8575490196793 Explore P: 0.0669 SOC: 0.5513 Cumulative_SOC_deviation: 77.2754 Fuel Consumption: 50.1039\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -893.8441962065742 Explore P: 0.0654 SOC: 0.5423 Cumulative_SOC_deviation: 84.5519 Fuel Consumption: 48.3248\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -944.6008547590338 Explore P: 0.0639 SOC: 0.5411 Cumulative_SOC_deviation: 89.5841 Fuel Consumption: 48.7598\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -941.8887490222061 Explore P: 0.0624 SOC: 0.5484 Cumulative_SOC_deviation: 89.3012 Fuel Consumption: 48.8771\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -923.8074068398489 Explore P: 0.0610 SOC: 0.5406 Cumulative_SOC_deviation: 87.6294 Fuel Consumption: 47.5135\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -913.5975216081868 Explore P: 0.0596 SOC: 0.5495 Cumulative_SOC_deviation: 86.5483 Fuel Consumption: 48.1146\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -909.7924386099608 Explore P: 0.0583 SOC: 0.5334 Cumulative_SOC_deviation: 86.1658 Fuel Consumption: 48.1342\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -920.1727726601268 Explore P: 0.0570 SOC: 0.5379 Cumulative_SOC_deviation: 87.0426 Fuel Consumption: 49.7471\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -914.2980186068963 Explore P: 0.0557 SOC: 0.5384 Cumulative_SOC_deviation: 86.6164 Fuel Consumption: 48.1342\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -894.825148450303 Explore P: 0.0545 SOC: 0.5446 Cumulative_SOC_deviation: 84.5406 Fuel Consumption: 49.4196\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -921.0042962755018 Explore P: 0.0533 SOC: 0.5414 Cumulative_SOC_deviation: 87.2567 Fuel Consumption: 48.4372\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -1016.7666503104574 Explore P: 0.0521 SOC: 0.5370 Cumulative_SOC_deviation: 96.7308 Fuel Consumption: 49.4587\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -890.5153138587827 Explore P: 0.0510 SOC: 0.5405 Cumulative_SOC_deviation: 84.2147 Fuel Consumption: 48.3688\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -901.9202911102824 Explore P: 0.0498 SOC: 0.5403 Cumulative_SOC_deviation: 85.4055 Fuel Consumption: 47.8654\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -921.5796432958191 Explore P: 0.0488 SOC: 0.5377 Cumulative_SOC_deviation: 87.3186 Fuel Consumption: 48.3932\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -913.5125672671269 Explore P: 0.0477 SOC: 0.5334 Cumulative_SOC_deviation: 86.6752 Fuel Consumption: 46.7608\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -897.168649803207 Explore P: 0.0467 SOC: 0.5414 Cumulative_SOC_deviation: 84.7769 Fuel Consumption: 49.4001\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -840.3527303121214 Explore P: 0.0457 SOC: 0.5461 Cumulative_SOC_deviation: 79.2785 Fuel Consumption: 47.5672\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -797.5710352637732 Explore P: 0.0447 SOC: 0.5454 Cumulative_SOC_deviation: 75.0805 Fuel Consumption: 46.7657\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -821.2405680794654 Explore P: 0.0438 SOC: 0.5421 Cumulative_SOC_deviation: 77.3791 Fuel Consumption: 47.4499\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -822.2618411936577 Explore P: 0.0429 SOC: 0.5489 Cumulative_SOC_deviation: 77.3619 Fuel Consumption: 48.6425\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -894.7659058465907 Explore P: 0.0420 SOC: 0.5434 Cumulative_SOC_deviation: 84.7370 Fuel Consumption: 47.3962\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -866.4815669843169 Explore P: 0.0411 SOC: 0.5450 Cumulative_SOC_deviation: 82.0102 Fuel Consumption: 46.3796\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -831.8794021108565 Explore P: 0.0403 SOC: 0.5482 Cumulative_SOC_deviation: 78.4928 Fuel Consumption: 46.9514\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -757.4897656604705 Explore P: 0.0395 SOC: 0.5517 Cumulative_SOC_deviation: 70.8710 Fuel Consumption: 48.7793\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -738.2781458622331 Explore P: 0.0387 SOC: 0.5470 Cumulative_SOC_deviation: 68.9054 Fuel Consumption: 49.2241\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -731.5911920171533 Explore P: 0.0379 SOC: 0.5493 Cumulative_SOC_deviation: 68.1605 Fuel Consumption: 49.9866\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -755.048067871282 Explore P: 0.0371 SOC: 0.5517 Cumulative_SOC_deviation: 70.5795 Fuel Consumption: 49.2534\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -792.8190847675075 Explore P: 0.0364 SOC: 0.5490 Cumulative_SOC_deviation: 74.4015 Fuel Consumption: 48.8038\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -738.4193666291898 Explore P: 0.0357 SOC: 0.5519 Cumulative_SOC_deviation: 68.9464 Fuel Consumption: 48.9553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -782.6645055151049 Explore P: 0.0350 SOC: 0.5546 Cumulative_SOC_deviation: 73.4276 Fuel Consumption: 48.3883\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -709.825312698526 Explore P: 0.0343 SOC: 0.5483 Cumulative_SOC_deviation: 66.2155 Fuel Consumption: 47.6699\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -759.5285026201933 Explore P: 0.0336 SOC: 0.5534 Cumulative_SOC_deviation: 71.1634 Fuel Consumption: 47.8947\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -792.3778560870995 Explore P: 0.0330 SOC: 0.5534 Cumulative_SOC_deviation: 74.4038 Fuel Consumption: 48.3395\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -772.6062921109019 Explore P: 0.0324 SOC: 0.5511 Cumulative_SOC_deviation: 72.5039 Fuel Consumption: 47.5672\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -770.4290836572862 Explore P: 0.0318 SOC: 0.5499 Cumulative_SOC_deviation: 72.1684 Fuel Consumption: 48.7451\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -802.1071914763089 Explore P: 0.0312 SOC: 0.5568 Cumulative_SOC_deviation: 75.4384 Fuel Consumption: 47.7236\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -789.8535132554109 Explore P: 0.0306 SOC: 0.5498 Cumulative_SOC_deviation: 74.2389 Fuel Consumption: 47.4646\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -779.4986980959978 Explore P: 0.0301 SOC: 0.5518 Cumulative_SOC_deviation: 73.2234 Fuel Consumption: 47.2642\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -765.1769904320616 Explore P: 0.0295 SOC: 0.5495 Cumulative_SOC_deviation: 71.7033 Fuel Consumption: 48.1440\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -730.496343472708 Explore P: 0.0290 SOC: 0.5554 Cumulative_SOC_deviation: 68.0344 Fuel Consumption: 50.1527\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -767.0873984150506 Explore P: 0.0285 SOC: 0.5510 Cumulative_SOC_deviation: 71.8665 Fuel Consumption: 48.4226\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -770.2850004632184 Explore P: 0.0280 SOC: 0.5524 Cumulative_SOC_deviation: 72.0504 Fuel Consumption: 49.7813\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -787.9823420863712 Explore P: 0.0275 SOC: 0.5506 Cumulative_SOC_deviation: 73.9638 Fuel Consumption: 48.3444\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -783.5039026935864 Explore P: 0.0270 SOC: 0.5509 Cumulative_SOC_deviation: 73.5614 Fuel Consumption: 47.8898\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -797.3223260204514 Explore P: 0.0265 SOC: 0.5506 Cumulative_SOC_deviation: 74.7878 Fuel Consumption: 49.4440\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -746.9055837648409 Explore P: 0.0261 SOC: 0.5516 Cumulative_SOC_deviation: 69.7413 Fuel Consumption: 49.4929\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -752.6232968173443 Explore P: 0.0257 SOC: 0.5498 Cumulative_SOC_deviation: 70.3873 Fuel Consumption: 48.7500\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -730.389672297806 Explore P: 0.0252 SOC: 0.5555 Cumulative_SOC_deviation: 67.8864 Fuel Consumption: 51.5261\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -723.6605099112194 Explore P: 0.0248 SOC: 0.5508 Cumulative_SOC_deviation: 67.4759 Fuel Consumption: 48.9015\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -787.0249707938161 Explore P: 0.0244 SOC: 0.5523 Cumulative_SOC_deviation: 73.7786 Fuel Consumption: 49.2388\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -789.2346167131258 Explore P: 0.0240 SOC: 0.5497 Cumulative_SOC_deviation: 74.2083 Fuel Consumption: 47.1518\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -763.8101542351626 Explore P: 0.0237 SOC: 0.5502 Cumulative_SOC_deviation: 71.5798 Fuel Consumption: 48.0120\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -781.756107789661 Explore P: 0.0233 SOC: 0.5526 Cumulative_SOC_deviation: 73.3788 Fuel Consumption: 47.9680\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -773.7398372902632 Explore P: 0.0229 SOC: 0.5500 Cumulative_SOC_deviation: 72.5664 Fuel Consumption: 48.0755\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -776.976999170698 Explore P: 0.0226 SOC: 0.5512 Cumulative_SOC_deviation: 72.8056 Fuel Consumption: 48.9211\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -762.3211516112008 Explore P: 0.0222 SOC: 0.5515 Cumulative_SOC_deviation: 71.3967 Fuel Consumption: 48.3541\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -811.941866646454 Explore P: 0.0219 SOC: 0.5529 Cumulative_SOC_deviation: 76.4316 Fuel Consumption: 47.6259\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -770.0405700876571 Explore P: 0.0216 SOC: 0.5530 Cumulative_SOC_deviation: 72.0460 Fuel Consumption: 49.5809\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -768.2916300588117 Explore P: 0.0213 SOC: 0.5519 Cumulative_SOC_deviation: 72.0265 Fuel Consumption: 48.0267\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -795.5286720214216 Explore P: 0.0210 SOC: 0.5463 Cumulative_SOC_deviation: 74.9066 Fuel Consumption: 46.4626\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -793.1197855919725 Explore P: 0.0207 SOC: 0.5487 Cumulative_SOC_deviation: 74.4512 Fuel Consumption: 48.6083\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -736.5757658187639 Explore P: 0.0204 SOC: 0.5492 Cumulative_SOC_deviation: 68.7308 Fuel Consumption: 49.2681\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -743.0425734441864 Explore P: 0.0201 SOC: 0.5518 Cumulative_SOC_deviation: 69.3946 Fuel Consumption: 49.0970\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -733.2699549642072 Explore P: 0.0198 SOC: 0.5539 Cumulative_SOC_deviation: 68.4188 Fuel Consumption: 49.0824\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -728.1421388765581 Explore P: 0.0196 SOC: 0.5499 Cumulative_SOC_deviation: 68.0443 Fuel Consumption: 47.6992\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -723.8083937060907 Explore P: 0.0193 SOC: 0.5508 Cumulative_SOC_deviation: 67.5244 Fuel Consumption: 48.5643\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -750.2988503040716 Explore P: 0.0190 SOC: 0.5529 Cumulative_SOC_deviation: 70.2189 Fuel Consumption: 48.1097\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -731.4687608373285 Explore P: 0.0188 SOC: 0.5562 Cumulative_SOC_deviation: 68.3471 Fuel Consumption: 47.9973\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -700.3482118867742 Explore P: 0.0186 SOC: 0.5546 Cumulative_SOC_deviation: 64.9692 Fuel Consumption: 50.6562\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -710.7632805116846 Explore P: 0.0183 SOC: 0.5524 Cumulative_SOC_deviation: 66.0699 Fuel Consumption: 50.0648\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -781.635558360083 Explore P: 0.0181 SOC: 0.5532 Cumulative_SOC_deviation: 73.2978 Fuel Consumption: 48.6572\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -741.386768827543 Explore P: 0.0179 SOC: 0.5536 Cumulative_SOC_deviation: 69.2187 Fuel Consumption: 49.1997\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -743.4422341976356 Explore P: 0.0177 SOC: 0.5520 Cumulative_SOC_deviation: 69.6022 Fuel Consumption: 47.4206\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -744.5011354536904 Explore P: 0.0175 SOC: 0.5518 Cumulative_SOC_deviation: 69.5155 Fuel Consumption: 49.3463\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -761.5274634567163 Explore P: 0.0173 SOC: 0.5521 Cumulative_SOC_deviation: 71.3291 Fuel Consumption: 48.2368\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -736.3566639033694 Explore P: 0.0171 SOC: 0.5596 Cumulative_SOC_deviation: 68.8413 Fuel Consumption: 47.9436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -714.8720031404621 Explore P: 0.0169 SOC: 0.5534 Cumulative_SOC_deviation: 66.6601 Fuel Consumption: 48.2710\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -715.8278378976908 Explore P: 0.0167 SOC: 0.5544 Cumulative_SOC_deviation: 66.6472 Fuel Consumption: 49.3561\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -718.6115881177732 Explore P: 0.0165 SOC: 0.5545 Cumulative_SOC_deviation: 67.2115 Fuel Consumption: 46.4969\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -723.8117899618937 Explore P: 0.0163 SOC: 0.5563 Cumulative_SOC_deviation: 67.6015 Fuel Consumption: 47.7969\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -794.3505691158199 Explore P: 0.0162 SOC: 0.5551 Cumulative_SOC_deviation: 74.5698 Fuel Consumption: 48.6523\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -765.558640360979 Explore P: 0.0160 SOC: 0.5547 Cumulative_SOC_deviation: 71.6432 Fuel Consumption: 49.1264\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -779.1380627420232 Explore P: 0.0158 SOC: 0.5554 Cumulative_SOC_deviation: 73.0759 Fuel Consumption: 48.3786\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -790.6490644086115 Explore P: 0.0157 SOC: 0.5540 Cumulative_SOC_deviation: 74.2906 Fuel Consumption: 47.7432\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -813.3368499691597 Explore P: 0.0155 SOC: 0.5529 Cumulative_SOC_deviation: 76.6517 Fuel Consumption: 46.8194\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -750.7171466375862 Explore P: 0.0154 SOC: 0.5540 Cumulative_SOC_deviation: 70.1312 Fuel Consumption: 49.4049\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -793.7775006104799 Explore P: 0.0152 SOC: 0.5528 Cumulative_SOC_deviation: 74.6650 Fuel Consumption: 47.1274\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -762.4646678278184 Explore P: 0.0151 SOC: 0.5541 Cumulative_SOC_deviation: 71.5073 Fuel Consumption: 47.3913\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -701.904999640832 Explore P: 0.0149 SOC: 0.5523 Cumulative_SOC_deviation: 65.1733 Fuel Consumption: 50.1723\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -729.3969637232145 Explore P: 0.0148 SOC: 0.5530 Cumulative_SOC_deviation: 68.0872 Fuel Consumption: 48.5252\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -716.1993989247504 Explore P: 0.0147 SOC: 0.5539 Cumulative_SOC_deviation: 66.7190 Fuel Consumption: 49.0091\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -681.5220045153435 Explore P: 0.0146 SOC: 0.5572 Cumulative_SOC_deviation: 63.0827 Fuel Consumption: 50.6953\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -643.362023718588 Explore P: 0.0144 SOC: 0.5552 Cumulative_SOC_deviation: 59.3854 Fuel Consumption: 49.5076\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -652.607077727978 Explore P: 0.0143 SOC: 0.5568 Cumulative_SOC_deviation: 60.3862 Fuel Consumption: 48.7451\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -646.7284358856526 Explore P: 0.0142 SOC: 0.5604 Cumulative_SOC_deviation: 59.7939 Fuel Consumption: 48.7891\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -702.1633505150471 Explore P: 0.0141 SOC: 0.5559 Cumulative_SOC_deviation: 65.4200 Fuel Consumption: 47.9631\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {i}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results_scaling_mass1200.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

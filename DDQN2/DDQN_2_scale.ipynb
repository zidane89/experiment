{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN_scaling import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 1000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 2\n",
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer sequential_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 1 Total reward: -4845.30442478257 Explore P: 0.9732 SOC: 1.0000 Cumulative_SOC_deviation: 471.7238 Fuel Consumption: 128.0667\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 2 Total reward: -4895.580807100905 Explore P: 0.9471 SOC: 1.0000 Cumulative_SOC_deviation: 476.9626 Fuel Consumption: 125.9553\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -4895.887529865797 Explore P: 0.9217 SOC: 1.0000 Cumulative_SOC_deviation: 476.8818 Fuel Consumption: 127.0696\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -4887.382052659463 Explore P: 0.8970 SOC: 1.0000 Cumulative_SOC_deviation: 476.4174 Fuel Consumption: 123.2085\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -4805.785618503359 Explore P: 0.8730 SOC: 1.0000 Cumulative_SOC_deviation: 468.3037 Fuel Consumption: 122.7491\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -4845.2845499728965 Explore P: 0.8496 SOC: 1.0000 Cumulative_SOC_deviation: 472.2438 Fuel Consumption: 122.8468\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -4692.451820921759 Explore P: 0.8269 SOC: 1.0000 Cumulative_SOC_deviation: 457.4952 Fuel Consumption: 117.4998\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -4699.1765719371715 Explore P: 0.8048 SOC: 1.0000 Cumulative_SOC_deviation: 457.8695 Fuel Consumption: 120.4812\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -4793.548713767003 Explore P: 0.7832 SOC: 1.0000 Cumulative_SOC_deviation: 466.5345 Fuel Consumption: 128.2036\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -4665.5224403705015 Explore P: 0.7623 SOC: 1.0000 Cumulative_SOC_deviation: 454.5999 Fuel Consumption: 119.5233\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -4555.176423722897 Explore P: 0.7419 SOC: 1.0000 Cumulative_SOC_deviation: 443.5516 Fuel Consumption: 119.6601\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -4751.437842973847 Explore P: 0.7221 SOC: 1.0000 Cumulative_SOC_deviation: 462.8415 Fuel Consumption: 123.0228\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -4822.131562876917 Explore P: 0.7028 SOC: 1.0000 Cumulative_SOC_deviation: 469.9901 Fuel Consumption: 122.2310\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -4757.699959772962 Explore P: 0.6840 SOC: 1.0000 Cumulative_SOC_deviation: 464.6730 Fuel Consumption: 110.9701\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -4766.232801831624 Explore P: 0.6658 SOC: 1.0000 Cumulative_SOC_deviation: 464.5312 Fuel Consumption: 120.9211\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -4756.834083060756 Explore P: 0.6480 SOC: 1.0000 Cumulative_SOC_deviation: 463.9412 Fuel Consumption: 117.4216\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -4699.428958972728 Explore P: 0.6307 SOC: 1.0000 Cumulative_SOC_deviation: 457.9270 Fuel Consumption: 120.1587\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -4785.96418631677 Explore P: 0.6139 SOC: 1.0000 Cumulative_SOC_deviation: 465.7585 Fuel Consumption: 128.3795\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -4707.129519765958 Explore P: 0.5976 SOC: 1.0000 Cumulative_SOC_deviation: 458.5397 Fuel Consumption: 121.7325\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -4626.363671846685 Explore P: 0.5816 SOC: 1.0000 Cumulative_SOC_deviation: 450.2891 Fuel Consumption: 123.4724\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -4675.34331398358 Explore P: 0.5662 SOC: 1.0000 Cumulative_SOC_deviation: 455.4119 Fuel Consumption: 121.2242\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -4720.095381517279 Explore P: 0.5511 SOC: 1.0000 Cumulative_SOC_deviation: 459.4091 Fuel Consumption: 126.0042\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -4737.211920811993 Explore P: 0.5364 SOC: 1.0000 Cumulative_SOC_deviation: 461.2694 Fuel Consumption: 124.5184\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -4660.776250283965 Explore P: 0.5222 SOC: 1.0000 Cumulative_SOC_deviation: 454.1273 Fuel Consumption: 119.5037\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -4705.400885222805 Explore P: 0.5083 SOC: 1.0000 Cumulative_SOC_deviation: 458.3150 Fuel Consumption: 122.2505\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -4758.176510138837 Explore P: 0.4948 SOC: 1.0000 Cumulative_SOC_deviation: 463.1361 Fuel Consumption: 126.8155\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -4541.791608052277 Explore P: 0.4817 SOC: 1.0000 Cumulative_SOC_deviation: 443.2561 Fuel Consumption: 109.2301\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -4554.761127394033 Explore P: 0.4689 SOC: 1.0000 Cumulative_SOC_deviation: 444.5492 Fuel Consumption: 109.2692\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -4403.261255811701 Explore P: 0.4565 SOC: 1.0000 Cumulative_SOC_deviation: 430.3239 Fuel Consumption: 100.0220\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -4129.490183232632 Explore P: 0.4444 SOC: 1.0000 Cumulative_SOC_deviation: 402.9185 Fuel Consumption: 100.3055\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -4379.900687485705 Explore P: 0.4326 SOC: 1.0000 Cumulative_SOC_deviation: 426.6379 Fuel Consumption: 113.5214\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -4394.944391814149 Explore P: 0.4212 SOC: 1.0000 Cumulative_SOC_deviation: 429.4727 Fuel Consumption: 100.2175\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -4439.415203499096 Explore P: 0.4100 SOC: 1.0000 Cumulative_SOC_deviation: 433.6255 Fuel Consumption: 103.1598\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -4346.4462658397415 Explore P: 0.3992 SOC: 1.0000 Cumulative_SOC_deviation: 425.4078 Fuel Consumption: 92.3681\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -3991.5038593070717 Explore P: 0.3887 SOC: 1.0000 Cumulative_SOC_deviation: 390.1609 Fuel Consumption: 89.8950\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -4074.6001959329374 Explore P: 0.3784 SOC: 1.0000 Cumulative_SOC_deviation: 397.9280 Fuel Consumption: 95.3202\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -3936.226834946168 Explore P: 0.3684 SOC: 1.0000 Cumulative_SOC_deviation: 384.1249 Fuel Consumption: 94.9781\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -4133.727209100721 Explore P: 0.3587 SOC: 1.0000 Cumulative_SOC_deviation: 403.3578 Fuel Consumption: 100.1491\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -3949.2374372754116 Explore P: 0.3493 SOC: 1.0000 Cumulative_SOC_deviation: 384.8463 Fuel Consumption: 100.7747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -4289.670650241488 Explore P: 0.3401 SOC: 1.0000 Cumulative_SOC_deviation: 419.7400 Fuel Consumption: 92.2704\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -4013.5951745148745 Explore P: 0.3311 SOC: 1.0000 Cumulative_SOC_deviation: 391.8275 Fuel Consumption: 95.3202\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -3918.6744382206352 Explore P: 0.3224 SOC: 1.0000 Cumulative_SOC_deviation: 381.3433 Fuel Consumption: 105.2419\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -3608.271220909915 Explore P: 0.3140 SOC: 1.0000 Cumulative_SOC_deviation: 350.0713 Fuel Consumption: 107.5586\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -3932.3963964973595 Explore P: 0.3057 SOC: 1.0000 Cumulative_SOC_deviation: 383.0380 Fuel Consumption: 102.0161\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -3423.835378315117 Explore P: 0.2977 SOC: 1.0000 Cumulative_SOC_deviation: 332.1634 Fuel Consumption: 102.2018\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -3408.8605202668095 Explore P: 0.2899 SOC: 1.0000 Cumulative_SOC_deviation: 331.4811 Fuel Consumption: 94.0494\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -3226.7006886454096 Explore P: 0.2824 SOC: 1.0000 Cumulative_SOC_deviation: 314.0765 Fuel Consumption: 85.9361\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -2993.9388390572826 Explore P: 0.2750 SOC: 1.0000 Cumulative_SOC_deviation: 291.0036 Fuel Consumption: 83.9029\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -2421.2521887764547 Explore P: 0.2678 SOC: 1.0000 Cumulative_SOC_deviation: 233.7916 Fuel Consumption: 83.3359\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -2437.9008478162746 Explore P: 0.2608 SOC: 1.0000 Cumulative_SOC_deviation: 235.0430 Fuel Consumption: 87.4708\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -1882.5872937900133 Explore P: 0.2540 SOC: 1.0000 Cumulative_SOC_deviation: 180.3376 Fuel Consumption: 79.2109\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -2521.1023247838757 Explore P: 0.2474 SOC: 1.0000 Cumulative_SOC_deviation: 243.1461 Fuel Consumption: 89.6409\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -2428.9541400081507 Explore P: 0.2410 SOC: 1.0000 Cumulative_SOC_deviation: 233.7348 Fuel Consumption: 91.6057\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -2094.833896836319 Explore P: 0.2347 SOC: 1.0000 Cumulative_SOC_deviation: 200.7676 Fuel Consumption: 87.1580\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 55 Total reward: -2376.9205661412207 Explore P: 0.2286 SOC: 1.0000 Cumulative_SOC_deviation: 229.1346 Fuel Consumption: 85.5744\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 56 Total reward: -2179.193549525294 Explore P: 0.2227 SOC: 1.0000 Cumulative_SOC_deviation: 209.0169 Fuel Consumption: 89.0250\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 57 Total reward: -1845.3873609931031 Explore P: 0.2170 SOC: 0.9872 Cumulative_SOC_deviation: 177.6499 Fuel Consumption: 68.8884\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 58 Total reward: -1631.6172236301766 Explore P: 0.2114 SOC: 0.9779 Cumulative_SOC_deviation: 156.1830 Fuel Consumption: 69.7877\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 59 Total reward: -1086.622907083706 Explore P: 0.2059 SOC: 0.8637 Cumulative_SOC_deviation: 102.5721 Fuel Consumption: 60.9021\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 60 Total reward: -978.2472142413816 Explore P: 0.2006 SOC: 0.8124 Cumulative_SOC_deviation: 92.0395 Fuel Consumption: 57.8523\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 61 Total reward: -634.4681860854963 Explore P: 0.1954 SOC: 0.7263 Cumulative_SOC_deviation: 58.3136 Fuel Consumption: 51.3323\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 62 Total reward: -384.06093826899087 Explore P: 0.1904 SOC: 0.6996 Cumulative_SOC_deviation: 33.4038 Fuel Consumption: 50.0225\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 63 Total reward: -413.50297429210076 Explore P: 0.1855 SOC: 0.6868 Cumulative_SOC_deviation: 36.4341 Fuel Consumption: 49.1623\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 64 Total reward: -294.70146466753806 Explore P: 0.1808 SOC: 0.6693 Cumulative_SOC_deviation: 24.6781 Fuel Consumption: 47.9208\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 65 Total reward: -403.1041469010656 Explore P: 0.1761 SOC: 0.7618 Cumulative_SOC_deviation: 34.8595 Fuel Consumption: 54.5092\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 66 Total reward: -1357.7724111702066 Explore P: 0.1716 SOC: 1.0000 Cumulative_SOC_deviation: 128.4954 Fuel Consumption: 72.8180\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 67 Total reward: -343.2319439209557 Explore P: 0.1673 SOC: 0.6328 Cumulative_SOC_deviation: 29.7374 Fuel Consumption: 45.8583\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 68 Total reward: -280.49741206128067 Explore P: 0.1630 SOC: 0.6260 Cumulative_SOC_deviation: 23.5871 Fuel Consumption: 44.6266\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 69 Total reward: -492.3052158826147 Explore P: 0.1589 SOC: 0.6078 Cumulative_SOC_deviation: 44.8275 Fuel Consumption: 44.0304\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 70 Total reward: -441.5704558088734 Explore P: 0.1548 SOC: 0.6035 Cumulative_SOC_deviation: 39.7110 Fuel Consumption: 44.4605\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 71 Total reward: -438.24852143194687 Explore P: 0.1509 SOC: 0.6440 Cumulative_SOC_deviation: 39.1618 Fuel Consumption: 46.6305\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 72 Total reward: -491.31311049485873 Explore P: 0.1471 SOC: 0.6695 Cumulative_SOC_deviation: 44.3470 Fuel Consumption: 47.8426\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 73 Total reward: -402.88277967076687 Explore P: 0.1434 SOC: 0.6696 Cumulative_SOC_deviation: 35.4903 Fuel Consumption: 47.9795\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 74 Total reward: -369.0165885649618 Explore P: 0.1398 SOC: 0.6310 Cumulative_SOC_deviation: 32.4009 Fuel Consumption: 45.0079\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 75 Total reward: -282.9958143542816 Explore P: 0.1362 SOC: 0.6280 Cumulative_SOC_deviation: 23.8672 Fuel Consumption: 44.3236\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 76 Total reward: -394.342760402402 Explore P: 0.1328 SOC: 0.6023 Cumulative_SOC_deviation: 35.1104 Fuel Consumption: 43.2386\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 77 Total reward: -423.7507696583027 Explore P: 0.1295 SOC: 0.6052 Cumulative_SOC_deviation: 37.9779 Fuel Consumption: 43.9717\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 78 Total reward: -450.7387697091176 Explore P: 0.1263 SOC: 0.5917 Cumulative_SOC_deviation: 40.7187 Fuel Consumption: 43.5514\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 79 Total reward: -417.60940632358677 Explore P: 0.1231 SOC: 0.5918 Cumulative_SOC_deviation: 37.3325 Fuel Consumption: 44.2845\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 80 Total reward: -473.56291646841083 Explore P: 0.1200 SOC: 0.5891 Cumulative_SOC_deviation: 43.1136 Fuel Consumption: 42.4272\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 81 Total reward: -497.03897231393654 Explore P: 0.1171 SOC: 0.5892 Cumulative_SOC_deviation: 45.4612 Fuel Consumption: 42.4272\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 82 Total reward: -575.9664376288014 Explore P: 0.1142 SOC: 0.5881 Cumulative_SOC_deviation: 53.3109 Fuel Consumption: 42.8573\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 83 Total reward: -692.2248849695083 Explore P: 0.1113 SOC: 0.5835 Cumulative_SOC_deviation: 64.9543 Fuel Consumption: 42.6814\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 84 Total reward: -763.750762874682 Explore P: 0.1086 SOC: 0.5660 Cumulative_SOC_deviation: 72.1861 Fuel Consumption: 41.8896\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 85 Total reward: -773.2219684629484 Explore P: 0.1059 SOC: 0.5654 Cumulative_SOC_deviation: 73.1704 Fuel Consumption: 41.5182\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 86 Total reward: -671.6274354522074 Explore P: 0.1033 SOC: 0.5647 Cumulative_SOC_deviation: 63.0803 Fuel Consumption: 40.8241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 87 Total reward: -705.0431198009971 Explore P: 0.1008 SOC: 0.5699 Cumulative_SOC_deviation: 66.3329 Fuel Consumption: 41.7137\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 88 Total reward: -749.5066006713433 Explore P: 0.0983 SOC: 0.5542 Cumulative_SOC_deviation: 71.0090 Fuel Consumption: 39.4165\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 89 Total reward: -828.3773863976431 Explore P: 0.0960 SOC: 0.5558 Cumulative_SOC_deviation: 78.8257 Fuel Consumption: 40.1203\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 90 Total reward: -923.386813404108 Explore P: 0.0936 SOC: 0.5416 Cumulative_SOC_deviation: 88.4664 Fuel Consumption: 38.7225\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 91 Total reward: -877.5037180588058 Explore P: 0.0914 SOC: 0.5433 Cumulative_SOC_deviation: 83.8410 Fuel Consumption: 39.0939\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 92 Total reward: -807.7771090086826 Explore P: 0.0892 SOC: 0.5398 Cumulative_SOC_deviation: 76.8781 Fuel Consumption: 38.9962\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 93 Total reward: -805.972086434893 Explore P: 0.0870 SOC: 0.5451 Cumulative_SOC_deviation: 76.7650 Fuel Consumption: 38.3217\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 94 Total reward: -794.9135568692726 Explore P: 0.0849 SOC: 0.5543 Cumulative_SOC_deviation: 75.4969 Fuel Consumption: 39.9444\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 95 Total reward: -676.4688226702589 Explore P: 0.0829 SOC: 0.5607 Cumulative_SOC_deviation: 63.6261 Fuel Consumption: 40.2083\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 96 Total reward: -764.0781436343276 Explore P: 0.0809 SOC: 0.5445 Cumulative_SOC_deviation: 72.5072 Fuel Consumption: 39.0060\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 97 Total reward: -756.5068974216348 Explore P: 0.0790 SOC: 0.5747 Cumulative_SOC_deviation: 71.5976 Fuel Consumption: 40.5309\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 98 Total reward: -661.183744135409 Explore P: 0.0771 SOC: 0.5518 Cumulative_SOC_deviation: 62.1611 Fuel Consumption: 39.5729\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 99 Total reward: -689.5626858738287 Explore P: 0.0753 SOC: 0.5608 Cumulative_SOC_deviation: 65.0000 Fuel Consumption: 39.5631\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 100 Total reward: -668.0011864928126 Explore P: 0.0735 SOC: 0.5616 Cumulative_SOC_deviation: 62.8018 Fuel Consumption: 39.9835\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 101 Total reward: -634.3219114209071 Explore P: 0.0718 SOC: 0.5665 Cumulative_SOC_deviation: 59.3586 Fuel Consumption: 40.7362\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 102 Total reward: -647.7984319668227 Explore P: 0.0701 SOC: 0.5595 Cumulative_SOC_deviation: 60.7727 Fuel Consumption: 40.0714\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 103 Total reward: -610.513039413021 Explore P: 0.0685 SOC: 0.5623 Cumulative_SOC_deviation: 56.9591 Fuel Consumption: 40.9219\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 104 Total reward: -612.0467563576062 Explore P: 0.0669 SOC: 0.5598 Cumulative_SOC_deviation: 57.1545 Fuel Consumption: 40.5016\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 105 Total reward: -594.8898187515192 Explore P: 0.0654 SOC: 0.5610 Cumulative_SOC_deviation: 55.5248 Fuel Consumption: 39.6413\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 106 Total reward: -665.2583045204749 Explore P: 0.0639 SOC: 0.5709 Cumulative_SOC_deviation: 62.4679 Fuel Consumption: 40.5798\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 107 Total reward: -717.0000460170374 Explore P: 0.0624 SOC: 0.5591 Cumulative_SOC_deviation: 67.6616 Fuel Consumption: 40.3843\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 108 Total reward: -593.401807589605 Explore P: 0.0610 SOC: 0.5638 Cumulative_SOC_deviation: 55.2597 Fuel Consumption: 40.8046\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 109 Total reward: -573.4637309444256 Explore P: 0.0596 SOC: 0.5649 Cumulative_SOC_deviation: 53.3490 Fuel Consumption: 39.9737\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 110 Total reward: -561.481962104081 Explore P: 0.0583 SOC: 0.5769 Cumulative_SOC_deviation: 52.0091 Fuel Consumption: 41.3911\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 111 Total reward: -537.4358934457006 Explore P: 0.0570 SOC: 0.5658 Cumulative_SOC_deviation: 49.6680 Fuel Consumption: 40.7557\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 112 Total reward: -539.099611351179 Explore P: 0.0557 SOC: 0.5681 Cumulative_SOC_deviation: 49.8383 Fuel Consumption: 40.7166\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 113 Total reward: -521.4905956922686 Explore P: 0.0545 SOC: 0.5756 Cumulative_SOC_deviation: 47.9660 Fuel Consumption: 41.8310\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 114 Total reward: -504.0482086223181 Explore P: 0.0533 SOC: 0.5747 Cumulative_SOC_deviation: 46.2374 Fuel Consumption: 41.6746\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 115 Total reward: -514.3918479031654 Explore P: 0.0521 SOC: 0.5703 Cumulative_SOC_deviation: 47.3753 Fuel Consumption: 40.6384\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 116 Total reward: -508.56855383624423 Explore P: 0.0510 SOC: 0.5726 Cumulative_SOC_deviation: 46.7060 Fuel Consumption: 41.5084\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 117 Total reward: -512.9410975684024 Explore P: 0.0498 SOC: 0.5693 Cumulative_SOC_deviation: 47.1785 Fuel Consumption: 41.1565\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 118 Total reward: -484.8645343406634 Explore P: 0.0488 SOC: 0.5739 Cumulative_SOC_deviation: 44.3200 Fuel Consumption: 41.6648\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 119 Total reward: -479.6917264313055 Explore P: 0.0477 SOC: 0.5750 Cumulative_SOC_deviation: 43.7362 Fuel Consumption: 42.3295\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 120 Total reward: -474.71094243620524 Explore P: 0.0467 SOC: 0.5749 Cumulative_SOC_deviation: 43.4200 Fuel Consumption: 40.5113\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 121 Total reward: -495.5571520257024 Explore P: 0.0457 SOC: 0.5798 Cumulative_SOC_deviation: 45.4156 Fuel Consumption: 41.4009\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 122 Total reward: -464.9228682189815 Explore P: 0.0447 SOC: 0.5714 Cumulative_SOC_deviation: 42.2662 Fuel Consumption: 42.2611\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 123 Total reward: -461.9960926430182 Explore P: 0.0438 SOC: 0.5751 Cumulative_SOC_deviation: 42.0322 Fuel Consumption: 41.6746\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 124 Total reward: -442.766914998471 Explore P: 0.0429 SOC: 0.5782 Cumulative_SOC_deviation: 40.0154 Fuel Consumption: 42.6130\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 125 Total reward: -460.0894841675768 Explore P: 0.0420 SOC: 0.5733 Cumulative_SOC_deviation: 41.8806 Fuel Consumption: 41.2836\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 126 Total reward: -422.49124660378106 Explore P: 0.0411 SOC: 0.5838 Cumulative_SOC_deviation: 37.9869 Fuel Consumption: 42.6227\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 127 Total reward: -425.27782012548766 Explore P: 0.0403 SOC: 0.5800 Cumulative_SOC_deviation: 38.1854 Fuel Consumption: 43.4243\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 128 Total reward: -379.2812208205302 Explore P: 0.0395 SOC: 0.5823 Cumulative_SOC_deviation: 33.6189 Fuel Consumption: 43.0919\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 129 Total reward: -394.02128741523694 Explore P: 0.0387 SOC: 0.5821 Cumulative_SOC_deviation: 35.2073 Fuel Consumption: 41.9483\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 130 Total reward: -364.3515567122448 Explore P: 0.0379 SOC: 0.5832 Cumulative_SOC_deviation: 32.1709 Fuel Consumption: 42.6423\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 131 Total reward: -380.633276211901 Explore P: 0.0371 SOC: 0.5831 Cumulative_SOC_deviation: 33.7854 Fuel Consumption: 42.7791\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 132 Total reward: -382.07026421614194 Explore P: 0.0364 SOC: 0.5849 Cumulative_SOC_deviation: 33.9623 Fuel Consumption: 42.4468\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 133 Total reward: -336.94598326333056 Explore P: 0.0357 SOC: 0.5932 Cumulative_SOC_deviation: 29.4284 Fuel Consumption: 42.6618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 134 Total reward: -354.20368566818934 Explore P: 0.0350 SOC: 0.5873 Cumulative_SOC_deviation: 31.2050 Fuel Consumption: 42.1535\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 135 Total reward: -314.9309251074737 Explore P: 0.0343 SOC: 0.5834 Cumulative_SOC_deviation: 27.2856 Fuel Consumption: 42.0753\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 136 Total reward: -285.726081215802 Explore P: 0.0336 SOC: 0.5859 Cumulative_SOC_deviation: 24.3172 Fuel Consumption: 42.5543\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 137 Total reward: -288.29065697368515 Explore P: 0.0330 SOC: 0.5868 Cumulative_SOC_deviation: 24.5336 Fuel Consumption: 42.9551\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 138 Total reward: -272.9070215630177 Explore P: 0.0324 SOC: 0.5876 Cumulative_SOC_deviation: 23.0529 Fuel Consumption: 42.3784\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 139 Total reward: -292.1248697665312 Explore P: 0.0318 SOC: 0.5894 Cumulative_SOC_deviation: 25.0147 Fuel Consumption: 41.9776\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 140 Total reward: -238.55463579895246 Explore P: 0.0312 SOC: 0.5926 Cumulative_SOC_deviation: 19.4847 Fuel Consumption: 43.7078\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 141 Total reward: -277.91338727508395 Explore P: 0.0306 SOC: 0.5940 Cumulative_SOC_deviation: 23.4499 Fuel Consumption: 43.4145\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 142 Total reward: -301.72535855421734 Explore P: 0.0301 SOC: 0.5943 Cumulative_SOC_deviation: 25.9122 Fuel Consumption: 42.6032\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 143 Total reward: -214.91637000062352 Explore P: 0.0295 SOC: 0.5948 Cumulative_SOC_deviation: 17.1473 Fuel Consumption: 43.4438\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 144 Total reward: -233.30551938757398 Explore P: 0.0290 SOC: 0.5924 Cumulative_SOC_deviation: 19.0810 Fuel Consumption: 42.4957\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 145 Total reward: -206.29581814265833 Explore P: 0.0285 SOC: 0.5931 Cumulative_SOC_deviation: 16.2471 Fuel Consumption: 43.8251\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 146 Total reward: -196.47189561163114 Explore P: 0.0280 SOC: 0.5978 Cumulative_SOC_deviation: 15.3370 Fuel Consumption: 43.1017\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 147 Total reward: -180.78446710076105 Explore P: 0.0275 SOC: 0.5932 Cumulative_SOC_deviation: 13.7243 Fuel Consumption: 43.5416\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 148 Total reward: -239.65062180509574 Explore P: 0.0270 SOC: 0.5955 Cumulative_SOC_deviation: 19.6803 Fuel Consumption: 42.8476\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 149 Total reward: -190.2146227046851 Explore P: 0.0265 SOC: 0.6037 Cumulative_SOC_deviation: 14.6223 Fuel Consumption: 43.9913\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 150 Total reward: -182.441637996365 Explore P: 0.0261 SOC: 0.5994 Cumulative_SOC_deviation: 13.8988 Fuel Consumption: 43.4536\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 151 Total reward: -183.40792824432023 Explore P: 0.0257 SOC: 0.5988 Cumulative_SOC_deviation: 14.0081 Fuel Consumption: 43.3265\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 152 Total reward: -158.7683115989794 Explore P: 0.0252 SOC: 0.5988 Cumulative_SOC_deviation: 11.5843 Fuel Consumption: 42.9258\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 153 Total reward: -170.27978465051342 Explore P: 0.0248 SOC: 0.5989 Cumulative_SOC_deviation: 12.6572 Fuel Consumption: 43.7078\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 154 Total reward: -179.27288589577122 Explore P: 0.0244 SOC: 0.6020 Cumulative_SOC_deviation: 13.4959 Fuel Consumption: 44.3138\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 155 Total reward: -153.7803279768751 Explore P: 0.0240 SOC: 0.6058 Cumulative_SOC_deviation: 10.9173 Fuel Consumption: 44.6071\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 156 Total reward: -162.61276510991266 Explore P: 0.0237 SOC: 0.6056 Cumulative_SOC_deviation: 11.9100 Fuel Consumption: 43.5123\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 157 Total reward: -154.0423832499968 Explore P: 0.0233 SOC: 0.6085 Cumulative_SOC_deviation: 10.9416 Fuel Consumption: 44.6266\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 158 Total reward: -193.53070594939015 Explore P: 0.0229 SOC: 0.6104 Cumulative_SOC_deviation: 14.8787 Fuel Consumption: 44.7439\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 159 Total reward: -176.9704932332625 Explore P: 0.0226 SOC: 0.6081 Cumulative_SOC_deviation: 13.3615 Fuel Consumption: 43.3559\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 160 Total reward: -261.265231270671 Explore P: 0.0222 SOC: 0.6533 Cumulative_SOC_deviation: 21.4332 Fuel Consumption: 46.9336\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 161 Total reward: -451.6112086882675 Explore P: 0.0219 SOC: 0.6241 Cumulative_SOC_deviation: 40.6515 Fuel Consumption: 45.0958\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 162 Total reward: -381.0552508018467 Explore P: 0.0216 SOC: 0.6987 Cumulative_SOC_deviation: 33.1218 Fuel Consumption: 49.8368\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 163 Total reward: -255.19517727787894 Explore P: 0.0213 SOC: 0.6066 Cumulative_SOC_deviation: 21.0705 Fuel Consumption: 44.4898\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 164 Total reward: -225.23542148912625 Explore P: 0.0210 SOC: 0.6211 Cumulative_SOC_deviation: 18.0775 Fuel Consumption: 44.4605\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 165 Total reward: -212.31424971629124 Explore P: 0.0207 SOC: 0.6044 Cumulative_SOC_deviation: 16.8479 Fuel Consumption: 43.8349\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 166 Total reward: -209.35313665401193 Explore P: 0.0204 SOC: 0.6000 Cumulative_SOC_deviation: 16.6711 Fuel Consumption: 42.6423\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 167 Total reward: -236.8831179175289 Explore P: 0.0201 SOC: 0.5990 Cumulative_SOC_deviation: 19.2520 Fuel Consumption: 44.3627\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 168 Total reward: -224.85465571946918 Explore P: 0.0198 SOC: 0.5985 Cumulative_SOC_deviation: 18.2427 Fuel Consumption: 42.4272\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 169 Total reward: -1455.332797310564 Explore P: 0.0196 SOC: 1.0000 Cumulative_SOC_deviation: 137.6904 Fuel Consumption: 78.4289\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 170 Total reward: -438.69399285987066 Explore P: 0.0193 SOC: 0.5945 Cumulative_SOC_deviation: 39.5690 Fuel Consumption: 43.0040\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 171 Total reward: -191.12948603692905 Explore P: 0.0190 SOC: 0.5972 Cumulative_SOC_deviation: 14.7607 Fuel Consumption: 43.5221\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 172 Total reward: -196.23729594944842 Explore P: 0.0188 SOC: 0.6072 Cumulative_SOC_deviation: 15.1738 Fuel Consumption: 44.4996\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 173 Total reward: -181.83552901468477 Explore P: 0.0186 SOC: 0.6009 Cumulative_SOC_deviation: 13.7971 Fuel Consumption: 43.8642\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 174 Total reward: -205.38629109194227 Explore P: 0.0183 SOC: 0.6298 Cumulative_SOC_deviation: 15.9997 Fuel Consumption: 45.3891\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 175 Total reward: -203.48823549563554 Explore P: 0.0181 SOC: 0.6001 Cumulative_SOC_deviation: 16.0123 Fuel Consumption: 43.3656\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 176 Total reward: -189.47979254395602 Explore P: 0.0179 SOC: 0.6063 Cumulative_SOC_deviation: 14.5166 Fuel Consumption: 44.3138\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 177 Total reward: -189.36551687134985 Explore P: 0.0177 SOC: 0.5982 Cumulative_SOC_deviation: 14.5814 Fuel Consumption: 43.5514\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 178 Total reward: -214.63887481037992 Explore P: 0.0175 SOC: 0.5969 Cumulative_SOC_deviation: 17.1830 Fuel Consumption: 42.8085\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 179 Total reward: -407.9704836444806 Explore P: 0.0173 SOC: 0.7359 Cumulative_SOC_deviation: 35.5240 Fuel Consumption: 52.7302\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 180 Total reward: -221.15261526651378 Explore P: 0.0171 SOC: 0.5977 Cumulative_SOC_deviation: 17.8149 Fuel Consumption: 43.0040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 181 Total reward: -272.0883996725025 Explore P: 0.0169 SOC: 0.5999 Cumulative_SOC_deviation: 22.8830 Fuel Consumption: 43.2581\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 182 Total reward: -2819.53821764992 Explore P: 0.0167 SOC: 1.0000 Cumulative_SOC_deviation: 271.7796 Fuel Consumption: 101.7424\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 183 Total reward: -275.5944639575794 Explore P: 0.0165 SOC: 0.5964 Cumulative_SOC_deviation: 23.2503 Fuel Consumption: 43.0919\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 184 Total reward: -271.8182926701394 Explore P: 0.0163 SOC: 0.5957 Cumulative_SOC_deviation: 22.9313 Fuel Consumption: 42.5054\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 185 Total reward: -269.3259913435959 Explore P: 0.0162 SOC: 0.5977 Cumulative_SOC_deviation: 22.6254 Fuel Consumption: 43.0724\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 186 Total reward: -261.9665018536926 Explore P: 0.0160 SOC: 0.5988 Cumulative_SOC_deviation: 21.8806 Fuel Consumption: 43.1604\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 187 Total reward: -259.2819690149459 Explore P: 0.0158 SOC: 0.5985 Cumulative_SOC_deviation: 21.5535 Fuel Consumption: 43.7469\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 188 Total reward: -271.79466942909085 Explore P: 0.0157 SOC: 0.5953 Cumulative_SOC_deviation: 22.8712 Fuel Consumption: 43.0822\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 189 Total reward: -300.001046990102 Explore P: 0.0155 SOC: 0.5955 Cumulative_SOC_deviation: 25.7691 Fuel Consumption: 42.3099\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 190 Total reward: -276.0609755673971 Explore P: 0.0154 SOC: 0.5957 Cumulative_SOC_deviation: 23.2275 Fuel Consumption: 43.7860\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 191 Total reward: -279.51647227150505 Explore P: 0.0152 SOC: 0.5951 Cumulative_SOC_deviation: 23.5730 Fuel Consumption: 43.7860\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 192 Total reward: -275.1315223106879 Explore P: 0.0151 SOC: 0.5970 Cumulative_SOC_deviation: 23.1737 Fuel Consumption: 43.3950\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 193 Total reward: -290.5614910538159 Explore P: 0.0149 SOC: 0.5985 Cumulative_SOC_deviation: 24.7215 Fuel Consumption: 43.3461\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 194 Total reward: -291.6908172461989 Explore P: 0.0148 SOC: 0.5956 Cumulative_SOC_deviation: 24.8921 Fuel Consumption: 42.7694\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 195 Total reward: -273.20496712046133 Explore P: 0.0147 SOC: 0.5927 Cumulative_SOC_deviation: 23.0230 Fuel Consumption: 42.9746\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 196 Total reward: -257.796670188862 Explore P: 0.0146 SOC: 0.5941 Cumulative_SOC_deviation: 21.4578 Fuel Consumption: 43.2190\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 197 Total reward: -247.07164534021783 Explore P: 0.0144 SOC: 0.5936 Cumulative_SOC_deviation: 20.4488 Fuel Consumption: 42.5836\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 198 Total reward: -249.11259055714814 Explore P: 0.0143 SOC: 0.5955 Cumulative_SOC_deviation: 20.5610 Fuel Consumption: 43.5025\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 199 Total reward: -253.58549860897577 Explore P: 0.0142 SOC: 0.6007 Cumulative_SOC_deviation: 20.9448 Fuel Consumption: 44.1379\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 200 Total reward: -213.17877636224762 Explore P: 0.0141 SOC: 0.5960 Cumulative_SOC_deviation: 16.8679 Fuel Consumption: 44.4996\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for reward_factor in reward_factors: \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {i}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 10))\n",
    "# for size, history in results_dict.items(): \n",
    "#     plt.plot(history, label=size, linewidth=3.0) \n",
    "\n",
    "\n",
    "# plt.grid() \n",
    "# plt.legend(fontsize=20)\n",
    "# plt.xlabel(\"episode number\", fontsize=20) \n",
    "# plt.ylabel(\"total rewards\", fontsize=20) \n",
    "# plt.xlim([0, 120])\n",
    "\n",
    "\n",
    "# plt.savefig(\"replay_memory_size_effect_300.png\")\n",
    "with open(\"results_scaling_2.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

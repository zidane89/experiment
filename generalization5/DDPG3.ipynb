{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import glob\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG1 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_nimh_6_240_panasonic_MY01_Prius.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_pm_95_145_X2.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):      \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    inputs_batchnorm = layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs_batchnorm)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_input_batchnorm = layers.BatchNormalization()(state_input)\n",
    "    \n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input_batchnorm)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 400\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 10000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(actor_model, critic_model, target_actor, target_critic, root): \n",
    "    actor_model.save_weights(\"./{}/actor_model_checkpoint\".format(root))\n",
    "    critic_model.save_weights(\"./{}/critic_model_checkpoint\".format(root))\n",
    "    target_actor.save_weights(\"./{}/target_actor_checkpoint\".format(root))\n",
    "    target_critic.save_weights(\"./{}/target_critic_checkpoint\".format(root))\n",
    "    print(\"model is saved..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_env(driving_path, reward_factor):\n",
    "    env = Environment(cell_model, driving_path, battery_path, motor_path, reward_factor)\n",
    "    return env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(actor_model, reward_factor, test_path_start):\n",
    "    test_cycles = glob.glob(\"../data/driving_cycles/city/*.mat\")[test_path_start:]\n",
    "    test_cycle = np.random.choice(test_cycles)\n",
    "    env = initialization_env(test_cycle, reward_factor)\n",
    "    \n",
    "    total_reward = 0\n",
    "    state = env.reset() \n",
    "    while True: \n",
    "        tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "        action = policy_epsilon_greedy(tf_state, -1)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        state = next_state \n",
    "        total_reward += reward \n",
    "        \n",
    "        if done: \n",
    "            break \n",
    "        \n",
    "    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "    \n",
    "    print(\"******************* Test is start *****************\")\n",
    "    print(test_cycle)\n",
    "    print('Total reward: {}'.format(total_reward), \n",
    "          \"SOC: {:.4f}\".format(env.SOC), \n",
    "          \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "          \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption))\n",
    "    print(\"******************* Test is done *****************\")\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Trial 0\n",
      "\n",
      "../data/driving_cycles/city\\ny_city_composite_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 12.536\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -3678.4126318791773 SOC: 1.0000 Cumulative_SOC_deviation: 356.3951 Fuel Consumption: 114.4618\n",
      "\n",
      "../data/driving_cycles/city\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 12.828\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -3680.6808161505264 SOC: 1.0000 Cumulative_SOC_deviation: 355.9992 Fuel Consumption: 120.6886\n",
      "\n",
      "../data/driving_cycles/city\\nuremberg_r36.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 13.539\n",
      "Episode: 3 Exploration P: 1.0000 Total reward: -3904.648992895813 SOC: 1.0000 Cumulative_SOC_deviation: 378.5172 Fuel Consumption: 119.4768\n",
      "\n",
      "../data/driving_cycles/city\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 23.066\n",
      "Episode: 4 Exploration P: 1.0000 Total reward: -6671.794015445364 SOC: 1.0000 Cumulative_SOC_deviation: 646.1876 Fuel Consumption: 209.9178\n",
      "\n",
      "../data/driving_cycles/city\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 16.461\n",
      "Episode: 5 Exploration P: 1.0000 Total reward: -4467.998627313784 SOC: 1.0000 Cumulative_SOC_deviation: 431.4597 Fuel Consumption: 153.4012\n",
      "\n",
      "../data/driving_cycles/city\\07_manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 13.028\n",
      "Episode: 6 Exploration P: 1.0000 Total reward: -3858.142716481762 SOC: 1.0000 Cumulative_SOC_deviation: 373.8722 Fuel Consumption: 119.4209\n",
      "\n",
      "../data/driving_cycles/city\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 22.683\n",
      "Episode: 7 Exploration P: 1.0000 Total reward: -6605.883020349262 SOC: 1.0000 Cumulative_SOC_deviation: 639.7887 Fuel Consumption: 207.9958\n",
      "\n",
      "../data/driving_cycles/city\\FTP_75_cycle.mat\n",
      "WARNING:tensorflow:Layer batch_normalization_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.138\n",
      "Episode: 8 Exploration P: 0.9754 Total reward: -6575.486361810903 SOC: 1.0000 Cumulative_SOC_deviation: 636.9309 Fuel Consumption: 206.1772\n",
      "\n",
      "../data/driving_cycles/city\\ny_city_composite_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 59.073\n",
      "Episode: 9 Exploration P: 0.9557 Total reward: -3661.1778377080836 SOC: 1.0000 Cumulative_SOC_deviation: 355.0715 Fuel Consumption: 110.4626\n",
      "\n",
      "../data/driving_cycles/city\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 63.790\n",
      "Episode: 10 Exploration P: 0.9353 Total reward: -3872.2520622093357 SOC: 1.0000 Cumulative_SOC_deviation: 375.6276 Fuel Consumption: 115.9762\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/city\\wvucity.mat\n",
      "Total reward: -1042.2947551629077 SOC: 0.4632 Cumulative_SOC_deviation: 103.8983 Fuel Consumption: 3.3120\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/city\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 109.518\n",
      "Episode: 11 Exploration P: 0.9012 Total reward: -6483.231265061939 SOC: 1.0000 Cumulative_SOC_deviation: 628.8899 Fuel Consumption: 194.3319\n",
      "\n",
      "../data/driving_cycles/city\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.358\n",
      "Episode: 12 Exploration P: 0.8771 Total reward: -4370.815765172206 SOC: 1.0000 Cumulative_SOC_deviation: 423.3118 Fuel Consumption: 137.6975\n",
      "\n",
      "../data/driving_cycles/city\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 111.811\n",
      "Episode: 13 Exploration P: 0.8452 Total reward: -6366.723926361876 SOC: 1.0000 Cumulative_SOC_deviation: 618.0462 Fuel Consumption: 186.2617\n",
      "\n",
      "../data/driving_cycles/city\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.701\n",
      "Episode: 14 Exploration P: 0.8226 Total reward: -4151.557984933772 SOC: 1.0000 Cumulative_SOC_deviation: 402.1704 Fuel Consumption: 129.8542\n",
      "\n",
      "../data/driving_cycles/city\\ny_city_composite_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.410\n",
      "Episode: 15 Exploration P: 0.8060 Total reward: -3569.1852025213298 SOC: 1.0000 Cumulative_SOC_deviation: 347.8288 Fuel Consumption: 90.8976\n",
      "\n",
      "../data/driving_cycles/city\\ny_city_composite_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 61.063\n",
      "Episode: 16 Exploration P: 0.7898 Total reward: -3528.7249966517734 SOC: 1.0000 Cumulative_SOC_deviation: 343.8040 Fuel Consumption: 90.6850\n",
      "\n",
      "../data/driving_cycles/city\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 64.217\n",
      "Episode: 17 Exploration P: 0.7734 Total reward: -3328.970204100223 SOC: 1.0000 Cumulative_SOC_deviation: 323.2012 Fuel Consumption: 96.9578\n",
      "\n",
      "../data/driving_cycles/city\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.234\n",
      "Episode: 18 Exploration P: 0.7569 Total reward: -3680.0974402847282 SOC: 1.0000 Cumulative_SOC_deviation: 358.5252 Fuel Consumption: 94.8453\n",
      "\n",
      "../data/driving_cycles/city\\nuremberg_r36.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.635\n",
      "Episode: 19 Exploration P: 0.7409 Total reward: -3681.5427358364395 SOC: 1.0000 Cumulative_SOC_deviation: 358.8836 Fuel Consumption: 92.7063\n",
      "\n",
      "../data/driving_cycles/city\\ny_city_composite_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 63.506\n",
      "Episode: 20 Exploration P: 0.7260 Total reward: -3500.6632196788187 SOC: 1.0000 Cumulative_SOC_deviation: 341.2047 Fuel Consumption: 88.6165\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/city\\wvucity.mat\n",
      "Total reward: -1042.2947551629077 SOC: 0.4632 Cumulative_SOC_deviation: 103.8983 Fuel Consumption: 3.3120\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/city\\manhattan.mat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 68.127\n",
      "Episode: 21 Exploration P: 0.7106 Total reward: -3640.4647599238724 SOC: 1.0000 Cumulative_SOC_deviation: 354.8849 Fuel Consumption: 91.6161\n",
      "\n",
      "../data/driving_cycles/city\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 109.836\n",
      "Episode: 22 Exploration P: 0.6848 Total reward: -5653.930601981522 SOC: 1.0000 Cumulative_SOC_deviation: 550.5653 Fuel Consumption: 148.2779\n",
      "\n",
      "../data/driving_cycles/city\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 110.577\n",
      "Episode: 23 Exploration P: 0.6600 Total reward: -5689.403607316033 SOC: 1.0000 Cumulative_SOC_deviation: 553.8069 Fuel Consumption: 151.3348\n",
      "\n",
      "../data/driving_cycles/city\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 63.716\n",
      "Episode: 24 Exploration P: 0.6463 Total reward: -2919.884837583574 SOC: 1.0000 Cumulative_SOC_deviation: 283.9519 Fuel Consumption: 80.3663\n",
      "\n",
      "../data/driving_cycles/city\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 112.222\n",
      "Episode: 25 Exploration P: 0.6229 Total reward: -4888.605002745208 SOC: 1.0000 Cumulative_SOC_deviation: 475.5287 Fuel Consumption: 133.3183\n",
      "\n",
      "../data/driving_cycles/city\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.103\n",
      "Episode: 26 Exploration P: 0.6063 Total reward: -3038.149206097337 SOC: 1.0000 Cumulative_SOC_deviation: 294.2004 Fuel Consumption: 96.1455\n",
      "\n",
      "../data/driving_cycles/city\\07_manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 63.865\n",
      "Episode: 27 Exploration P: 0.5934 Total reward: -3441.028968538136 SOC: 1.0000 Cumulative_SOC_deviation: 336.6970 Fuel Consumption: 74.0593\n",
      "\n",
      "../data/driving_cycles/city\\07_manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.227\n",
      "Episode: 28 Exploration P: 0.5809 Total reward: -3567.2687460415964 SOC: 1.0000 Cumulative_SOC_deviation: 349.1127 Fuel Consumption: 76.1422\n",
      "\n",
      "../data/driving_cycles/city\\nuremberg_r36.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.132\n",
      "Episode: 29 Exploration P: 0.5686 Total reward: -3484.930052502286 SOC: 1.0000 Cumulative_SOC_deviation: 341.1539 Fuel Consumption: 73.3912\n",
      "\n",
      "../data/driving_cycles/city\\nuremberg_r36.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 64.912\n",
      "Episode: 30 Exploration P: 0.5567 Total reward: -3335.6255245986663 SOC: 1.0000 Cumulative_SOC_deviation: 326.4208 Fuel Consumption: 71.4175\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/city\\VITO_RW_Kangoo_DePost_Brussels_101_1.mat\n",
      "Total reward: -3834.1646647404073 SOC: 0.3629 Cumulative_SOC_deviation: 382.7844 Fuel Consumption: 6.3208\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/city\\FTP_75_cycle.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 110.509\n",
      "Episode: 31 Exploration P: 0.5366 Total reward: -4280.872355877282 SOC: 0.9563 Cumulative_SOC_deviation: 416.1419 Fuel Consumption: 119.4532\n",
      "\n",
      "../data/driving_cycles/city\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 63.123\n",
      "Episode: 32 Exploration P: 0.5255 Total reward: -1765.5546383842666 SOC: 0.8300 Cumulative_SOC_deviation: 170.1754 Fuel Consumption: 63.8006\n",
      "\n",
      "../data/driving_cycles/city\\nuremberg_r36.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 64.429\n",
      "Episode: 33 Exploration P: 0.5145 Total reward: -3265.4592815685505 SOC: 1.0000 Cumulative_SOC_deviation: 320.0687 Fuel Consumption: 64.7722\n",
      "\n",
      "../data/driving_cycles/city\\07_manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 63.564\n",
      "Episode: 34 Exploration P: 0.5036 Total reward: -3242.4136485635436 SOC: 1.0000 Cumulative_SOC_deviation: 318.0484 Fuel Consumption: 61.9296\n",
      "\n",
      "../data/driving_cycles/city\\ny_city_composite_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 62.307\n",
      "Episode: 35 Exploration P: 0.4935 Total reward: -3069.0540743240736 SOC: 1.0000 Cumulative_SOC_deviation: 300.7225 Fuel Consumption: 61.8287\n",
      "\n",
      "../data/driving_cycles/city\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 63.480\n",
      "Episode: 36 Exploration P: 0.4834 Total reward: -1631.3251813908262 SOC: 0.8144 Cumulative_SOC_deviation: 156.7806 Fuel Consumption: 63.5191\n",
      "\n",
      "../data/driving_cycles/city\\manhattan.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 64.232\n",
      "Episode: 37 Exploration P: 0.4732 Total reward: -3175.0192273508183 SOC: 1.0000 Cumulative_SOC_deviation: 311.5428 Fuel Consumption: 59.5909\n",
      "\n",
      "../data/driving_cycles/city\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 63.564\n",
      "Episode: 38 Exploration P: 0.4634 Total reward: -1357.5572284710859 SOC: 0.7565 Cumulative_SOC_deviation: 129.8934 Fuel Consumption: 58.6235\n",
      "\n",
      "../data/driving_cycles/city\\06_udds_truck.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 63.936\n",
      "Episode: 39 Exploration P: 0.4539 Total reward: -1510.6679921192283 SOC: 0.7845 Cumulative_SOC_deviation: 144.9890 Fuel Consumption: 60.7783\n",
      "\n",
      "../data/driving_cycles/city\\01_FTP72_fuds.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.724\n",
      "Episode: 40 Exploration P: 0.4419 Total reward: -1688.4035152880758 SOC: 0.9528 Cumulative_SOC_deviation: 161.2492 Fuel Consumption: 75.9111\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "******************* Test is start *****************\n",
      "../data/driving_cycles/city\\VITO_RW_Kangoo_DePost_Brussels_101_1.mat\n",
      "Total reward: -3834.1646647404073 SOC: 0.3629 Cumulative_SOC_deviation: 382.7844 Fuel Consumption: 6.3208\n",
      "******************* Test is done *****************\n",
      "\n",
      "../data/driving_cycles/city\\nuremberg_r36.mat\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 65.353\n",
      "Episode: 41 Exploration P: 0.4327 Total reward: -3028.7754654576743 SOC: 1.0000 Cumulative_SOC_deviation: 296.9499 Fuel Consumption: 59.2762\n",
      "\n",
      "../data/driving_cycles/city\\FTP_75_cycle.mat\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 1\n",
    "results_dict = {} \n",
    "driving_cycle_paths = glob.glob(\"../data/driving_cycles/city/*.mat\")[:7]\n",
    "\n",
    "for trial in range(num_trials): \n",
    "    print(\"\")\n",
    "    print(\"Trial {}\".format(trial))\n",
    "    print(\"\")\n",
    "    \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer = initialization()\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    for ep in range(total_episodes): \n",
    "        driving_cycle_path = np.random.choice(driving_cycle_paths)\n",
    "        print(driving_cycle_path)\n",
    "        env = initialization_env(driving_cycle_path, 10)\n",
    "        \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * (steps\n",
    "                                                                        -DELAY_TRAINING))\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "        )\n",
    "        print(\"\")\n",
    "        \n",
    "        if (ep + 1) % 10 == 0: \n",
    "            test_agent(actor_model, 10, 7)\n",
    "    \n",
    "    root = \"DDPG3_trial{}\".format(trial+1)\n",
    "    save_weights(actor_model, critic_model, target_actor, target_critic, root)\n",
    "    \n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG3.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

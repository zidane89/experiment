{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG5 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):      \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    inputs_batchnorm = layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs_batchnorm)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_input_batchnorm = layers.BatchNormalization()(state_input)\n",
    "    \n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input_batchnorm)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 200\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(reward_factor): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer, env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(actor_model, critic_model, target_actor, target_critic, root): \n",
    "    actor_model.save_weights(\"./{}/actor_model_checkpoint\".format(root))\n",
    "    critic_model.save_weights(\"./{}/critic_model_checkpoint\".format(root))\n",
    "    target_actor.save_weights(\"./{}/target_actor_checkpoint\".format(root))\n",
    "    target_critic.save_weights(\"./{}/target_critic_checkpoint\".format(root))\n",
    "    print(\"model is saved..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Trial 0\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 16.687\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -235.05211281757136 SOC: 0.7945 Cumulative_SOC_deviation: 88.0117 Fuel Consumption: 60.5107\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 17.063\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -289.86154843690184 SOC: 0.8238 Cumulative_SOC_deviation: 98.3217 Fuel Consumption: 62.9928\n",
      "WARNING:tensorflow:Layer batch_normalization_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer batch_normalization is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.877\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -167.03281448666513 SOC: 0.7500 Cumulative_SOC_deviation: 70.2388 Fuel Consumption: 57.0793\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 130.145\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -172.26146060669845 SOC: 0.7272 Cumulative_SOC_deviation: 73.2015 Fuel Consumption: 55.2801\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 113.346\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -162.6065533102705 SOC: 0.7276 Cumulative_SOC_deviation: 71.6658 Fuel Consumption: 55.4363\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 141.595\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -166.57470323354096 SOC: 0.6950 Cumulative_SOC_deviation: 74.2728 Fuel Consumption: 52.7990\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 145.064\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -181.17994985683896 SOC: 0.6675 Cumulative_SOC_deviation: 77.7170 Fuel Consumption: 50.6443\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 107.417\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -182.94410621308626 SOC: 0.6746 Cumulative_SOC_deviation: 77.9926 Fuel Consumption: 51.1653\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 109.840\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -200.19015806773837 SOC: 0.6452 Cumulative_SOC_deviation: 83.2849 Fuel Consumption: 49.1476\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 128.582\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -176.44655595766395 SOC: 0.6459 Cumulative_SOC_deviation: 76.8963 Fuel Consumption: 49.0887\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 125.946\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -271.36777600352815 SOC: 0.5996 Cumulative_SOC_deviation: 106.4407 Fuel Consumption: 45.4083\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.951\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -235.5924939352342 SOC: 0.6078 Cumulative_SOC_deviation: 96.3653 Fuel Consumption: 46.1080\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 133.132\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -362.5457752276209 SOC: 0.5729 Cumulative_SOC_deviation: 129.8657 Fuel Consumption: 43.3938\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.875\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -264.0110493230103 SOC: 0.5748 Cumulative_SOC_deviation: 109.5267 Fuel Consumption: 43.5671\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.843\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -409.5393648551397 SOC: 0.5359 Cumulative_SOC_deviation: 143.2084 Fuel Consumption: 40.5747\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 158.633\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -367.5321892932234 SOC: 0.5478 Cumulative_SOC_deviation: 136.1967 Fuel Consumption: 41.5943\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 180.702\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -497.4486567944907 SOC: 0.5290 Cumulative_SOC_deviation: 161.1528 Fuel Consumption: 40.2270\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.497\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -549.6146646817965 SOC: 0.5019 Cumulative_SOC_deviation: 171.4551 Fuel Consumption: 38.1257\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.533\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -605.0395923351296 SOC: 0.4863 Cumulative_SOC_deviation: 181.1362 Fuel Consumption: 36.9991\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 110.770\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -576.8721935905666 SOC: 0.4778 Cumulative_SOC_deviation: 176.7116 Fuel Consumption: 36.1347\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 113.806\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -640.4813759135335 SOC: 0.4806 Cumulative_SOC_deviation: 186.6152 Fuel Consumption: 36.7285\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 111.951\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -716.2328258207684 SOC: 0.4627 Cumulative_SOC_deviation: 198.3222 Fuel Consumption: 35.1579\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 109.635\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -908.7791951319354 SOC: 0.4351 Cumulative_SOC_deviation: 224.8957 Fuel Consumption: 33.2995\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 105.662\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -909.2377080313016 SOC: 0.4317 Cumulative_SOC_deviation: 226.0173 Fuel Consumption: 33.0192\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 113.689\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -850.0462584300612 SOC: 0.4344 Cumulative_SOC_deviation: 217.6879 Fuel Consumption: 33.0674\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 116.611\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -879.2952032082521 SOC: 0.4176 Cumulative_SOC_deviation: 221.3074 Fuel Consumption: 31.8670\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 109.758\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -1010.8429739152222 SOC: 0.3997 Cumulative_SOC_deviation: 237.9483 Fuel Consumption: 30.5649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.482\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -1100.6972983777798 SOC: 0.3817 Cumulative_SOC_deviation: 248.4510 Fuel Consumption: 29.2201\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 126.193\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -1264.495966763679 SOC: 0.3696 Cumulative_SOC_deviation: 267.1054 Fuel Consumption: 28.4905\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.897\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -1120.5884938405343 SOC: 0.3757 Cumulative_SOC_deviation: 251.4934 Fuel Consumption: 29.1153\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.994\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -1285.802506827862 SOC: 0.3580 Cumulative_SOC_deviation: 270.0326 Fuel Consumption: 27.5104\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 107.360\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -1283.0892054970711 SOC: 0.3684 Cumulative_SOC_deviation: 268.9192 Fuel Consumption: 28.6595\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 111.893\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -1407.7335067262056 SOC: 0.3467 Cumulative_SOC_deviation: 282.2752 Fuel Consumption: 26.8386\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 110.345\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -1434.8752216190599 SOC: 0.3360 Cumulative_SOC_deviation: 283.8183 Fuel Consumption: 26.1710\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 137.569\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -1621.8757218185453 SOC: 0.3087 Cumulative_SOC_deviation: 303.0713 Fuel Consumption: 24.1617\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 148.301\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -1619.0251897124374 SOC: 0.3044 Cumulative_SOC_deviation: 301.0744 Fuel Consumption: 23.8857\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 148.092\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -1706.153231651157 SOC: 0.3083 Cumulative_SOC_deviation: 312.0219 Fuel Consumption: 24.4110\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 125.067\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -1752.3947783598057 SOC: 0.2895 Cumulative_SOC_deviation: 315.2918 Fuel Consumption: 22.8265\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 117.623\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -1986.4104896533097 SOC: 0.2587 Cumulative_SOC_deviation: 335.1071 Fuel Consumption: 20.6868\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 110.495\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -1890.6427433365147 SOC: 0.2726 Cumulative_SOC_deviation: 325.5808 Fuel Consumption: 21.7235\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 111.193\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -1743.9206150739683 SOC: 0.2828 Cumulative_SOC_deviation: 313.1570 Fuel Consumption: 22.4318\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 122.033\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -346.26247653185953 SOC: 0.5127 Cumulative_SOC_deviation: 133.2925 Fuel Consumption: 38.3186\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 124.811\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -268.9871594522151 SOC: 0.5304 Cumulative_SOC_deviation: 116.8414 Fuel Consumption: 38.9268\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 109.774\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -257.297509334498 SOC: 0.5223 Cumulative_SOC_deviation: 114.2954 Fuel Consumption: 38.5294\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 112.913\n",
      "Episode: 45 Exploration P: 0.2977 Total reward: -310.55809663310316 SOC: 0.5255 Cumulative_SOC_deviation: 127.1316 Fuel Consumption: 38.5543\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 113.667\n",
      "Episode: 46 Exploration P: 0.2899 Total reward: -264.19225571456775 SOC: 0.5218 Cumulative_SOC_deviation: 115.8499 Fuel Consumption: 38.3026\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 113.579\n",
      "Episode: 47 Exploration P: 0.2824 Total reward: -240.7669521238338 SOC: 0.5347 Cumulative_SOC_deviation: 108.1933 Fuel Consumption: 39.3999\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 127.639\n",
      "Episode: 48 Exploration P: 0.2750 Total reward: -283.1352290779909 SOC: 0.5107 Cumulative_SOC_deviation: 121.9800 Fuel Consumption: 37.5015\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 116.461\n",
      "Episode: 49 Exploration P: 0.2678 Total reward: -262.73407106915994 SOC: 0.5231 Cumulative_SOC_deviation: 113.9489 Fuel Consumption: 38.7662\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.049\n",
      "Episode: 50 Exploration P: 0.2608 Total reward: -286.7157824012443 SOC: 0.5074 Cumulative_SOC_deviation: 121.8951 Fuel Consumption: 37.7013\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 128.093\n",
      "Episode: 51 Exploration P: 0.2540 Total reward: -305.3651212122057 SOC: 0.5119 Cumulative_SOC_deviation: 126.8409 Fuel Consumption: 37.9133\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 115.465\n",
      "Episode: 52 Exploration P: 0.2474 Total reward: -257.367415356365 SOC: 0.5140 Cumulative_SOC_deviation: 113.0556 Fuel Consumption: 38.2464\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 126.060\n",
      "Episode: 53 Exploration P: 0.2410 Total reward: -310.44816827393265 SOC: 0.5046 Cumulative_SOC_deviation: 128.2438 Fuel Consumption: 37.3890\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 111.224\n",
      "Episode: 54 Exploration P: 0.2347 Total reward: -273.88038556269277 SOC: 0.5242 Cumulative_SOC_deviation: 117.4791 Fuel Consumption: 38.9905\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 120.060\n",
      "Episode: 55 Exploration P: 0.2286 Total reward: -289.4218288641605 SOC: 0.5343 Cumulative_SOC_deviation: 120.6640 Fuel Consumption: 39.6656\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 126.577\n",
      "Episode: 56 Exploration P: 0.2227 Total reward: -257.051681794267 SOC: 0.5048 Cumulative_SOC_deviation: 113.2818 Fuel Consumption: 37.8035\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 128.908\n",
      "Episode: 57 Exploration P: 0.2170 Total reward: -303.12423052267945 SOC: 0.5033 Cumulative_SOC_deviation: 126.4884 Fuel Consumption: 37.3278\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 111.115\n",
      "Episode: 58 Exploration P: 0.2114 Total reward: -319.1818564144549 SOC: 0.4969 Cumulative_SOC_deviation: 131.6120 Fuel Consumption: 36.6890\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 129.369\n",
      "Episode: 59 Exploration P: 0.2059 Total reward: -349.657277130479 SOC: 0.5011 Cumulative_SOC_deviation: 137.3476 Fuel Consumption: 37.2618\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 115.152\n",
      "Episode: 60 Exploration P: 0.2006 Total reward: -333.91238595028716 SOC: 0.4986 Cumulative_SOC_deviation: 133.7754 Fuel Consumption: 37.0549\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 109.410\n",
      "Episode: 61 Exploration P: 0.1954 Total reward: -339.2044634397676 SOC: 0.4974 Cumulative_SOC_deviation: 134.9276 Fuel Consumption: 36.8679\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 146.914\n",
      "Episode: 62 Exploration P: 0.1904 Total reward: -324.9122873050661 SOC: 0.5090 Cumulative_SOC_deviation: 132.0943 Fuel Consumption: 37.7907\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 115.163\n",
      "Episode: 63 Exploration P: 0.1855 Total reward: -348.8180444188046 SOC: 0.4965 Cumulative_SOC_deviation: 137.8067 Fuel Consumption: 36.7465\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 121.059\n",
      "Episode: 64 Exploration P: 0.1808 Total reward: -296.366586534153 SOC: 0.4969 Cumulative_SOC_deviation: 123.5945 Fuel Consumption: 36.9873\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 140.097\n",
      "Episode: 65 Exploration P: 0.1761 Total reward: -320.72984483225054 SOC: 0.4990 Cumulative_SOC_deviation: 129.8922 Fuel Consumption: 37.2605\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 138.959\n",
      "Episode: 66 Exploration P: 0.1716 Total reward: -358.46700153806785 SOC: 0.4934 Cumulative_SOC_deviation: 140.2553 Fuel Consumption: 36.5913\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 142.732\n",
      "Episode: 67 Exploration P: 0.1673 Total reward: -261.8878862199841 SOC: 0.5430 Cumulative_SOC_deviation: 111.5689 Fuel Consumption: 40.3684\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 143.001\n",
      "Episode: 68 Exploration P: 0.1630 Total reward: -354.430848176187 SOC: 0.4930 Cumulative_SOC_deviation: 139.4747 Fuel Consumption: 36.4175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 145.846\n",
      "Episode: 69 Exploration P: 0.1589 Total reward: -334.1382485692348 SOC: 0.4943 Cumulative_SOC_deviation: 133.0432 Fuel Consumption: 36.8857\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 129.914\n",
      "Episode: 70 Exploration P: 0.1548 Total reward: -251.36462723616154 SOC: 0.5480 Cumulative_SOC_deviation: 106.4037 Fuel Consumption: 40.8723\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 111.326\n",
      "Episode: 71 Exploration P: 0.1509 Total reward: -286.00793720510393 SOC: 0.4895 Cumulative_SOC_deviation: 120.4052 Fuel Consumption: 36.4762\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 109.000\n",
      "Episode: 72 Exploration P: 0.1471 Total reward: -290.53373381634935 SOC: 0.4899 Cumulative_SOC_deviation: 121.1940 Fuel Consumption: 36.6433\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 121.197\n",
      "Episode: 73 Exploration P: 0.1434 Total reward: -227.7896005241036 SOC: 0.5378 Cumulative_SOC_deviation: 100.5564 Fuel Consumption: 40.2005\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 130.619\n",
      "Episode: 74 Exploration P: 0.1398 Total reward: -209.97491192868554 SOC: 0.5508 Cumulative_SOC_deviation: 98.0888 Fuel Consumption: 40.9282\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 122.429\n",
      "Episode: 75 Exploration P: 0.1362 Total reward: -191.55762986996461 SOC: 0.5533 Cumulative_SOC_deviation: 90.1979 Fuel Consumption: 41.2268\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.445\n",
      "Episode: 76 Exploration P: 0.1328 Total reward: -308.4090772098999 SOC: 0.5454 Cumulative_SOC_deviation: 124.8718 Fuel Consumption: 40.4635\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-640315b47e9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mDELAY_TRAINING\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                 \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m                 \u001b[0mupdate_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMIN_EPSILON\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mMAX_EPSILON\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mMIN_EPSILON\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mDECAY_RATE\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-4679c004126e>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactor_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mcritic_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcritic_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0mactor_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mactor_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    706\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m    707\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[1;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m           \u001b[1;31m# Compute outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 860\u001b[1;33m           \u001b[0moutput_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m           \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1056\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1058\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1059\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[1;34m(value, bias, data_format, name)\u001b[0m\n\u001b[0;32m   2716\u001b[0m       \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"input\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m       \u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"bias\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2718\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[1;34m(value, bias, data_format, name)\u001b[0m\n\u001b[0;32m    740\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"BiasAdd\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data_format\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m         data_format)\n\u001b[0m\u001b[0;32m    743\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 3\n",
    "reward_factor = 30\n",
    "results_dict = {} \n",
    "for trial in range(num_trials): \n",
    "    print()\n",
    "    print(\"Trial {}\".format(trial))\n",
    "    \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer, env = initialization(\n",
    "        reward_factor\n",
    "    )\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    for ep in range(total_episodes): \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "        )\n",
    "    \n",
    "    root = \"DDPG5_trial{}\".format(trial+1)\n",
    "    save_weights(actor_model, critic_model, target_actor, target_critic, root)\n",
    "    \n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG5.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "\n",
    "from vehicle_model_DDQN1 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATE_SIZE = env.calculation_comp[\"state_size\"]\n",
    "STATE_SIZE = 4\n",
    "ACTION_SIZE = env.calculation_comp[\"action_size\"] \n",
    "LEARNING_RATE = 0.00025 \n",
    "\n",
    "TOTAL_EPISODES = 200\n",
    "MAX_STEPS = 50000 \n",
    "\n",
    "GAMMA = 0.95 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "TAU = 0.001 \n",
    "DELAY_TRAINING = 3000 \n",
    "EPSILON_MIN_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "target_network = keras.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()), \n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#     keras.layers.BatchNormalization(), \n",
    "    keras.layers.Dense(ACTION_SIZE),\n",
    "])\n",
    "\n",
    "primary_network.compile(\n",
    "    loss=\"mse\", \n",
    "    optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    ")\n",
    "\n",
    "# for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "#     t.assign(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_network(primary_network, target_network): \n",
    "    for t, p in zip(target_network.trainable_variables, primary_network.trainable_variables): \n",
    "        t.assign(t * (1 - TAU) + p * TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory: \n",
    "    def __init__(self, max_memory): \n",
    "        self.max_memory = max_memory \n",
    "        self._samples = [] \n",
    "        \n",
    "    def add_sample(self, sample): \n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self.max_memory: \n",
    "            self._samples.pop(0)\n",
    "        \n",
    "    def sample(self, no_samples): \n",
    "        if no_samples > len(self._samples): \n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else: \n",
    "            return random.sample(self._samples, no_samples)\n",
    "    \n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)\n",
    "    \n",
    "\n",
    "# memory = Memory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, primary_network, eps): \n",
    "    if random.random() < eps: \n",
    "        return random.randint(0, ACTION_SIZE - 1)\n",
    "    else: \n",
    "        return np.argmax(primary_network(np.array(state).reshape(1, -1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(primary_network, target_network, memory): \n",
    "    batch = memory.sample(BATCH_SIZE)\n",
    "    states = np.array([val[0] for val in batch]) \n",
    "    actions = np.array([val[1] for val in batch])\n",
    "    rewards = np.array([val[2] for val in batch])\n",
    "    next_states = np.array([np.zeros(STATE_SIZE) if val[3] is None else val[3]  \n",
    "                            for val in batch])\n",
    "    \n",
    "    prim_qt = primary_network(states)\n",
    "    prim_qtp1 = primary_network(next_states)\n",
    "    target_q = prim_qt.numpy() \n",
    "    updates = rewards \n",
    "    valid_idxs = next_states.sum(axis=1) != 0 \n",
    "    batch_idxs = np.arange(BATCH_SIZE)\n",
    "    prim_action_tp1 = np.argmax(prim_qtp1.numpy(), axis=1)\n",
    "    q_from_target = target_network(next_states)\n",
    "    updates[valid_idxs] += GAMMA * q_from_target.numpy()[batch_idxs[valid_idxs], \n",
    "                                                        prim_action_tp1[valid_idxs]]\n",
    "    \n",
    "    target_q[batch_idxs, actions] = updates \n",
    "    loss = primary_network.train_on_batch(states, target_q)\n",
    "    return loss \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_with_rewardFactor(reward_factor):\n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    \n",
    "    memory = Memory(10000)\n",
    "    \n",
    "    primary_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", input_shape=[STATE_SIZE], \n",
    "                           kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(),  \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    target_network = keras.Sequential([\n",
    "        keras.layers.Dense(30, activation=\"relu\", input_shape=[STATE_SIZE], \n",
    "                           kernel_initializer=keras.initializers.he_normal()), \n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(30, activation=\"relu\", kernel_initializer=keras.initializers.he_normal()),\n",
    "#         keras.layers.BatchNormalization(), \n",
    "        keras.layers.Dense(ACTION_SIZE),\n",
    "    ])\n",
    "    primary_network.compile(\n",
    "        loss=\"mse\", \n",
    "        optimizer=keras.optimizers.Adam(lr=LEARNING_RATE) \n",
    "    )\n",
    "    return env, memory, primary_network, target_network \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(primary_net, target_net, root): \n",
    "    primary_net.save_weights(\"./{}/primary_net_checkpoint\".format(root))\n",
    "    target_net.save_weights(\"./{}/target_net_checkpoint\".format(root))\n",
    "    print(\"model is saved..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment version: 1\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 0\n",
      "maximum steps, simulation is done ... \n",
      "Pre-training...Episode: 1\n",
      "WARNING:tensorflow:Layer dense_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 3 Total reward: -847.717683777498 Explore P: 0.9217 SOC: 0.7783 Cumulative_SOC_deviation: 78.8739 Fuel Consumption: 58.9787\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 4 Total reward: -939.4967558413841 Explore P: 0.8970 SOC: 0.7876 Cumulative_SOC_deviation: 87.9645 Fuel Consumption: 59.8519\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 5 Total reward: -840.5190605289966 Explore P: 0.8730 SOC: 0.7692 Cumulative_SOC_deviation: 78.2198 Fuel Consumption: 58.3214\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 6 Total reward: -845.4770069373682 Explore P: 0.8496 SOC: 0.7663 Cumulative_SOC_deviation: 78.7211 Fuel Consumption: 58.2662\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 7 Total reward: -894.9185748442262 Explore P: 0.8269 SOC: 0.7748 Cumulative_SOC_deviation: 83.6112 Fuel Consumption: 58.8064\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 8 Total reward: -870.0391754715092 Explore P: 0.8048 SOC: 0.7796 Cumulative_SOC_deviation: 81.0629 Fuel Consumption: 59.4100\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 9 Total reward: -775.9590595299857 Explore P: 0.7832 SOC: 0.7534 Cumulative_SOC_deviation: 71.8865 Fuel Consumption: 57.0940\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 10 Total reward: -850.9757158003997 Explore P: 0.7623 SOC: 0.7644 Cumulative_SOC_deviation: 79.2821 Fuel Consumption: 58.1547\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 11 Total reward: -825.5761011363967 Explore P: 0.7419 SOC: 0.7590 Cumulative_SOC_deviation: 76.7916 Fuel Consumption: 57.6601\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 12 Total reward: -729.3645486377118 Explore P: 0.7221 SOC: 0.7380 Cumulative_SOC_deviation: 67.3476 Fuel Consumption: 55.8889\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 13 Total reward: -706.4065016038682 Explore P: 0.7028 SOC: 0.7267 Cumulative_SOC_deviation: 65.1437 Fuel Consumption: 54.9696\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 14 Total reward: -659.7117099492576 Explore P: 0.6840 SOC: 0.7055 Cumulative_SOC_deviation: 60.6501 Fuel Consumption: 53.2105\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 15 Total reward: -713.2591912053289 Explore P: 0.6658 SOC: 0.6730 Cumulative_SOC_deviation: 66.2816 Fuel Consumption: 50.4430\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 16 Total reward: -766.6676820798491 Explore P: 0.6480 SOC: 0.6760 Cumulative_SOC_deviation: 71.5861 Fuel Consumption: 50.8064\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 17 Total reward: -788.8224122655564 Explore P: 0.6307 SOC: 0.6785 Cumulative_SOC_deviation: 73.7907 Fuel Consumption: 50.9158\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 18 Total reward: -706.271692800558 Explore P: 0.6139 SOC: 0.6697 Cumulative_SOC_deviation: 65.6078 Fuel Consumption: 50.1937\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 19 Total reward: -698.7338433170884 Explore P: 0.5976 SOC: 0.6806 Cumulative_SOC_deviation: 64.7836 Fuel Consumption: 50.8976\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 20 Total reward: -776.7525459568586 Explore P: 0.5816 SOC: 0.6657 Cumulative_SOC_deviation: 72.6766 Fuel Consumption: 49.9864\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 21 Total reward: -729.1172884251529 Explore P: 0.5662 SOC: 0.6685 Cumulative_SOC_deviation: 67.9228 Fuel Consumption: 49.8891\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 22 Total reward: -751.7453842209125 Explore P: 0.5511 SOC: 0.6516 Cumulative_SOC_deviation: 70.2944 Fuel Consumption: 48.8015\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 23 Total reward: -784.5868128160171 Explore P: 0.5364 SOC: 0.6463 Cumulative_SOC_deviation: 73.6317 Fuel Consumption: 48.2699\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 24 Total reward: -720.062421560207 Explore P: 0.5222 SOC: 0.6538 Cumulative_SOC_deviation: 67.1092 Fuel Consumption: 48.9708\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 25 Total reward: -869.8173113914293 Explore P: 0.5083 SOC: 0.6281 Cumulative_SOC_deviation: 82.3009 Fuel Consumption: 46.8084\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 26 Total reward: -795.6708527939112 Explore P: 0.4948 SOC: 0.6198 Cumulative_SOC_deviation: 74.9270 Fuel Consumption: 46.4004\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 27 Total reward: -673.580764343639 Explore P: 0.4817 SOC: 0.6687 Cumulative_SOC_deviation: 62.3291 Fuel Consumption: 50.2894\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 28 Total reward: -748.7951991136891 Explore P: 0.4689 SOC: 0.6410 Cumulative_SOC_deviation: 70.0760 Fuel Consumption: 48.0353\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 29 Total reward: -712.0155469323382 Explore P: 0.4565 SOC: 0.6495 Cumulative_SOC_deviation: 66.3380 Fuel Consumption: 48.6353\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 30 Total reward: -718.1945034696821 Explore P: 0.4444 SOC: 0.6706 Cumulative_SOC_deviation: 66.7803 Fuel Consumption: 50.3913\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 31 Total reward: -670.2238986593993 Explore P: 0.4326 SOC: 0.6626 Cumulative_SOC_deviation: 62.0799 Fuel Consumption: 49.4254\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 32 Total reward: -856.8200088696958 Explore P: 0.4212 SOC: 0.6597 Cumulative_SOC_deviation: 80.7321 Fuel Consumption: 49.4989\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 33 Total reward: -704.9392155927648 Explore P: 0.4100 SOC: 0.6622 Cumulative_SOC_deviation: 65.5251 Fuel Consumption: 49.6884\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 34 Total reward: -763.1426469789537 Explore P: 0.3992 SOC: 0.6395 Cumulative_SOC_deviation: 71.5180 Fuel Consumption: 47.9623\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 35 Total reward: -740.8331501397392 Explore P: 0.3887 SOC: 0.7025 Cumulative_SOC_deviation: 68.7776 Fuel Consumption: 53.0570\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 36 Total reward: -844.1118593141192 Explore P: 0.3784 SOC: 0.6729 Cumulative_SOC_deviation: 79.3265 Fuel Consumption: 50.8469\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 37 Total reward: -769.9794586792152 Explore P: 0.3684 SOC: 0.6119 Cumulative_SOC_deviation: 72.4194 Fuel Consumption: 45.7852\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 38 Total reward: -700.0648511975346 Explore P: 0.3587 SOC: 0.6348 Cumulative_SOC_deviation: 65.2491 Fuel Consumption: 47.5741\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 39 Total reward: -685.8391371683871 Explore P: 0.3493 SOC: 0.6348 Cumulative_SOC_deviation: 63.8404 Fuel Consumption: 47.4353\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 40 Total reward: -707.4611049436493 Explore P: 0.3401 SOC: 0.6508 Cumulative_SOC_deviation: 65.8666 Fuel Consumption: 48.7949\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 41 Total reward: -767.011674512984 Explore P: 0.3311 SOC: 0.6564 Cumulative_SOC_deviation: 71.8015 Fuel Consumption: 48.9966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "Episode: 42 Total reward: -733.277307582848 Explore P: 0.3224 SOC: 0.6360 Cumulative_SOC_deviation: 68.5646 Fuel Consumption: 47.6314\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 43 Total reward: -640.0878029956598 Explore P: 0.3140 SOC: 0.6674 Cumulative_SOC_deviation: 59.0292 Fuel Consumption: 49.7958\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 44 Total reward: -702.539169949364 Explore P: 0.3057 SOC: 0.6037 Cumulative_SOC_deviation: 65.7450 Fuel Consumption: 45.0894\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 45 Total reward: -685.1064180835925 Explore P: 0.2977 SOC: 0.6504 Cumulative_SOC_deviation: 63.6287 Fuel Consumption: 48.8193\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 46 Total reward: -524.0918253742685 Explore P: 0.2899 SOC: 0.6687 Cumulative_SOC_deviation: 47.4191 Fuel Consumption: 49.9007\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 47 Total reward: -676.2755549840443 Explore P: 0.2824 SOC: 0.7334 Cumulative_SOC_deviation: 62.1150 Fuel Consumption: 55.1257\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 48 Total reward: -752.7285625153993 Explore P: 0.2750 SOC: 0.6845 Cumulative_SOC_deviation: 70.1228 Fuel Consumption: 51.5001\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 49 Total reward: -666.8481421714049 Explore P: 0.2678 SOC: 0.6872 Cumulative_SOC_deviation: 61.5104 Fuel Consumption: 51.7439\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 50 Total reward: -616.7727185654514 Explore P: 0.2608 SOC: 0.6830 Cumulative_SOC_deviation: 56.5274 Fuel Consumption: 51.4991\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 51 Total reward: -587.9694812976027 Explore P: 0.2540 SOC: 0.6680 Cumulative_SOC_deviation: 53.7483 Fuel Consumption: 50.4861\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 52 Total reward: -610.275201385328 Explore P: 0.2474 SOC: 0.6263 Cumulative_SOC_deviation: 56.3555 Fuel Consumption: 46.7202\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 53 Total reward: -428.09921643556515 Explore P: 0.2410 SOC: 0.6387 Cumulative_SOC_deviation: 38.0228 Fuel Consumption: 47.8716\n",
      "maximum steps, simulation is done ... \n",
      "Episode: 54 Total reward: -648.524457602154 Explore P: 0.2347 SOC: 0.6376 Cumulative_SOC_deviation: 60.0940 Fuel Consumption: 47.5848\n"
     ]
    }
   ],
   "source": [
    "print(\"environment version: {}\".format(env.version)) \n",
    "\n",
    " \n",
    "reward_factors = [10]\n",
    "results_dict = {} \n",
    "\n",
    "for trial, reward_factor in enumerate(reward_factors): \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    \n",
    "    env, memory, primary_network, target_network = initialization_with_rewardFactor(reward_factor)\n",
    "    for episode in range(TOTAL_EPISODES): \n",
    "        state = env.reset() \n",
    "        avg_loss = 0 \n",
    "        total_reward = 0\n",
    "        cnt = 1 \n",
    "\n",
    "        while True:\n",
    "            action = choose_action(state, primary_network, eps)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward \n",
    "            if done: \n",
    "                next_state = None \n",
    "            memory.add_sample((state, action, reward, next_state))\n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                loss = train(primary_network, target_network, memory)\n",
    "                update_network(primary_network, target_network)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "            else: \n",
    "                loss = -1\n",
    "\n",
    "            avg_loss += loss \n",
    "            steps += 1 \n",
    "\n",
    "            if done: \n",
    "                if steps > DELAY_TRAINING: \n",
    "                    SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "                    avg_loss /= cnt \n",
    "                    print('Episode: {}'.format(episode + 1),\n",
    "                          'Total reward: {}'.format(total_reward), \n",
    "                          'Explore P: {:.4f}'.format(eps), \n",
    "                          \"SOC: {:.4f}\".format(env.SOC), \n",
    "                         \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "                         \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "                         )\n",
    "                else: \n",
    "                    print(f\"Pre-training...Episode: {episode}\")\n",
    "                \n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_SOCs.append(env.SOC)\n",
    "                episode_FCs.append(env.fuel_consumption)\n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "            cnt += 1 \n",
    "    \n",
    "    root = \"DDQN1_trial{}\".format(trial+1)\n",
    "    save_weights(primary_network, target_network, root)\n",
    "    \n",
    "    results_dict[reward_factor] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDQN1.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"results/replay_memory_size_effect.pkl\", \"rb\") as f: \n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

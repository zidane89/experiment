{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG10_2 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64): \n",
    "        self.power_mean = 0 \n",
    "        self.power_std = 0\n",
    "        self.sum = 0 \n",
    "        self.sum_deviation = 0 \n",
    "        self.N = 0 \n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        self.N += 1 \n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "        power = obs_tuple[0][0] \n",
    "        \n",
    "        self.sum += power \n",
    "        self.power_mean = self.sum / self.N \n",
    "        self.sum_deviation += (power - self.power_mean) ** 2  \n",
    "        self.power_std = np.sqrt(self.sum_deviation / self.N) \n",
    "            \n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        state_batch = self.state_buffer[batch_indices]\n",
    "        power_batch = (state_batch[:, 0] - self.power_mean) / self.power_std\n",
    "        state_batch[:, 0] = power_batch \n",
    "        \n",
    "        next_state_batch = self.next_state_buffer[batch_indices]\n",
    "        power_batch = (next_state_batch[:, 0] - self.power_mean) / self.power_std\n",
    "        next_state_batch[:, 0] = power_batch \n",
    "#         print(state_batch)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(state_batch)\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(next_state_batch)\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_input)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(reward_factor): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    \n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 22.015\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -152.57881443726922 SOC: 0.7981 Cumulative_SOC_deviation: 89.2214 Fuel Consumption: 60.9354\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 22.205\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -163.10822437025922 SOC: 0.8064 Cumulative_SOC_deviation: 94.1987 Fuel Consumption: 61.6758\n",
      "WARNING:tensorflow:Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_13 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 111.639\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -133.6098418063978 SOC: 0.7790 Cumulative_SOC_deviation: 80.9976 Fuel Consumption: 59.3793\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 112.595\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -115.16649150772564 SOC: 0.7411 Cumulative_SOC_deviation: 72.8686 Fuel Consumption: 56.3671\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.778\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -100.0231760023345 SOC: 0.7122 Cumulative_SOC_deviation: 64.0516 Fuel Consumption: 54.0240\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 110.221\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -112.31049929820878 SOC: 0.6817 Cumulative_SOC_deviation: 76.4746 Fuel Consumption: 51.7741\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 129.990\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -111.45832433584411 SOC: 0.6800 Cumulative_SOC_deviation: 74.4799 Fuel Consumption: 51.7302\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 131.350\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -110.95052453548335 SOC: 0.6611 Cumulative_SOC_deviation: 75.4569 Fuel Consumption: 50.2014\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 132.056\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -143.93280902565084 SOC: 0.6385 Cumulative_SOC_deviation: 94.2019 Fuel Consumption: 48.3580\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 131.210\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -137.96956180663614 SOC: 0.6168 Cumulative_SOC_deviation: 92.8838 Fuel Consumption: 46.8965\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 132.184\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -139.8395236891424 SOC: 0.6151 Cumulative_SOC_deviation: 94.9984 Fuel Consumption: 46.7115\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 138.851\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -232.34701344398226 SOC: 0.5556 Cumulative_SOC_deviation: 144.8760 Fuel Consumption: 42.1709\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 136.992\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -185.78994637797788 SOC: 0.5775 Cumulative_SOC_deviation: 123.0608 Fuel Consumption: 43.8346\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 135.241\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -180.58044505987903 SOC: 0.5837 Cumulative_SOC_deviation: 119.4797 Fuel Consumption: 44.2561\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 133.837\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -284.53272875327235 SOC: 0.5176 Cumulative_SOC_deviation: 167.3835 Fuel Consumption: 39.1303\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 133.935\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -224.99209737840766 SOC: 0.5423 Cumulative_SOC_deviation: 143.8978 Fuel Consumption: 41.2819\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.867\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -258.4920695347322 SOC: 0.5134 Cumulative_SOC_deviation: 159.0436 Fuel Consumption: 39.0437\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 120.362\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -286.40371104817746 SOC: 0.5162 Cumulative_SOC_deviation: 167.4955 Fuel Consumption: 39.2587\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 71.660\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -267.7424495884869 SOC: 0.4983 Cumulative_SOC_deviation: 163.1472 Fuel Consumption: 37.9781\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.456\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -348.8474194154318 SOC: 0.4784 Cumulative_SOC_deviation: 190.1137 Fuel Consumption: 36.3776\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 87.111\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -386.13598395913743 SOC: 0.4550 Cumulative_SOC_deviation: 201.6444 Fuel Consumption: 34.6636\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.494\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -420.9299062717185 SOC: 0.4417 Cumulative_SOC_deviation: 211.5650 Fuel Consumption: 33.6173\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.345\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -509.60829537476803 SOC: 0.4115 Cumulative_SOC_deviation: 235.5595 Fuel Consumption: 31.4315\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.379\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -396.13500982405293 SOC: 0.4492 Cumulative_SOC_deviation: 204.5527 Fuel Consumption: 34.1715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.389\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -509.337662349591 SOC: 0.4125 Cumulative_SOC_deviation: 236.3888 Fuel Consumption: 31.5374\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.334\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -437.03420058848627 SOC: 0.4211 Cumulative_SOC_deviation: 216.0726 Fuel Consumption: 32.2853\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.000\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -472.69710055809725 SOC: 0.4194 Cumulative_SOC_deviation: 226.3512 Fuel Consumption: 32.1077\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.164\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -511.26421503556185 SOC: 0.4067 Cumulative_SOC_deviation: 236.7377 Fuel Consumption: 31.1084\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.054\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -615.4861440055188 SOC: 0.3611 Cumulative_SOC_deviation: 260.1053 Fuel Consumption: 27.7341\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.386\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -710.2381018087566 SOC: 0.3489 Cumulative_SOC_deviation: 280.5974 Fuel Consumption: 27.0376\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.898\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -655.4629313507047 SOC: 0.3654 Cumulative_SOC_deviation: 269.1563 Fuel Consumption: 28.3599\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 103.622\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -756.8891266351771 SOC: 0.3308 Cumulative_SOC_deviation: 290.5313 Fuel Consumption: 25.7548\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.518\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -729.8034994685936 SOC: 0.3420 Cumulative_SOC_deviation: 284.1581 Fuel Consumption: 26.6374\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 100.214\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -684.8954713039252 SOC: 0.3450 Cumulative_SOC_deviation: 275.7087 Fuel Consumption: 26.7765\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.114\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -738.2205240289869 SOC: 0.3197 Cumulative_SOC_deviation: 284.9787 Fuel Consumption: 24.7769\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.902\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -869.2027208260638 SOC: 0.3042 Cumulative_SOC_deviation: 312.5294 Fuel Consumption: 23.9660\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.501\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -901.0620807244919 SOC: 0.2914 Cumulative_SOC_deviation: 317.3519 Fuel Consumption: 22.9079\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.606\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -897.1085705392408 SOC: 0.2851 Cumulative_SOC_deviation: 316.3765 Fuel Consumption: 22.4885\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.310\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -878.1382384969869 SOC: 0.2771 Cumulative_SOC_deviation: 312.8944 Fuel Consumption: 21.8851\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.431\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -884.7859304254539 SOC: 0.2924 Cumulative_SOC_deviation: 315.3586 Fuel Consumption: 23.0309\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 99.127\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -954.361628816126 SOC: 0.2669 Cumulative_SOC_deviation: 327.4391 Fuel Consumption: 21.3362\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 102.488\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -1060.9166367372593 SOC: 0.2529 Cumulative_SOC_deviation: 345.0300 Fuel Consumption: 20.3701\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.550\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -974.1544088113277 SOC: 0.2683 Cumulative_SOC_deviation: 329.8432 Fuel Consumption: 21.3972\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.513\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -1030.9020271675795 SOC: 0.2499 Cumulative_SOC_deviation: 339.2473 Fuel Consumption: 20.0492\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.483\n",
      "Episode: 45 Exploration P: 0.2977 Total reward: -1076.8591705414244 SOC: 0.2430 Cumulative_SOC_deviation: 347.1717 Fuel Consumption: 19.6405\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 72.865\n",
      "Episode: 46 Exploration P: 0.2899 Total reward: -1067.18237235405 SOC: 0.2494 Cumulative_SOC_deviation: 347.4813 Fuel Consumption: 20.1465\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.188\n",
      "Episode: 47 Exploration P: 0.2824 Total reward: -1169.234551089141 SOC: 0.2269 Cumulative_SOC_deviation: 362.0763 Fuel Consumption: 18.5717\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.996\n",
      "Episode: 48 Exploration P: 0.2750 Total reward: -1226.9180076715827 SOC: 0.2122 Cumulative_SOC_deviation: 370.5669 Fuel Consumption: 17.5778\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 72.614\n",
      "Episode: 49 Exploration P: 0.2678 Total reward: -1148.6135905698416 SOC: 0.2240 Cumulative_SOC_deviation: 358.4874 Fuel Consumption: 18.3085\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.113\n",
      "Episode: 50 Exploration P: 0.2608 Total reward: -1145.5071136891916 SOC: 0.2221 Cumulative_SOC_deviation: 358.3299 Fuel Consumption: 18.1480\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.824\n",
      "Episode: 51 Exploration P: 0.2540 Total reward: -1247.4143975304135 SOC: 0.2022 Cumulative_SOC_deviation: 373.2413 Fuel Consumption: 16.8706\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.859\n",
      "Episode: 52 Exploration P: 0.2474 Total reward: -1263.7732034041717 SOC: 0.2009 Cumulative_SOC_deviation: 376.7140 Fuel Consumption: 16.7967\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.851\n",
      "Episode: 53 Exploration P: 0.2410 Total reward: -1303.5119745627887 SOC: 0.1940 Cumulative_SOC_deviation: 384.2521 Fuel Consumption: 16.3688\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.672\n",
      "Episode: 54 Exploration P: 0.2347 Total reward: -1314.703572424273 SOC: 0.1925 Cumulative_SOC_deviation: 383.0754 Fuel Consumption: 16.2147\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.164\n",
      "Episode: 55 Exploration P: 0.2286 Total reward: -1332.1054659371466 SOC: 0.1801 Cumulative_SOC_deviation: 384.5790 Fuel Consumption: 15.2839\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.406\n",
      "Episode: 56 Exploration P: 0.2227 Total reward: -1357.3127588834475 SOC: 0.1802 Cumulative_SOC_deviation: 390.0144 Fuel Consumption: 15.3032\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 107.893\n",
      "Episode: 57 Exploration P: 0.2170 Total reward: -1392.9568828369306 SOC: 0.1761 Cumulative_SOC_deviation: 396.0793 Fuel Consumption: 15.0325\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 131.793\n",
      "Episode: 58 Exploration P: 0.2114 Total reward: -1273.4261425575135 SOC: 0.1951 Cumulative_SOC_deviation: 377.0493 Fuel Consumption: 16.5196\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 134.704\n",
      "Episode: 59 Exploration P: 0.2059 Total reward: -1380.7624722167302 SOC: 0.1646 Cumulative_SOC_deviation: 393.1767 Fuel Consumption: 14.3168\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 131.832\n",
      "Episode: 60 Exploration P: 0.2006 Total reward: -1557.263708635209 SOC: 0.1423 Cumulative_SOC_deviation: 416.9910 Fuel Consumption: 13.0212\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 131.762\n",
      "Episode: 61 Exploration P: 0.1954 Total reward: -1571.8687640920666 SOC: 0.1316 Cumulative_SOC_deviation: 418.9096 Fuel Consumption: 12.2006\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 116.519\n",
      "Episode: 62 Exploration P: 0.1904 Total reward: -1455.3595190178105 SOC: 0.1570 Cumulative_SOC_deviation: 403.2966 Fuel Consumption: 13.8888\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.450\n",
      "Episode: 63 Exploration P: 0.1855 Total reward: -1595.3538695181483 SOC: 0.1305 Cumulative_SOC_deviation: 422.0859 Fuel Consumption: 12.2648\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 110.516\n",
      "Episode: 64 Exploration P: 0.1808 Total reward: -1593.1907324091917 SOC: 0.1375 Cumulative_SOC_deviation: 422.2263 Fuel Consumption: 12.6916\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 102.474\n",
      "Episode: 65 Exploration P: 0.1761 Total reward: -1565.0251007572595 SOC: 0.1402 Cumulative_SOC_deviation: 419.9359 Fuel Consumption: 12.8917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.639\n",
      "Episode: 66 Exploration P: 0.1716 Total reward: -1673.9328522235405 SOC: 0.1255 Cumulative_SOC_deviation: 433.1562 Fuel Consumption: 12.1043\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.238\n",
      "Episode: 67 Exploration P: 0.1673 Total reward: -1667.3176632973498 SOC: 0.1138 Cumulative_SOC_deviation: 432.1938 Fuel Consumption: 11.0879\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 90.376\n",
      "Episode: 68 Exploration P: 0.1630 Total reward: -1719.4658168492658 SOC: 0.1109 Cumulative_SOC_deviation: 439.1682 Fuel Consumption: 10.9905\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.452\n",
      "Episode: 69 Exploration P: 0.1589 Total reward: -1652.628256128241 SOC: 0.1202 Cumulative_SOC_deviation: 430.1098 Fuel Consumption: 11.6089\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 113.243\n",
      "Episode: 70 Exploration P: 0.1548 Total reward: -1664.0406729942085 SOC: 0.1092 Cumulative_SOC_deviation: 431.7601 Fuel Consumption: 10.7274\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 102.450\n",
      "Episode: 71 Exploration P: 0.1509 Total reward: -1658.6970838094253 SOC: 0.1189 Cumulative_SOC_deviation: 431.1920 Fuel Consumption: 11.5287\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 5\n",
    "reward_factor = 10 \n",
    "results_dict = {} \n",
    "for trial in range(num_trials):\n",
    "    print()\n",
    "    print(\"Trial {}\".format(trial + 1))\n",
    "    \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer, env = initialization(\n",
    "        reward_factor\n",
    "    )\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    for ep in range(total_episodes): \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "        )\n",
    "\n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG10_2.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

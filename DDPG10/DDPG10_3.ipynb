{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG10_3 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64): \n",
    "        self.power_mean = 0 \n",
    "        self.power_std = 0\n",
    "        self.sum = 0 \n",
    "        self.sum_deviation = 0 \n",
    "        self.N = 0 \n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        self.N += 1 \n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "        power = obs_tuple[0][0] \n",
    "        \n",
    "        self.sum += power \n",
    "        self.power_mean = self.sum / self.N \n",
    "        self.sum_deviation += (power - self.power_mean) ** 2  \n",
    "        self.power_std = np.sqrt(self.sum_deviation / self.N) \n",
    "            \n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        state_batch = self.state_buffer[batch_indices]\n",
    "        power_batch = (state_batch[:, 0] - self.power_mean) / self.power_std\n",
    "        state_batch[:, 0] = power_batch \n",
    "        \n",
    "        next_state_batch = self.next_state_buffer[batch_indices]\n",
    "        power_batch = (next_state_batch[:, 0] - self.power_mean) / self.power_std\n",
    "        next_state_batch[:, 0] = power_batch \n",
    "#         print(state_batch)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(state_batch)\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(next_state_batch)\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_input)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(reward_factor): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    \n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 21.731\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -207.9093573402503 SOC: 0.8049 Cumulative_SOC_deviation: 91.7535 Fuel Consumption: 61.4179\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 22.181\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -186.7780113981993 SOC: 0.7937 Cumulative_SOC_deviation: 84.6292 Fuel Consumption: 60.4443\n",
      "WARNING:tensorflow:Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_13 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 112.153\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -159.70558141855915 SOC: 0.7600 Cumulative_SOC_deviation: 78.1586 Fuel Consumption: 57.8718\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 112.794\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -152.0707820125547 SOC: 0.7439 Cumulative_SOC_deviation: 75.5545 Fuel Consumption: 56.5243\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.166\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -136.72508915123558 SOC: 0.7366 Cumulative_SOC_deviation: 68.7238 Fuel Consumption: 56.0345\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 111.402\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -158.57895650242992 SOC: 0.6694 Cumulative_SOC_deviation: 81.6816 Fuel Consumption: 50.8925\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 130.828\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -140.6508770299679 SOC: 0.6973 Cumulative_SOC_deviation: 73.8904 Fuel Consumption: 53.0023\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 132.087\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -137.4384703168906 SOC: 0.6855 Cumulative_SOC_deviation: 73.1269 Fuel Consumption: 52.0533\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 133.199\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -126.03816688224845 SOC: 0.6667 Cumulative_SOC_deviation: 66.3344 Fuel Consumption: 50.6989\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 132.044\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -185.53348803775643 SOC: 0.6215 Cumulative_SOC_deviation: 93.0336 Fuel Consumption: 47.1608\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 133.517\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -252.95318987548282 SOC: 0.5898 Cumulative_SOC_deviation: 119.4744 Fuel Consumption: 44.7001\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 139.191\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -254.52355950724163 SOC: 0.5715 Cumulative_SOC_deviation: 123.7611 Fuel Consumption: 43.3103\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 138.154\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -227.9760080991022 SOC: 0.5768 Cumulative_SOC_deviation: 114.4784 Fuel Consumption: 43.7062\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 135.542\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -309.99157689069426 SOC: 0.5445 Cumulative_SOC_deviation: 142.3020 Fuel Consumption: 41.1663\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 135.341\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -333.51421589037074 SOC: 0.5413 Cumulative_SOC_deviation: 146.8336 Fuel Consumption: 41.0390\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 130.071\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -368.74078080641846 SOC: 0.5262 Cumulative_SOC_deviation: 158.1097 Fuel Consumption: 39.9263\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.818\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -306.67019939586214 SOC: 0.5480 Cumulative_SOC_deviation: 140.6611 Fuel Consumption: 41.5921\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.249\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -421.6407209091135 SOC: 0.5053 Cumulative_SOC_deviation: 171.5434 Fuel Consumption: 38.4863\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 71.959\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -480.15537716596396 SOC: 0.4893 Cumulative_SOC_deviation: 184.2572 Fuel Consumption: 37.1596\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.848\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -481.8511583928461 SOC: 0.4824 Cumulative_SOC_deviation: 185.1150 Fuel Consumption: 36.6793\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.799\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -451.29728640081265 SOC: 0.4885 Cumulative_SOC_deviation: 178.6667 Fuel Consumption: 37.2409\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.774\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -512.8489652251275 SOC: 0.4645 Cumulative_SOC_deviation: 192.0768 Fuel Consumption: 35.4018\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.642\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -519.1791709214564 SOC: 0.4676 Cumulative_SOC_deviation: 193.4832 Fuel Consumption: 35.7420\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.831\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -671.4998697127367 SOC: 0.4323 Cumulative_SOC_deviation: 221.8928 Fuel Consumption: 33.1776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.899\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -664.804954938758 SOC: 0.4161 Cumulative_SOC_deviation: 221.1280 Fuel Consumption: 31.8499\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.863\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -737.5607730205878 SOC: 0.4052 Cumulative_SOC_deviation: 233.4263 Fuel Consumption: 31.0464\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.508\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -823.996901709343 SOC: 0.3912 Cumulative_SOC_deviation: 247.2131 Fuel Consumption: 30.0514\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.543\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -773.380404421197 SOC: 0.3992 Cumulative_SOC_deviation: 240.1941 Fuel Consumption: 30.5050\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.439\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -868.4111270930572 SOC: 0.3922 Cumulative_SOC_deviation: 253.8804 Fuel Consumption: 30.0150\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.972\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -960.2347445273313 SOC: 0.3584 Cumulative_SOC_deviation: 267.9003 Fuel Consumption: 27.7009\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.819\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -830.5292445680229 SOC: 0.3813 Cumulative_SOC_deviation: 248.1795 Fuel Consumption: 29.3977\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 103.855\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -1017.4018979598234 SOC: 0.3467 Cumulative_SOC_deviation: 275.9618 Fuel Consumption: 26.7434\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.321\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -1049.9815398882797 SOC: 0.3403 Cumulative_SOC_deviation: 280.9562 Fuel Consumption: 26.2373\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 100.146\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -952.4725626208534 SOC: 0.3525 Cumulative_SOC_deviation: 267.0771 Fuel Consumption: 27.1788\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.275\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -1107.778090640056 SOC: 0.3405 Cumulative_SOC_deviation: 287.9471 Fuel Consumption: 26.4641\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.965\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -1104.6087369289862 SOC: 0.3241 Cumulative_SOC_deviation: 288.1123 Fuel Consumption: 25.2733\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.076\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -1226.9799497112278 SOC: 0.3060 Cumulative_SOC_deviation: 303.2780 Fuel Consumption: 24.0537\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.619\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -1129.0661003192586 SOC: 0.3220 Cumulative_SOC_deviation: 291.4114 Fuel Consumption: 25.1482\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.192\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -1314.8458888487025 SOC: 0.2919 Cumulative_SOC_deviation: 315.2350 Fuel Consumption: 23.1603\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.419\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -1433.6089886581572 SOC: 0.2744 Cumulative_SOC_deviation: 328.2506 Fuel Consumption: 21.9086\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 111.962\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -1411.6582694748915 SOC: 0.2598 Cumulative_SOC_deviation: 324.1898 Fuel Consumption: 20.7852\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 90.455\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -1525.6890306969356 SOC: 0.2647 Cumulative_SOC_deviation: 339.9465 Fuel Consumption: 21.1169\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.178\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -1754.6217093599662 SOC: 0.2262 Cumulative_SOC_deviation: 362.2028 Fuel Consumption: 18.5856\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.103\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -1431.0208405276915 SOC: 0.2618 Cumulative_SOC_deviation: 326.6692 Fuel Consumption: 20.8986\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 72.719\n",
      "Episode: 45 Exploration P: 0.2977 Total reward: -1418.2735640264357 SOC: 0.2619 Cumulative_SOC_deviation: 326.2484 Fuel Consumption: 20.8986\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.139\n",
      "Episode: 46 Exploration P: 0.2899 Total reward: -1706.194859493522 SOC: 0.2330 Cumulative_SOC_deviation: 359.0400 Fuel Consumption: 18.9397\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.114\n",
      "Episode: 47 Exploration P: 0.2824 Total reward: -1597.7514690636206 SOC: 0.2451 Cumulative_SOC_deviation: 346.7656 Fuel Consumption: 19.8662\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.037\n",
      "Episode: 48 Exploration P: 0.2750 Total reward: -1829.7102233859634 SOC: 0.2165 Cumulative_SOC_deviation: 371.1304 Fuel Consumption: 17.8270\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.233\n",
      "Episode: 49 Exploration P: 0.2678 Total reward: -1697.356586455166 SOC: 0.2291 Cumulative_SOC_deviation: 356.1389 Fuel Consumption: 18.6348\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.243\n",
      "Episode: 50 Exploration P: 0.2608 Total reward: -1847.0744015563437 SOC: 0.2084 Cumulative_SOC_deviation: 372.2475 Fuel Consumption: 17.2022\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.264\n",
      "Episode: 51 Exploration P: 0.2540 Total reward: -1803.0262639671234 SOC: 0.2018 Cumulative_SOC_deviation: 367.1463 Fuel Consumption: 16.6930\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 92.166\n",
      "Episode: 52 Exploration P: 0.2474 Total reward: -413.13845659867513 SOC: 0.5456 Cumulative_SOC_deviation: 156.6612 Fuel Consumption: 41.4333\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.499\n",
      "Episode: 53 Exploration P: 0.2410 Total reward: -103.14443433821427 SOC: 0.6077 Cumulative_SOC_deviation: 53.4099 Fuel Consumption: 44.9526\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.921\n",
      "Episode: 54 Exploration P: 0.2347 Total reward: -104.9177060726454 SOC: 0.5763 Cumulative_SOC_deviation: 61.5590 Fuel Consumption: 42.6096\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.660\n",
      "Episode: 55 Exploration P: 0.2286 Total reward: -131.24807311035792 SOC: 0.5656 Cumulative_SOC_deviation: 80.1661 Fuel Consumption: 41.8160\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.233\n",
      "Episode: 56 Exploration P: 0.2227 Total reward: -99.28680100910427 SOC: 0.5928 Cumulative_SOC_deviation: 59.6266 Fuel Consumption: 43.8542\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 122.456\n",
      "Episode: 57 Exploration P: 0.2170 Total reward: -131.80970692894752 SOC: 0.5602 Cumulative_SOC_deviation: 75.8822 Fuel Consumption: 41.4777\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 132.164\n",
      "Episode: 58 Exploration P: 0.2114 Total reward: -138.50148807597276 SOC: 0.5559 Cumulative_SOC_deviation: 83.4527 Fuel Consumption: 41.4766\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 134.726\n",
      "Episode: 59 Exploration P: 0.2059 Total reward: -171.28318914968304 SOC: 0.5706 Cumulative_SOC_deviation: 95.4732 Fuel Consumption: 42.2859\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 132.037\n",
      "Episode: 60 Exploration P: 0.2006 Total reward: -205.6045361675291 SOC: 0.5661 Cumulative_SOC_deviation: 98.1796 Fuel Consumption: 42.9072\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 133.772\n",
      "Episode: 61 Exploration P: 0.1954 Total reward: -234.29474599193944 SOC: 0.5649 Cumulative_SOC_deviation: 109.9712 Fuel Consumption: 42.9432\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 99.157\n",
      "Episode: 62 Exploration P: 0.1904 Total reward: -345.0615217000819 SOC: 0.5518 Cumulative_SOC_deviation: 145.1337 Fuel Consumption: 42.4941\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.234\n",
      "Episode: 63 Exploration P: 0.1855 Total reward: -307.6255760486446 SOC: 0.5593 Cumulative_SOC_deviation: 132.1318 Fuel Consumption: 43.0952\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 127.049\n",
      "Episode: 64 Exploration P: 0.1808 Total reward: -313.186624827306 SOC: 0.5423 Cumulative_SOC_deviation: 134.5745 Fuel Consumption: 41.9099\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 85.342\n",
      "Episode: 65 Exploration P: 0.1761 Total reward: -367.14164622085974 SOC: 0.5451 Cumulative_SOC_deviation: 151.8378 Fuel Consumption: 42.2558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 74.811\n",
      "Episode: 66 Exploration P: 0.1716 Total reward: -431.2758710428058 SOC: 0.5117 Cumulative_SOC_deviation: 171.3235 Fuel Consumption: 39.7610\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 87.479\n",
      "Episode: 67 Exploration P: 0.1673 Total reward: -415.9163752463772 SOC: 0.5070 Cumulative_SOC_deviation: 167.9695 Fuel Consumption: 39.5110\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.675\n",
      "Episode: 68 Exploration P: 0.1630 Total reward: -434.4098925731533 SOC: 0.4931 Cumulative_SOC_deviation: 173.8728 Fuel Consumption: 38.4822\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.992\n",
      "Episode: 69 Exploration P: 0.1589 Total reward: -429.5556632463939 SOC: 0.5158 Cumulative_SOC_deviation: 170.4857 Fuel Consumption: 40.1878\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 116.717\n",
      "Episode: 70 Exploration P: 0.1548 Total reward: -418.3526641062047 SOC: 0.5179 Cumulative_SOC_deviation: 165.6458 Fuel Consumption: 40.4860\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.242\n",
      "Episode: 71 Exploration P: 0.1509 Total reward: -432.01451914323457 SOC: 0.4950 Cumulative_SOC_deviation: 172.7161 Fuel Consumption: 38.7889\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 111.520\n",
      "Episode: 72 Exploration P: 0.1471 Total reward: -456.3130476929558 SOC: 0.5175 Cumulative_SOC_deviation: 176.4549 Fuel Consumption: 40.5747\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.529\n",
      "Episode: 73 Exploration P: 0.1434 Total reward: -406.0499705807437 SOC: 0.5195 Cumulative_SOC_deviation: 161.7580 Fuel Consumption: 40.5304\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 129.741\n",
      "Episode: 74 Exploration P: 0.1398 Total reward: -387.8091913006244 SOC: 0.5297 Cumulative_SOC_deviation: 156.3897 Fuel Consumption: 41.1977\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 5\n",
    "reward_factor = 15 \n",
    "results_dict = {} \n",
    "for trial in range(num_trials): \n",
    "    print()\n",
    "    print(\"Trial {}\".format(trial + 1))\n",
    "    \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer, env = initialization(\n",
    "        reward_factor\n",
    "    )\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    for ep in range(total_episodes): \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "        )\n",
    "        \n",
    "        if \n",
    "\n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG10_3.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

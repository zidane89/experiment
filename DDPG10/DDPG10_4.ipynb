{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG10_4 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64): \n",
    "        self.power_mean = 0 \n",
    "        self.power_std = 0\n",
    "        self.sum = 0 \n",
    "        self.sum_deviation = 0 \n",
    "        self.N = 0 \n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        self.N += 1 \n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "        power = obs_tuple[0][0] \n",
    "        \n",
    "        self.sum += power \n",
    "        self.power_mean = self.sum / self.N \n",
    "        self.sum_deviation += (power - self.power_mean) ** 2  \n",
    "        self.power_std = np.sqrt(self.sum_deviation / self.N) \n",
    "            \n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        state_batch = self.state_buffer[batch_indices]\n",
    "        power_batch = (state_batch[:, 0] - self.power_mean) / self.power_std\n",
    "        state_batch[:, 0] = power_batch \n",
    "        \n",
    "        next_state_batch = self.next_state_buffer[batch_indices]\n",
    "        power_batch = (next_state_batch[:, 0] - self.power_mean) / self.power_std\n",
    "        next_state_batch[:, 0] = power_batch \n",
    "#         print(state_batch)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(state_batch)\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(next_state_batch)\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_input)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(reward_factor): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    \n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 21.839\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -229.33944537921903 SOC: 0.7893 Cumulative_SOC_deviation: 85.7149 Fuel Consumption: 60.0560\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 22.444\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -293.9898059225189 SOC: 0.8198 Cumulative_SOC_deviation: 100.1165 Fuel Consumption: 62.5402\n",
      "WARNING:tensorflow:Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_13 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 113.224\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -203.38341226115054 SOC: 0.7669 Cumulative_SOC_deviation: 79.2549 Fuel Consumption: 58.4670\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 111.631\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -185.9828010274841 SOC: 0.7500 Cumulative_SOC_deviation: 75.9997 Fuel Consumption: 57.0549\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.184\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -170.7148972368558 SOC: 0.7287 Cumulative_SOC_deviation: 73.3652 Fuel Consumption: 55.3368\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 112.520\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -169.68163967417607 SOC: 0.6925 Cumulative_SOC_deviation: 74.2577 Fuel Consumption: 52.5487\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 130.722\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -182.26669444610147 SOC: 0.6583 Cumulative_SOC_deviation: 76.7510 Fuel Consumption: 49.8772\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 132.269\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -184.47511373754287 SOC: 0.6599 Cumulative_SOC_deviation: 79.1278 Fuel Consumption: 50.1875\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 133.210\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -190.00641421186108 SOC: 0.6486 Cumulative_SOC_deviation: 80.6541 Fuel Consumption: 49.1850\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 131.628\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -208.45389580536653 SOC: 0.6247 Cumulative_SOC_deviation: 88.4077 Fuel Consumption: 47.3084\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 133.490\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -296.22420982831875 SOC: 0.5847 Cumulative_SOC_deviation: 114.0258 Fuel Consumption: 44.2700\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 138.995\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -306.3018259798964 SOC: 0.5937 Cumulative_SOC_deviation: 115.6633 Fuel Consumption: 45.0574\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 137.470\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -282.87021863649045 SOC: 0.5799 Cumulative_SOC_deviation: 113.5311 Fuel Consumption: 44.0336\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 135.160\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -378.038676220785 SOC: 0.5477 Cumulative_SOC_deviation: 136.3348 Fuel Consumption: 41.4948\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 134.551\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -365.21152298358896 SOC: 0.5503 Cumulative_SOC_deviation: 134.3411 Fuel Consumption: 41.7205\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 129.770\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -410.9375713132609 SOC: 0.5453 Cumulative_SOC_deviation: 143.5231 Fuel Consumption: 41.4199\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.505\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -536.3279099412263 SOC: 0.5115 Cumulative_SOC_deviation: 168.6034 Fuel Consumption: 38.7709\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 119.019\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -483.3681149350989 SOC: 0.5251 Cumulative_SOC_deviation: 158.4670 Fuel Consumption: 39.9852\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 71.651\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -607.3489658820909 SOC: 0.4694 Cumulative_SOC_deviation: 181.7304 Fuel Consumption: 35.6158\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.034\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -711.7909292293488 SOC: 0.4655 Cumulative_SOC_deviation: 197.6145 Fuel Consumption: 35.4125\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.105\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -684.5011240942426 SOC: 0.4860 Cumulative_SOC_deviation: 192.7275 Fuel Consumption: 37.0152\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.436\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -815.1255025867761 SOC: 0.4422 Cumulative_SOC_deviation: 212.6837 Fuel Consumption: 33.6654\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.318\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -845.6162267240346 SOC: 0.4482 Cumulative_SOC_deviation: 216.4047 Fuel Consumption: 34.2934\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.387\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -837.3332163822446 SOC: 0.4360 Cumulative_SOC_deviation: 215.9218 Fuel Consumption: 33.3038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.388\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -857.6868562833884 SOC: 0.4319 Cumulative_SOC_deviation: 218.3676 Fuel Consumption: 33.0139\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.267\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -982.8911146309802 SOC: 0.4209 Cumulative_SOC_deviation: 234.2987 Fuel Consumption: 32.2393\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.228\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -1064.4420510597454 SOC: 0.3949 Cumulative_SOC_deviation: 244.6960 Fuel Consumption: 30.4312\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.145\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -1101.511182590016 SOC: 0.3967 Cumulative_SOC_deviation: 249.6270 Fuel Consumption: 30.5810\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.192\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -1209.4844041836875 SOC: 0.3641 Cumulative_SOC_deviation: 261.0543 Fuel Consumption: 28.0732\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.507\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -1331.636661590461 SOC: 0.3549 Cumulative_SOC_deviation: 274.0687 Fuel Consumption: 27.4259\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.448\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -1169.0392286771619 SOC: 0.3755 Cumulative_SOC_deviation: 256.6250 Fuel Consumption: 28.7151\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 103.882\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -1072.8858994591624 SOC: 0.3966 Cumulative_SOC_deviation: 245.5776 Fuel Consumption: 30.5136\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.212\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -1496.604699580707 SOC: 0.3278 Cumulative_SOC_deviation: 290.9321 Fuel Consumption: 25.6029\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 100.302\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -1432.4399392957268 SOC: 0.3260 Cumulative_SOC_deviation: 283.9758 Fuel Consumption: 25.3707\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.721\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -1510.142363490333 SOC: 0.3145 Cumulative_SOC_deviation: 290.7665 Fuel Consumption: 24.5169\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.322\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -1430.5627246321487 SOC: 0.3242 Cumulative_SOC_deviation: 284.4899 Fuel Consumption: 25.3247\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.143\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -1585.8403315520463 SOC: 0.2981 Cumulative_SOC_deviation: 297.5919 Fuel Consumption: 23.2984\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.817\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -1688.5183394456515 SOC: 0.3006 Cumulative_SOC_deviation: 308.5045 Fuel Consumption: 23.6215\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.035\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -1762.038824458867 SOC: 0.2821 Cumulative_SOC_deviation: 314.8536 Fuel Consumption: 22.3291\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.514\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -1894.9627820526366 SOC: 0.2739 Cumulative_SOC_deviation: 327.3298 Fuel Consumption: 21.6903\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 104.475\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -1866.8121073639568 SOC: 0.2775 Cumulative_SOC_deviation: 325.3166 Fuel Consumption: 22.0049\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 96.913\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -1941.1045450521751 SOC: 0.2694 Cumulative_SOC_deviation: 331.0090 Fuel Consumption: 21.5545\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.020\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -2224.800705822255 SOC: 0.2428 Cumulative_SOC_deviation: 355.5884 Fuel Consumption: 19.7303\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.264\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -1918.0984023609087 SOC: 0.2686 Cumulative_SOC_deviation: 328.9367 Fuel Consumption: 21.3940\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 72.773\n",
      "Episode: 45 Exploration P: 0.2977 Total reward: -1879.790102865188 SOC: 0.2716 Cumulative_SOC_deviation: 325.1941 Fuel Consumption: 21.6475\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.013\n",
      "Episode: 46 Exploration P: 0.2899 Total reward: -2021.3411423947998 SOC: 0.2644 Cumulative_SOC_deviation: 340.0141 Fuel Consumption: 21.2121\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.291\n",
      "Episode: 47 Exploration P: 0.2824 Total reward: -2308.236709117775 SOC: 0.2317 Cumulative_SOC_deviation: 360.6984 Fuel Consumption: 18.9504\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.588\n",
      "Episode: 48 Exploration P: 0.2750 Total reward: -2286.8224348724 SOC: 0.2218 Cumulative_SOC_deviation: 359.0786 Fuel Consumption: 18.2111\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 72.404\n",
      "Episode: 49 Exploration P: 0.2678 Total reward: -2264.3030476555246 SOC: 0.2226 Cumulative_SOC_deviation: 357.0534 Fuel Consumption: 18.2026\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.035\n",
      "Episode: 50 Exploration P: 0.2608 Total reward: -2447.915564037083 SOC: 0.2062 Cumulative_SOC_deviation: 372.3505 Fuel Consumption: 17.1241\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.876\n",
      "Episode: 51 Exploration P: 0.2540 Total reward: -2428.80211284003 SOC: 0.2131 Cumulative_SOC_deviation: 370.7867 Fuel Consumption: 17.5681\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 93.004\n",
      "Episode: 52 Exploration P: 0.2474 Total reward: -2506.5884555292805 SOC: 0.2055 Cumulative_SOC_deviation: 377.2566 Fuel Consumption: 17.0696\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.535\n",
      "Episode: 53 Exploration P: 0.2410 Total reward: -2457.331562834484 SOC: 0.2077 Cumulative_SOC_deviation: 372.9439 Fuel Consumption: 17.1969\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.329\n",
      "Episode: 54 Exploration P: 0.2347 Total reward: -509.11183961770064 SOC: 0.6016 Cumulative_SOC_deviation: 131.5966 Fuel Consumption: 45.6322\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 83.272\n",
      "Episode: 55 Exploration P: 0.2286 Total reward: -111.60890177740922 SOC: 0.5925 Cumulative_SOC_deviation: 55.0081 Fuel Consumption: 43.0333\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.570\n",
      "Episode: 56 Exploration P: 0.2227 Total reward: -130.61649683867927 SOC: 0.5839 Cumulative_SOC_deviation: 66.7438 Fuel Consumption: 42.3963\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 112.823\n",
      "Episode: 57 Exploration P: 0.2170 Total reward: -116.1113716501496 SOC: 0.5930 Cumulative_SOC_deviation: 52.7995 Fuel Consumption: 43.1621\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 130.982\n",
      "Episode: 58 Exploration P: 0.2114 Total reward: -102.96342569188934 SOC: 0.5901 Cumulative_SOC_deviation: 49.0882 Fuel Consumption: 43.0730\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 134.873\n",
      "Episode: 59 Exploration P: 0.2059 Total reward: -130.55021541098083 SOC: 0.5877 Cumulative_SOC_deviation: 62.9732 Fuel Consumption: 42.9029\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 131.848\n",
      "Episode: 60 Exploration P: 0.2006 Total reward: -110.30910162211227 SOC: 0.5885 Cumulative_SOC_deviation: 55.1100 Fuel Consumption: 43.1542\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 131.901\n",
      "Episode: 61 Exploration P: 0.1954 Total reward: -177.97703790878404 SOC: 0.5828 Cumulative_SOC_deviation: 80.1130 Fuel Consumption: 42.7476\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 112.284\n",
      "Episode: 62 Exploration P: 0.1904 Total reward: -97.01180529000371 SOC: 0.6027 Cumulative_SOC_deviation: 38.0131 Fuel Consumption: 44.2246\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.385\n",
      "Episode: 63 Exploration P: 0.1855 Total reward: -114.02842568885741 SOC: 0.5860 Cumulative_SOC_deviation: 56.9272 Fuel Consumption: 42.9262\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 117.272\n",
      "Episode: 64 Exploration P: 0.1808 Total reward: -141.54640888287506 SOC: 0.5935 Cumulative_SOC_deviation: 66.5794 Fuel Consumption: 43.4499\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 96.179\n",
      "Episode: 65 Exploration P: 0.1761 Total reward: -144.9435157957717 SOC: 0.5972 Cumulative_SOC_deviation: 61.1020 Fuel Consumption: 43.6181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.679\n",
      "Episode: 66 Exploration P: 0.1716 Total reward: -126.39111621094618 SOC: 0.6010 Cumulative_SOC_deviation: 59.5883 Fuel Consumption: 43.9744\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.125\n",
      "Episode: 67 Exploration P: 0.1673 Total reward: -119.91214354815718 SOC: 0.5959 Cumulative_SOC_deviation: 55.4578 Fuel Consumption: 43.5365\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 90.670\n",
      "Episode: 68 Exploration P: 0.1630 Total reward: -241.37980145985426 SOC: 0.5730 Cumulative_SOC_deviation: 96.5977 Fuel Consumption: 42.3856\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.075\n",
      "Episode: 69 Exploration P: 0.1589 Total reward: -236.73913362898548 SOC: 0.5803 Cumulative_SOC_deviation: 92.7393 Fuel Consumption: 43.1257\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 113.287\n",
      "Episode: 70 Exploration P: 0.1548 Total reward: -261.8423471750428 SOC: 0.5526 Cumulative_SOC_deviation: 104.4317 Fuel Consumption: 41.8832\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.939\n",
      "Episode: 71 Exploration P: 0.1509 Total reward: -315.54882254114347 SOC: 0.5670 Cumulative_SOC_deviation: 113.0500 Fuel Consumption: 43.2917\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 105.547\n",
      "Episode: 72 Exploration P: 0.1471 Total reward: -347.583458319158 SOC: 0.5541 Cumulative_SOC_deviation: 128.0268 Fuel Consumption: 42.1728\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.925\n",
      "Episode: 73 Exploration P: 0.1434 Total reward: -245.18714493140718 SOC: 0.5840 Cumulative_SOC_deviation: 87.8943 Fuel Consumption: 44.4761\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 127.122\n",
      "Episode: 74 Exploration P: 0.1398 Total reward: -161.93096520851626 SOC: 0.5866 Cumulative_SOC_deviation: 69.9443 Fuel Consumption: 43.5210\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 90.171\n",
      "Episode: 75 Exploration P: 0.1362 Total reward: -202.75885573841984 SOC: 0.5863 Cumulative_SOC_deviation: 80.7063 Fuel Consumption: 43.5607\n"
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 5\n",
    "reward_factor = 20\n",
    "results_dict = {} \n",
    "for trial in range(num_trials): \n",
    "    print()\n",
    "    print(\"Trial {}\".format(trial + 1))\n",
    "    \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer, env = initialization(\n",
    "        reward_factor\n",
    "    )\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    for ep in range(total_episodes): \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "        )\n",
    "        \n",
    "        if \n",
    "\n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG10_4.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow import keras \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import deque \n",
    "from tensorflow.keras import layers\n",
    "import time \n",
    "\n",
    "from vehicle_model_DDPG10_1 import Environment \n",
    "from cell_model import CellModel \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drving_cycle = '../../OC_SIM_DB/OC_SIM_DB_Cycles/Highway/01_FTP72_fuds.mat'\n",
    "battery_path = \"../../OC_SIM_DB/OC_SIM_DB_Bat/OC_SIM_DB_Bat_e-4wd_Battery.mat\"\n",
    "motor_path = \"../../OC_SIM_DB/OC_SIM_DB_Mot/OC_SIM_DB_Mot_id_75_110_Westinghouse.mat\"\n",
    "cell_model = CellModel()\n",
    "env = Environment(cell_model, drving_cycle, battery_path, motor_path, 10)\n",
    "\n",
    "num_states = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise: \n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None): \n",
    "        self.theta = theta \n",
    "        self.mean = mean \n",
    "        self.std_dev = std_deviation \n",
    "        self.dt = dt \n",
    "        self.x_initial = x_initial \n",
    "        self.reset() \n",
    "        \n",
    "    def reset(self): \n",
    "        if self.x_initial is not None: \n",
    "            self.x_prev = self.x_initial \n",
    "        else: \n",
    "            self.x_prev = 0 \n",
    "            \n",
    "    def __call__(self): \n",
    "        x = (\n",
    "             self.x_prev + self.theta * (self.mean - self.x_prev) * self.dt \n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal() \n",
    "        )\n",
    "        self.x_prev = x \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer: \n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64): \n",
    "        self.power_mean = 0 \n",
    "        self.power_std = 0\n",
    "        self.sum = 0 \n",
    "        self.sum_deviation = 0 \n",
    "        self.N = 0 \n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity \n",
    "        self.batch_size = batch_size \n",
    "        self.buffer_counter = 0 \n",
    "        \n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "    def record(self, obs_tuple):\n",
    "        self.N += 1 \n",
    "        index = self.buffer_counter % self.buffer_capacity \n",
    "        power = obs_tuple[0][0] \n",
    "        \n",
    "        self.sum += power \n",
    "        self.power_mean = self.sum / self.N \n",
    "        self.sum_deviation += (power - self.power_mean) ** 2  \n",
    "        self.power_std = np.sqrt(self.sum_deviation / self.N) \n",
    "            \n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        \n",
    "        self.buffer_counter += 1 \n",
    "        \n",
    "    def learn(self): \n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        state_batch = self.state_buffer[batch_indices]\n",
    "        power_batch = (state_batch[:, 0] - self.power_mean) / self.power_std\n",
    "        state_batch[:, 0] = power_batch \n",
    "        \n",
    "        next_state_batch = self.next_state_buffer[batch_indices]\n",
    "        power_batch = (next_state_batch[:, 0] - self.power_mean) / self.power_std\n",
    "        next_state_batch[:, 0] = power_batch \n",
    "#         print(state_batch)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(state_batch)\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(next_state_batch)\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.square(y - critic_value)) \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables) \n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            actor_loss = - tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables) \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(tau): \n",
    "    new_weights = [] \n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_critic.set_weights(new_weights)\n",
    "    \n",
    "    new_weights = [] \n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights): \n",
    "        new_weights.append(target_variables[i] * (1 - tau) + tau * variable)\n",
    "    target_actor.set_weights(new_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(): \n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    \n",
    "    inputs = layers.Input(shape=(num_states))\n",
    "    out = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", \n",
    "                          kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_critic(): \n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_input)\n",
    "#     state_out = layers.BatchNormalization()(state_out)\n",
    "    \n",
    "    action_input = layers.Input(shape=(1))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "#     action_out = layers.BatchNormalization()(action_out)\n",
    "    \n",
    "    concat = layers.Concatenate()([state_out, action_out]) \n",
    "    \n",
    "    out = layers.Dense(512, activation=\"relu\")(concat)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(512, activation=\"relu\")(out)\n",
    "#     out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "    \n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object): \n",
    "    j_min = state[0][2].numpy()\n",
    "    j_max = state[0][3].numpy()\n",
    "    sampled_action = tf.squeeze(actor_model(state)) \n",
    "    noise = noise_object()\n",
    "    sampled_action = sampled_action.numpy() + noise \n",
    "    legal_action = sampled_action * j_max \n",
    "    legal_action = np.clip(legal_action, j_min, j_max)\n",
    "#     print(j_min, j_max, legal_action, noise)\n",
    "    return legal_action \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_epsilon_greedy(state, eps): \n",
    "    j_min = state[0][-2].numpy()\n",
    "    j_max = state[0][-1].numpy()\n",
    "\n",
    "    if random.random() < eps: \n",
    "        a = random.randint(0, 9)\n",
    "        return np.linspace(j_min, j_max, 10)[a]\n",
    "    else: \n",
    "        sampled_action = tf.squeeze(actor_model(state)).numpy()  \n",
    "        legal_action = sampled_action * j_max \n",
    "        legal_action = np.clip(legal_action, j_min, j_max)\n",
    "        return legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2 \n",
    "ou_noise = OUActionNoise(mean=0, std_deviation=0.2)\n",
    "\n",
    "critic_lr = 0.0005 \n",
    "actor_lr = 0.00025 \n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "gamma = 0.95 \n",
    "tau = 0.001 \n",
    "\n",
    "MAX_EPSILON = 1 \n",
    "MIN_EPSILON = 0.01 \n",
    "DECAY_RATE = 0.00002\n",
    "BATCH_SIZE = 32 \n",
    "DELAY_TRAINING = 3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(reward_factor): \n",
    "    actor_model = get_actor() \n",
    "    critic_model = get_critic() \n",
    "\n",
    "    target_actor = get_actor() \n",
    "    target_critic = get_critic() \n",
    "    target_actor.set_weights(actor_model.get_weights())\n",
    "    target_critic.set_weights(critic_model.get_weights())\n",
    "    \n",
    "    buffer = Buffer(500000, BATCH_SIZE)\n",
    "    \n",
    "    env = Environment(cell_model, drving_cycle, battery_path, motor_path, reward_factor)\n",
    "    return actor_model, critic_model, target_actor, target_critic, buffer, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 17.861\n",
      "Episode: 1 Exploration P: 1.0000 Total reward: -112.98787110906343 SOC: 0.8117 Cumulative_SOC_deviation: 93.9394 Fuel Consumption: 61.9903\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 18.298\n",
      "Episode: 2 Exploration P: 1.0000 Total reward: -104.20500497312737 SOC: 0.7885 Cumulative_SOC_deviation: 88.8813 Fuel Consumption: 60.0849\n",
      "WARNING:tensorflow:Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_13 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 70.040\n",
      "Episode: 3 Exploration P: 0.9217 Total reward: -88.2952942792603 SOC: 0.7557 Cumulative_SOC_deviation: 73.9735 Fuel Consumption: 57.3628\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 126.036\n",
      "Episode: 4 Exploration P: 0.8970 Total reward: -83.10531520741735 SOC: 0.7330 Cumulative_SOC_deviation: 70.9502 Fuel Consumption: 55.7172\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 107.593\n",
      "Episode: 5 Exploration P: 0.8730 Total reward: -83.67808617624266 SOC: 0.7261 Cumulative_SOC_deviation: 73.7338 Fuel Consumption: 55.2834\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.197\n",
      "Episode: 6 Exploration P: 0.8496 Total reward: -81.30215783445576 SOC: 0.7025 Cumulative_SOC_deviation: 73.6734 Fuel Consumption: 53.4388\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 113.880\n",
      "Episode: 7 Exploration P: 0.8269 Total reward: -75.45074781064018 SOC: 0.6854 Cumulative_SOC_deviation: 65.8046 Fuel Consumption: 52.0608\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 130.321\n",
      "Episode: 8 Exploration P: 0.8048 Total reward: -92.58728627217363 SOC: 0.6364 Cumulative_SOC_deviation: 92.1007 Fuel Consumption: 48.2767\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 130.992\n",
      "Episode: 9 Exploration P: 0.7832 Total reward: -86.18706581402445 SOC: 0.6435 Cumulative_SOC_deviation: 83.5919 Fuel Consumption: 48.8608\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 132.808\n",
      "Episode: 10 Exploration P: 0.7623 Total reward: -85.0198992047718 SOC: 0.6422 Cumulative_SOC_deviation: 80.5120 Fuel Consumption: 48.7581\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 130.939\n",
      "Episode: 11 Exploration P: 0.7419 Total reward: -114.50820412081143 SOC: 0.5896 Cumulative_SOC_deviation: 120.2222 Fuel Consumption: 44.6808\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 132.335\n",
      "Episode: 12 Exploration P: 0.7221 Total reward: -106.23348862662553 SOC: 0.5966 Cumulative_SOC_deviation: 113.4115 Fuel Consumption: 45.1858\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 139.498\n",
      "Episode: 13 Exploration P: 0.7028 Total reward: -106.73125487575966 SOC: 0.5723 Cumulative_SOC_deviation: 116.8174 Fuel Consumption: 43.3488\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 136.744\n",
      "Episode: 14 Exploration P: 0.6840 Total reward: -125.76160277689914 SOC: 0.5704 Cumulative_SOC_deviation: 133.7636 Fuel Consumption: 43.2451\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 134.219\n",
      "Episode: 15 Exploration P: 0.6658 Total reward: -122.28031893433614 SOC: 0.5676 Cumulative_SOC_deviation: 131.8582 Fuel Consumption: 43.0739\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 134.155\n",
      "Episode: 16 Exploration P: 0.6480 Total reward: -113.44854559996189 SOC: 0.5609 Cumulative_SOC_deviation: 124.8003 Fuel Consumption: 42.5422\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 129.153\n",
      "Episode: 17 Exploration P: 0.6307 Total reward: -134.41852243306212 SOC: 0.5350 Cumulative_SOC_deviation: 146.2269 Fuel Consumption: 40.7351\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.274\n",
      "Episode: 18 Exploration P: 0.6139 Total reward: -173.69702674603838 SOC: 0.4881 Cumulative_SOC_deviation: 177.5347 Fuel Consumption: 37.1468\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.767\n",
      "Episode: 19 Exploration P: 0.5976 Total reward: -184.85207666547782 SOC: 0.4834 Cumulative_SOC_deviation: 184.5793 Fuel Consumption: 36.7477\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 71.555\n",
      "Episode: 20 Exploration P: 0.5816 Total reward: -196.1469741727945 SOC: 0.4727 Cumulative_SOC_deviation: 192.8138 Fuel Consumption: 35.9667\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.956\n",
      "Episode: 21 Exploration P: 0.5662 Total reward: -225.65172827101637 SOC: 0.4470 Cumulative_SOC_deviation: 210.6401 Fuel Consumption: 34.1865\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.554\n",
      "Episode: 22 Exploration P: 0.5511 Total reward: -219.85156655679046 SOC: 0.4486 Cumulative_SOC_deviation: 207.2004 Fuel Consumption: 34.0506\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.204\n",
      "Episode: 23 Exploration P: 0.5364 Total reward: -207.41441807801183 SOC: 0.4588 Cumulative_SOC_deviation: 199.7303 Fuel Consumption: 34.9022\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.119\n",
      "Episode: 24 Exploration P: 0.5222 Total reward: -211.41451249507503 SOC: 0.4431 Cumulative_SOC_deviation: 202.6340 Fuel Consumption: 33.6162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.189\n",
      "Episode: 25 Exploration P: 0.5083 Total reward: -238.1019420853262 SOC: 0.4337 Cumulative_SOC_deviation: 217.5106 Fuel Consumption: 33.1990\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.104\n",
      "Episode: 26 Exploration P: 0.4948 Total reward: -295.7400386110852 SOC: 0.4043 Cumulative_SOC_deviation: 247.9284 Fuel Consumption: 30.8805\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.123\n",
      "Episode: 27 Exploration P: 0.4817 Total reward: -254.02245322532346 SOC: 0.4124 Cumulative_SOC_deviation: 227.1875 Fuel Consumption: 31.4786\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.914\n",
      "Episode: 28 Exploration P: 0.4689 Total reward: -292.23257669328626 SOC: 0.3798 Cumulative_SOC_deviation: 247.0959 Fuel Consumption: 29.0446\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.059\n",
      "Episode: 29 Exploration P: 0.4565 Total reward: -254.2275669048554 SOC: 0.4069 Cumulative_SOC_deviation: 226.5640 Fuel Consumption: 31.1052\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.991\n",
      "Episode: 30 Exploration P: 0.4444 Total reward: -306.6167435385396 SOC: 0.3872 Cumulative_SOC_deviation: 253.6933 Fuel Consumption: 29.8085\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.055\n",
      "Episode: 31 Exploration P: 0.4326 Total reward: -349.3687132918006 SOC: 0.3550 Cumulative_SOC_deviation: 273.1505 Fuel Consumption: 27.4623\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.953\n",
      "Episode: 32 Exploration P: 0.4212 Total reward: -351.66506324600465 SOC: 0.3441 Cumulative_SOC_deviation: 273.0749 Fuel Consumption: 26.7359\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 103.287\n",
      "Episode: 33 Exploration P: 0.4100 Total reward: -378.25472153900415 SOC: 0.3440 Cumulative_SOC_deviation: 284.0237 Fuel Consumption: 26.8514\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.215\n",
      "Episode: 34 Exploration P: 0.3992 Total reward: -386.33125781804137 SOC: 0.3191 Cumulative_SOC_deviation: 287.7767 Fuel Consumption: 24.7245\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 100.112\n",
      "Episode: 35 Exploration P: 0.3887 Total reward: -419.8026182496043 SOC: 0.3086 Cumulative_SOC_deviation: 301.1816 Fuel Consumption: 24.1607\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.909\n",
      "Episode: 36 Exploration P: 0.3784 Total reward: -438.0411035955828 SOC: 0.3010 Cumulative_SOC_deviation: 308.8093 Fuel Consumption: 23.5188\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.656\n",
      "Episode: 37 Exploration P: 0.3684 Total reward: -450.58009114971776 SOC: 0.3021 Cumulative_SOC_deviation: 313.7733 Fuel Consumption: 23.8568\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.488\n",
      "Episode: 38 Exploration P: 0.3587 Total reward: -449.06343806976906 SOC: 0.3020 Cumulative_SOC_deviation: 313.0446 Fuel Consumption: 23.7381\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.344\n",
      "Episode: 39 Exploration P: 0.3493 Total reward: -438.35021747176773 SOC: 0.3032 Cumulative_SOC_deviation: 309.8593 Fuel Consumption: 23.8686\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.159\n",
      "Episode: 40 Exploration P: 0.3401 Total reward: -453.23873424686417 SOC: 0.2979 Cumulative_SOC_deviation: 315.7228 Fuel Consumption: 23.5423\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.302\n",
      "Episode: 41 Exploration P: 0.3311 Total reward: -550.8472580253891 SOC: 0.2382 Cumulative_SOC_deviation: 348.2952 Fuel Consumption: 19.3666\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 99.969\n",
      "Episode: 42 Exploration P: 0.3224 Total reward: -522.0322974528725 SOC: 0.2526 Cumulative_SOC_deviation: 337.5404 Fuel Consumption: 20.4279\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 101.105\n",
      "Episode: 43 Exploration P: 0.3140 Total reward: -554.1265990326639 SOC: 0.2403 Cumulative_SOC_deviation: 348.6058 Fuel Consumption: 19.4019\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 75.490\n",
      "Episode: 44 Exploration P: 0.3057 Total reward: -560.1059600209716 SOC: 0.2386 Cumulative_SOC_deviation: 349.4014 Fuel Consumption: 19.2992\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 78.040\n",
      "Episode: 45 Exploration P: 0.2977 Total reward: -535.7407431875863 SOC: 0.2535 Cumulative_SOC_deviation: 344.9835 Fuel Consumption: 20.4439\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.255\n",
      "Episode: 46 Exploration P: 0.2899 Total reward: -519.3645650561682 SOC: 0.2599 Cumulative_SOC_deviation: 336.5876 Fuel Consumption: 20.8312\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 72.631\n",
      "Episode: 47 Exploration P: 0.2824 Total reward: -591.0688482244047 SOC: 0.2157 Cumulative_SOC_deviation: 361.6487 Fuel Consumption: 17.6772\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.021\n",
      "Episode: 48 Exploration P: 0.2750 Total reward: -617.1164486719565 SOC: 0.2156 Cumulative_SOC_deviation: 370.1285 Fuel Consumption: 17.6644\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 80.882\n",
      "Episode: 49 Exploration P: 0.2678 Total reward: -629.7179025317374 SOC: 0.2079 Cumulative_SOC_deviation: 372.6216 Fuel Consumption: 17.2193\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 72.446\n",
      "Episode: 50 Exploration P: 0.2608 Total reward: -588.7825280987529 SOC: 0.2190 Cumulative_SOC_deviation: 358.6395 Fuel Consumption: 17.9843\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 72.834\n",
      "Episode: 51 Exploration P: 0.2540 Total reward: -593.5306830105851 SOC: 0.2121 Cumulative_SOC_deviation: 361.5031 Fuel Consumption: 17.3541\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.414\n",
      "Episode: 52 Exploration P: 0.2474 Total reward: -589.3309271883578 SOC: 0.2230 Cumulative_SOC_deviation: 359.4034 Fuel Consumption: 18.2058\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 89.775\n",
      "Episode: 53 Exploration P: 0.2410 Total reward: -655.3934579787489 SOC: 0.1944 Cumulative_SOC_deviation: 380.0067 Fuel Consumption: 16.3602\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 81.331\n",
      "Episode: 54 Exploration P: 0.2347 Total reward: -654.8124399956084 SOC: 0.1976 Cumulative_SOC_deviation: 380.9464 Fuel Consumption: 16.5507\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.539\n",
      "Episode: 55 Exploration P: 0.2286 Total reward: -678.0145874739463 SOC: 0.1789 Cumulative_SOC_deviation: 385.7825 Fuel Consumption: 15.3128\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 82.932\n",
      "Episode: 56 Exploration P: 0.2227 Total reward: -708.445172708112 SOC: 0.1730 Cumulative_SOC_deviation: 396.0665 Fuel Consumption: 14.8806\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.113\n",
      "Episode: 57 Exploration P: 0.2170 Total reward: -711.2240441952339 SOC: 0.1685 Cumulative_SOC_deviation: 397.2788 Fuel Consumption: 14.6238\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 105.965\n",
      "Episode: 58 Exploration P: 0.2114 Total reward: -727.9767944509425 SOC: 0.1646 Cumulative_SOC_deviation: 400.6012 Fuel Consumption: 14.5607\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 131.616\n",
      "Episode: 59 Exploration P: 0.2059 Total reward: -712.2679208339715 SOC: 0.1582 Cumulative_SOC_deviation: 395.8939 Fuel Consumption: 13.9359\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 134.375\n",
      "Episode: 60 Exploration P: 0.2006 Total reward: -751.6845550749462 SOC: 0.1653 Cumulative_SOC_deviation: 409.2440 Fuel Consumption: 14.4548\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 131.779\n",
      "Episode: 61 Exploration P: 0.1954 Total reward: -789.0902824205999 SOC: 0.1323 Cumulative_SOC_deviation: 417.4503 Fuel Consumption: 12.3846\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 131.338\n",
      "Episode: 62 Exploration P: 0.1904 Total reward: -736.3212828926376 SOC: 0.1581 Cumulative_SOC_deviation: 404.6492 Fuel Consumption: 14.0065\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 118.760\n",
      "Episode: 63 Exploration P: 0.1855 Total reward: -741.6523356948413 SOC: 0.1535 Cumulative_SOC_deviation: 406.8206 Fuel Consumption: 13.6620\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 77.029\n",
      "Episode: 64 Exploration P: 0.1808 Total reward: -801.7842566936038 SOC: 0.1364 Cumulative_SOC_deviation: 422.3750 Fuel Consumption: 12.6735\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.354\n",
      "Episode: 65 Exploration P: 0.1761 Total reward: -766.0190133468504 SOC: 0.1432 Cumulative_SOC_deviation: 411.3361 Fuel Consumption: 13.0864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 106.794\n",
      "Episode: 66 Exploration P: 0.1716 Total reward: -832.5758161899992 SOC: 0.1268 Cumulative_SOC_deviation: 430.5351 Fuel Consumption: 11.9652\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 73.268\n",
      "Episode: 67 Exploration P: 0.1673 Total reward: -837.7159463224549 SOC: 0.1258 Cumulative_SOC_deviation: 432.1231 Fuel Consumption: 12.0016\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 76.515\n",
      "Episode: 68 Exploration P: 0.1630 Total reward: -769.6192114837215 SOC: 0.1473 Cumulative_SOC_deviation: 414.3132 Fuel Consumption: 13.1870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ValueCreatorSong\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\DDPG10\\vehicle_model_DDPG10_1.py:246: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  2 * r_dis)) * (v_dis - (v_dis ** 2 - 4 * r_dis * p_bat) ** (0.5)) * (p_bat >= 0)\n",
      "C:\\Users\\ValueCreatorSong\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\DDPG10\\vehicle_model_DDPG10_1.py:272: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  2 * r_dis)) * (v_dis - (v_dis ** 2 - 4 * r_dis * p_bat) ** (0.5)) * (p_bat >= 0)\n",
      "C:\\Users\\ValueCreatorSong\\Desktop\\Academic\\graduate_paper\\degradation_model\\experiment\\DDPG10\\cell_model.py:139: IntegrationWarning: The occurrence of roundoff error is detected, which prevents \n",
      "  the requested tolerance from being achieved.  The error may be \n",
      "  underestimated.\n",
      "  resistance = quad(self.conductivity_integrand, 0, t_m * 10 ** -4, (a, b, c))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 90.693\n",
      "Episode: 69 Exploration P: 0.1589 Total reward: nan SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: nan\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 79.401\n",
      "Episode: 70 Exploration P: 0.1548 Total reward: nan SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: nan\n",
      "maximum steps, simulation is done ... \n",
      "elapsed_time: 114.504\n",
      "Episode: 71 Exploration P: 0.1509 Total reward: nan SOC: nan Cumulative_SOC_deviation: nan Fuel Consumption: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-da793f84e57c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mDELAY_TRAINING\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                 \u001b[0mupdate_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMIN_EPSILON\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mMAX_EPSILON\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mMIN_EPSILON\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mDECAY_RATE\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-33e9fbf376af>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mcritic_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcritic_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mcritic_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcritic_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mcritic_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         critic_optimizer.apply_gradients(\n\u001b[0;32m     61\u001b[0m             \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MulGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1195\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m     gy = array_ops.reshape(\n\u001b[1;32m-> 1197\u001b[1;33m         math_ops.reduce_sum(gen_math_ops.mul(x, grad), ry), sy)\n\u001b[0m\u001b[0;32m   1198\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.1_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6683\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m   6684\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Mul\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6685\u001b[1;33m         name, _ctx._post_execution_callbacks, x, y)\n\u001b[0m\u001b[0;32m   6686\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6687\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(env.version)\n",
    "\n",
    "num_trials = 5\n",
    "reward_factor = 5 \n",
    "results_dict = {} \n",
    "for trial in range(num_trials): \n",
    "    print()\n",
    "    print(\"Trial {}\".format(trial + 1))\n",
    "    \n",
    "    actor_model, critic_model, target_actor, target_critic, buffer, env = initialization(\n",
    "        reward_factor\n",
    "    )\n",
    "    \n",
    "    eps = MAX_EPSILON \n",
    "    steps = 0\n",
    "    \n",
    "    episode_rewards = [] \n",
    "    episode_SOCs = [] \n",
    "    episode_FCs = [] \n",
    "    for ep in range(total_episodes): \n",
    "        start = time.time() \n",
    "        state = env.reset() \n",
    "        episodic_reward = 0 \n",
    "\n",
    "        while True: \n",
    "            tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "            action = policy_epsilon_greedy(tf_state, eps)\n",
    "    #         print(action)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            if done: \n",
    "                next_state = [0] * num_states \n",
    "\n",
    "            buffer.record((state, action, reward, next_state))\n",
    "            episodic_reward += reward \n",
    "\n",
    "            if steps > DELAY_TRAINING: \n",
    "                buffer.learn() \n",
    "                update_target(tau)\n",
    "                eps = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * np.exp(-DECAY_RATE * steps)\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            if done: \n",
    "                break \n",
    "\n",
    "            state = next_state \n",
    "\n",
    "        elapsed_time = time.time() - start \n",
    "        print(\"elapsed_time: {:.3f}\".format(elapsed_time))\n",
    "        episode_rewards.append(episodic_reward) \n",
    "        episode_SOCs.append(env.SOC)\n",
    "        episode_FCs.append(env.fuel_consumption) \n",
    "\n",
    "    #     print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "        SOC_deviation_history = np.sum(np.abs(np.array(env.history[\"SOC\"]) - 0.6)) \n",
    "        print(\n",
    "              'Episode: {}'.format(ep + 1),\n",
    "              \"Exploration P: {:.4f}\".format(eps),\n",
    "              'Total reward: {}'.format(episodic_reward), \n",
    "              \"SOC: {:.4f}\".format(env.SOC), \n",
    "              \"Cumulative_SOC_deviation: {:.4f}\".format(SOC_deviation_history), \n",
    "              \"Fuel Consumption: {:.4f}\".format(env.fuel_consumption), \n",
    "        )\n",
    "\n",
    "    results_dict[trial + 1] = {\n",
    "        \"rewards\": episode_rewards, \n",
    "        \"SOCs\": episode_SOCs, \n",
    "        \"FCs\": episode_FCs \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DDPG10_1.pkl\", \"wb\") as f: \n",
    "    pickle.dump(results_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
